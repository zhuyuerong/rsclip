#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
è°ƒè¯•å½’ä¸€åŒ–å‰åçš„ç›¸ä¼¼åº¦åˆ†å¸ƒ
"""
import sys
import os
from pathlib import Path
import torch
import numpy as np
from PIL import Image

# æ·»åŠ é¡¹ç›®è·¯å¾„
project_root = Path(__file__).parent
sys.path.insert(0, str(project_root))

from src.competitors.clip_methods.surgeryclip.model_wrapper import SurgeryCLIPWrapper

print("="*70)
print("ğŸ” è°ƒè¯•å½’ä¸€åŒ–å‰åçš„ç›¸ä¼¼åº¦åˆ†å¸ƒ")
print("="*70)

# æµ‹è¯•å›¾åƒ
image_path = "datasets/mini-DIOR/images/00679.jpg"
image = Image.open(image_path).convert('RGB')

# ============ æµ‹è¯•1: å•ç±»åˆ«Surgery ============
print("\n" + "="*70)
print("æµ‹è¯•1: å•ç±»åˆ«Surgery")
print("="*70)

model1 = SurgeryCLIPWrapper(
    model_name='surgeryclip',
    checkpoint_path='checkpoints/ViT-B-32.pt',
    use_surgery_single_class=True,
    use_surgery_multi_class=False,
    device='cpu'
)
model1.load_model()

with torch.no_grad():
    image_tensor = model1.preprocess(image).unsqueeze(0).to(model1.device)
    image_features_all = model1.model.encode_image(image_tensor)
    image_features_all = image_features_all / image_features_all.norm(dim=-1, keepdim=True)
    
    from src.competitors.clip_methods.surgeryclip.clip import encode_text_with_prompt_ensemble, clip_feature_surgery
    text_features = encode_text_with_prompt_ensemble(model1.model, ['vehicle'], model1.device)
    
    redundant_features = encode_text_with_prompt_ensemble(model1.model, [""], model1.device)
    similarity_maps = clip_feature_surgery(image_features_all, text_features, redundant_features)
    
    # æ’é™¤class token
    similarity_maps_patches = similarity_maps[:, 1:, :]  # [1, 49, 1]
    
    print(f"\nå½’ä¸€åŒ–å‰ç»Ÿè®¡:")
    similarity_np = similarity_maps_patches.detach().cpu().numpy().flatten()
    print(f"  min={similarity_np.min():.6f}, max={similarity_np.max():.6f}")
    print(f"  mean={similarity_np.mean():.6f}, std={similarity_np.std():.6f}")
    print(f"  è´Ÿå€¼%: {(similarity_np < 0).sum() / len(similarity_np) * 100:.2f}%")
    
    # å½’ä¸€åŒ–
    from src.competitors.clip_methods.surgeryclip.clip import get_similarity_map
    target_h, target_w = image.size[1], image.size[0]
    heatmap_tensor = get_similarity_map(similarity_maps_patches, (target_h, target_w))
    heatmap = heatmap_tensor[0, :, :, 0].detach().cpu().numpy()
    
    print(f"\nå½’ä¸€åŒ–åç»Ÿè®¡:")
    print(f"  min={heatmap.min():.6f}, max={heatmap.max():.6f}")
    print(f"  mean={heatmap.mean():.6f}, std={heatmap.std():.6f}")
    
    # åˆ†ä½æ•°
    print(f"\nå½’ä¸€åŒ–ååˆ†ä½æ•°:")
    for p in [0, 10, 25, 50, 75, 90, 100]:
        val = np.percentile(heatmap, p)
        print(f"  {p:3d}%: {val:.6f}")

# ============ æµ‹è¯•2: å•ç±»åˆ«ä½™å¼¦ ============
print("\n" + "="*70)
print("æµ‹è¯•2: å•ç±»åˆ«ä½™å¼¦")
print("="*70)

model2 = SurgeryCLIPWrapper(
    model_name='surgeryclip',
    checkpoint_path='checkpoints/ViT-B-32.pt',
    use_surgery_single_class=False,
    use_surgery_multi_class=False,
    device='cpu'
)
model2.load_model()

with torch.no_grad():
    image_tensor = model2.preprocess(image).unsqueeze(0).to(model2.device)
    image_features_all = model2.model.encode_image(image_tensor)
    image_features_all = image_features_all / image_features_all.norm(dim=-1, keepdim=True)
    
    from src.competitors.clip_methods.surgeryclip.clip import encode_text_with_prompt_ensemble
    text_features = encode_text_with_prompt_ensemble(model2.model, ['vehicle'], model2.device)
    
    similarity_maps = image_features_all @ text_features.t()
    
    # æ’é™¤class token
    similarity_maps_patches = similarity_maps[:, 1:, :]  # [1, 49, 1]
    
    print(f"\nå½’ä¸€åŒ–å‰ç»Ÿè®¡:")
    similarity_np = similarity_maps_patches.detach().cpu().numpy().flatten()
    print(f"  min={similarity_np.min():.6f}, max={similarity_np.max():.6f}")
    print(f"  mean={similarity_np.mean():.6f}, std={similarity_np.std():.6f}")
    print(f"  è´Ÿå€¼%: {(similarity_np < 0).sum() / len(similarity_np) * 100:.2f}%")
    
    # å½’ä¸€åŒ–
    from src.competitors.clip_methods.surgeryclip.clip import get_similarity_map
    target_h, target_w = image.size[1], image.size[0]
    heatmap_tensor = get_similarity_map(similarity_maps_patches, (target_h, target_w))
    heatmap = heatmap_tensor[0, :, :, 0].detach().cpu().numpy()
    
    print(f"\nå½’ä¸€åŒ–åç»Ÿè®¡:")
    print(f"  min={heatmap.min():.6f}, max={heatmap.max():.6f}")
    print(f"  mean={heatmap.mean():.6f}, std={heatmap.std():.6f}")
    
    # åˆ†ä½æ•°
    print(f"\nå½’ä¸€åŒ–ååˆ†ä½æ•°:")
    for p in [0, 10, 25, 50, 75, 90, 100]:
        val = np.percentile(heatmap, p)
        print(f"  {p:3d}%: {val:.6f}")

# ============ å¯¹æ¯”åˆ†æ ============
print("\n" + "="*70)
print("å¯¹æ¯”åˆ†æ")
print("="*70)

print("\nå…³é”®å‘ç°:")
print("1. å¦‚æœå½’ä¸€åŒ–åçš„stdï¼ˆæ ‡å‡†å·®ï¼‰å¾ˆæ¥è¿‘ â†’ è§†è§‰ä¸Šä¼šå¾ˆåƒ")
print("2. å¦‚æœå½’ä¸€åŒ–åçš„åˆ†ä½æ•°åˆ†å¸ƒå¾ˆæ¥è¿‘ â†’ çƒ­å›¾çœ‹èµ·æ¥ä¸€æ ·")
print("3. éœ€è¦æ£€æŸ¥å½’ä¸€åŒ–å‰çš„ç›¸å¯¹åˆ†å¸ƒæ˜¯å¦ç›¸ä¼¼")

print("\n" + "="*70)






