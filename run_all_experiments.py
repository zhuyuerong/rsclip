#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
åœ¨ mini_dataset ä¸Šè¿è¡Œæ‰€æœ‰å®éªŒå¹¶ç”Ÿæˆæ€§èƒ½æŠ¥å‘Š

åŠŸèƒ½ï¼š
1. è¿è¡Œ Experiment1 (ä¸¤é˜¶æ®µæ£€æµ‹)
2. è¿è¡Œ Experiment2 (ä¸Šä¸‹æ–‡å¼•å¯¼æ£€æµ‹) 
3. è¿è¡Œ Experiment3 (OVA-DETR)
4. æ”¶é›†æ€§èƒ½æŒ‡æ ‡
5. ç”Ÿæˆå¯¹æ¯”æŠ¥å‘Š
"""

import torch
import time
import json
from pathlib import Path
import numpy as np
from collections import defaultdict
import sys

# æ·»åŠ è·¯å¾„
sys.path.append('experiment1')
sys.path.append('experiment2')
sys.path.append('experiment3')


class PerformanceMonitor:
    """æ€§èƒ½ç›‘æ§å™¨"""
    
    def __init__(self):
        self.metrics = {}
        self.start_time = None
    
    def start(self):
        """å¼€å§‹è®¡æ—¶"""
        self.start_time = time.time()
        if torch.cuda.is_available():
            torch.cuda.synchronize()
    
    def end(self):
        """ç»“æŸè®¡æ—¶"""
        if torch.cuda.is_available():
            torch.cuda.synchronize()
        elapsed = time.time() - self.start_time
        return elapsed
    
    def get_model_params(self, model):
        """è·å–æ¨¡å‹å‚æ•°é‡"""
        total_params = sum(p.numel() for p in model.parameters())
        trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
        return {
            'total_params': total_params,
            'trainable_params': trainable_params,
            'frozen_params': total_params - trainable_params
        }
    
    def get_gpu_memory(self):
        """è·å–GPUå†…å­˜ä½¿ç”¨"""
        if torch.cuda.is_available():
            return {
                'allocated': torch.cuda.memory_allocated() / 1024**2,  # MB
                'reserved': torch.cuda.memory_reserved() / 1024**2,
                'max_allocated': torch.cuda.max_memory_allocated() / 1024**2
            }
        return {}


def evaluate_experiment3_mini():
    """åœ¨ mini_dataset ä¸Šè¯„ä¼° Experiment3"""
    
    print("=" * 70)
    print("Experiment3: OVA-DETR è¯„ä¼°")
    print("=" * 70)
    
    from experiment3.config.default_config import DefaultConfig
    from experiment3.models.ova_detr import OVADETR
    from experiment3.utils.data_loader import create_data_loader, DIOR_CLASSES
    from experiment3.utils.transforms import get_transforms
    
    monitor = PerformanceMonitor()
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    
    # é…ç½®
    config = DefaultConfig()
    config.batch_size = 4
    config.image_size = (800, 800)
    
    # åˆ›å»ºæ¨¡å‹
    print("\nåˆ›å»ºæ¨¡å‹...")
    model = OVADETR(config).to(device)
    model.eval()
    
    # æ¨¡å‹å‚æ•°
    model_params = monitor.get_model_params(model)
    
    print(f"\næ¨¡å‹å‚æ•°:")
    print(f"  æ€»å‚æ•°: {model_params['total_params']:,}")
    print(f"  å¯è®­ç»ƒ: {model_params['trainable_params']:,}")
    print(f"  å†»ç»“: {model_params['frozen_params']:,}")
    
    # æå–æ–‡æœ¬ç‰¹å¾
    print("\næå–æ–‡æœ¬ç‰¹å¾...")
    with torch.no_grad():
        text_features = model.backbone.forward_text(DIOR_CLASSES).to(device)
    
    # åˆ›å»ºæ•°æ®åŠ è½½å™¨
    print("\nåŠ è½½ mini_dataset...")
    val_transforms = get_transforms(mode='val', image_size=config.image_size)
    
    # ä½¿ç”¨ mini_dataset
    from experiment3.utils.data_loader import DiorDataset
    
    dataset = DiorDataset(
        root_dir='datasets/mini_dataset',
        split='train',  # mini_dataset åªæœ‰trainåˆ†å‰²
        transforms=val_transforms
    )
    
    from torch.utils.data import DataLoader
    from experiment3.utils.data_loader import collate_fn
    
    data_loader = DataLoader(
        dataset,
        batch_size=config.batch_size,
        shuffle=False,
        collate_fn=collate_fn
    )
    
    print(f"æ•°æ®é›†å¤§å°: {len(dataset)}")
    
    # æ¨ç†
    print("\nå¼€å§‹æ¨ç†...")
    monitor.start()
    
    all_predictions = []
    all_targets = []
    
    with torch.no_grad():
        for images, targets in data_loader:
            images = images.to(device)
            
            # å‰å‘ä¼ æ’­
            outputs = model(images, text_features)
            
            # ä½¿ç”¨æœ€åä¸€å±‚
            pred_logits = outputs['pred_logits'][-1]
            pred_boxes = outputs['pred_boxes'][-1]
            
            # å¤„ç†æ¯å¼ å›¾
            for i in range(pred_logits.shape[0]):
                logits = pred_logits[i]
                boxes = pred_boxes[i]
                
                scores = logits.sigmoid()
                max_scores, labels = scores.max(dim=-1)
                
                # è¿‡æ»¤
                keep = max_scores > 0.3
                
                all_predictions.append({
                    'boxes': boxes[keep].cpu(),
                    'scores': max_scores[keep].cpu(),
                    'labels': labels[keep].cpu()
                })
                
                all_targets.append({
                    'boxes': targets[i]['boxes'].cpu(),
                    'labels': targets[i]['labels'].cpu()
                })
    
    inference_time = monitor.end()
    
    # è®¡ç®—æŒ‡æ ‡
    print("\nè®¡ç®—æ€§èƒ½æŒ‡æ ‡...")
    from experiment3.evaluate import evaluate_detections
    from experiment3.losses.bbox_loss import box_cxcywh_to_xyxy
    
    # è½¬æ¢åæ ‡
    for i, (pred, target) in enumerate(zip(all_predictions, all_targets)):
        if len(pred['boxes']) > 0:
            pred['boxes'] = box_cxcywh_to_xyxy(pred['boxes']) * 800
        if len(target['boxes']) > 0:
            target['boxes'] = box_cxcywh_to_xyxy(target['boxes']) * 800
    
    metrics = evaluate_detections(all_predictions, all_targets, len(DIOR_CLASSES))
    
    # GPUå†…å­˜
    gpu_memory = monitor.get_gpu_memory()
    
    return {
        'experiment': 'Experiment3',
        'model': 'OVA-DETR',
        'params': model_params,
        'inference_time': inference_time,
        'fps': len(dataset) / inference_time,
        'metrics': metrics,
        'gpu_memory': gpu_memory
    }


def create_experiment1_evaluator():
    """ä¸º Experiment1 åˆ›å»ºè¯„ä¼°è„šæœ¬"""
    
    print("=" * 70)
    print("Experiment1: ä¸¤é˜¶æ®µæ£€æµ‹ è¯„ä¼°")  
    print("=" * 70)
    
    # Experiment1 ä¸»è¦æ˜¯åŸºäºæ£€ç´¢å’ŒåŒºåŸŸçš„æ–¹æ³•
    # éœ€è¦é’ˆå¯¹æ€§åœ°åˆ›å»ºè¯„ä¼°æµç¨‹
    
    print("\nâš ï¸ Experiment1 éœ€è¦åˆ›å»ºä¸“é—¨çš„è¯„ä¼°è„šæœ¬")
    print("   Experiment1 ä½¿ç”¨ä¸¤é˜¶æ®µæ–¹æ³•ï¼š")
    print("   - Stage1: æè®®ç”Ÿæˆ + åˆ†ç±»")
    print("   - Stage2: ç›®æ ‡æ£€æµ‹ + è¾¹ç•Œæ¡†ç»†åŒ–")
    print("   éœ€è¦åˆ›å»ºé€‚é… mini_dataset çš„è¯„ä¼°æµç¨‹")
    
    return None


def create_experiment2_complete():
    """ä¸º Experiment2 åˆ›å»ºå®Œæ•´ç³»ç»Ÿ"""
    
    print("=" * 70)
    print("Experiment2: ä¸Šä¸‹æ–‡å¼•å¯¼æ£€æµ‹ è¯„ä¼°")
    print("=" * 70)
    
    print("\nâš ï¸ Experiment2 ç¼ºå°‘ä»¥ä¸‹ç»„ä»¶ï¼š")
    print("   âŒ æ•°æ®åŠ è½½å™¨")
    print("   âŒ è®­ç»ƒè„šæœ¬")
    print("   âŒ è¯„ä¼°è„šæœ¬")
    print("   éœ€è¦è¡¥å……å®Œæ•´ç³»ç»Ÿåæ‰èƒ½è¿è¡Œ")
    
    return None


def generate_comparison_report(results):
    """ç”Ÿæˆå¯¹æ¯”æŠ¥å‘Š"""
    
    report = {
        'generated_at': time.strftime('%Y-%m-%d %H:%M:%S'),
        'dataset': 'mini_dataset',
        'dataset_size': 100,
        'experiments': results
    }
    
    # ä¿å­˜æŠ¥å‘Š
    report_path = Path('experiment_comparison_report.json')
    with open(report_path, 'w', encoding='utf-8') as f:
        json.dump(report, f, indent=2, ensure_ascii=False)
    
    print(f"\nâœ… æŠ¥å‘Šä¿å­˜åˆ°: {report_path}")
    
    return report


def print_comparison_table(results):
    """æ‰“å°å¯¹æ¯”è¡¨æ ¼"""
    
    print("\n" + "=" * 70)
    print("å®éªŒæ€§èƒ½å¯¹æ¯”")
    print("=" * 70)
    
    # è¡¨å¤´
    print(f"\n{'å®éªŒ':<15} {'æ¨¡å‹':<15} {'å‚æ•°é‡':<15} {'æ¨ç†æ—¶é—´':<12} {'FPS':<8} {'mAP':<8}")
    print("-" * 70)
    
    for result in results:
        if result is None:
            continue
        
        exp_name = result['experiment']
        model_name = result['model']
        total_params = result['params']['total_params'] / 1e6  # M
        inference_time = result['inference_time']
        fps = result['fps']
        mAP = result['metrics'].get('mAP', 0.0)
        
        print(f"{exp_name:<15} {model_name:<15} {total_params:>8.2f}M {inference_time:>8.2f}s {fps:>6.2f} {mAP:>6.4f}")


def main():
    """ä¸»å‡½æ•°"""
    
    print("\n" + "ğŸ¯" * 35)
    print("åœ¨ Mini Dataset ä¸Šè¿è¡Œæ‰€æœ‰å®éªŒ")
    print("ğŸ¯" * 35 + "\n")
    
    results = []
    
    # Experiment1
    print("\n" + "â–¶" * 35)
    result1 = create_experiment1_evaluator()
    if result1:
        results.append(result1)
    
    # Experiment2  
    print("\n" + "â–¶" * 35)
    result2 = create_experiment2_complete()
    if result2:
        results.append(result2)
    
    # Experiment3
    print("\n" + "â–¶" * 35)
    result3 = evaluate_experiment3_mini()
    if result3:
        results.append(result3)
    
    # ç”ŸæˆæŠ¥å‘Š
    if len(results) > 0:
        print_comparison_table(results)
        generate_comparison_report(results)
    
    print("\n" + "=" * 70)
    print("è¯„ä¼°å®Œæˆï¼")
    print("=" * 70)


if __name__ == '__main__':
    main()

