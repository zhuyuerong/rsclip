#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
RemoteCLIP åŒºåŸŸå¯è§†åŒ–å·¥å…·
å°†æå–çš„åŒºåŸŸå’Œè¯†åˆ«ç»“æœç»˜åˆ¶åœ¨å›¾åƒä¸Š
"""

import torch
import open_clip
from PIL import Image, ImageDraw, ImageFont
import os
import numpy as np
import cv2
import argparse
from sampling import sample_regions
from output_manager import get_output_manager

# é¢œè‰²æ–¹æ¡ˆï¼ˆBGRæ ¼å¼ï¼‰
COLORS = {
    'critical': (0, 0, 255),      # çº¢è‰²
    'high': (0, 165, 255),        # æ©™è‰²
    'medium': (0, 255, 255),      # é»„è‰²
    'low': (0, 255, 0),           # ç»¿è‰²
    'fallback': (128, 128, 128),  # ç°è‰²
    'default': (255, 0, 0),       # è“è‰²
}

def draw_regions_on_image(image_path, regions, output_path, region_labels=None, strategy_name=""):
    """
    åœ¨å›¾åƒä¸Šç»˜åˆ¶åŒºåŸŸæ¡†å’Œæ ‡ç­¾
    
    å‚æ•°:
        image_path: è¾“å…¥å›¾åƒè·¯å¾„
        regions: åŒºåŸŸåˆ—è¡¨
        output_path: è¾“å‡ºå›¾åƒè·¯å¾„
        region_labels: æ¯ä¸ªåŒºåŸŸçš„æ ‡ç­¾å­—å…¸ {region_idx: (label, confidence)}
        strategy_name: é‡‡æ ·ç­–ç•¥åç§°
    """
    # è¯»å–å›¾åƒ
    image = cv2.imread(image_path)
    if image is None:
        print(f"âŒ æ— æ³•è¯»å–å›¾åƒ: {image_path}")
        return
    
    # åˆ›å»ºå‰¯æœ¬ç”¨äºç»˜åˆ¶
    vis_image = image.copy()
    
    # ç»˜åˆ¶æ¯ä¸ªåŒºåŸŸ
    for idx, region in enumerate(regions):
        x1, y1, x2, y2 = region['bbox']
        
        # ç¡®ä¿åæ ‡åœ¨æœ‰æ•ˆèŒƒå›´å†…
        x1, y1 = max(0, int(x1)), max(0, int(y1))
        x2, y2 = min(image.shape[1], int(x2)), min(image.shape[0], int(y2))
        
        # é€‰æ‹©é¢œè‰²
        priority = region.get('priority', 'default')
        threshold = region.get('threshold', None)
        
        if priority in COLORS:
            color = COLORS[priority]
        elif threshold is not None:
            # æ ¹æ®é˜ˆå€¼é€‰æ‹©é¢œè‰²
            if threshold >= 0.7:
                color = COLORS['critical']
            elif threshold >= 0.5:
                color = COLORS['high']
            elif threshold >= 0.3:
                color = COLORS['medium']
            else:
                color = COLORS['low']
        else:
            color = COLORS['default']
        
        # ç»˜åˆ¶çŸ©å½¢æ¡†ï¼ˆçº¿æ¡ç»†ä¸€å€ï¼‰
        thickness = 1
        cv2.rectangle(vis_image, (x1, y1), (x2, y2), color, thickness)
        
        # å‡†å¤‡æ ‡ç­¾æ–‡æœ¬
        if region_labels and idx in region_labels:
            label, confidence = region_labels[idx]
            text = f"{idx+1}: {label} {confidence:.1f}%"
        else:
            text = f"{idx+1}"
        
        # æ·»åŠ ä¼˜å…ˆçº§ä¿¡æ¯
        if priority != 'default':
            text += f" [{priority}]"
        elif threshold is not None:
            text += f" [t:{threshold}]"
        
        # ç»˜åˆ¶æ ‡ç­¾èƒŒæ™¯ï¼ˆå­—å·å°ä¸€å€ï¼‰
        font = cv2.FONT_HERSHEY_SIMPLEX
        font_scale = 0.25
        font_thickness = 1
        (text_width, text_height), baseline = cv2.getTextSize(text, font, font_scale, font_thickness)
        
        # æ ‡ç­¾ä½ç½®ï¼ˆåœ¨æ¡†çš„ä¸Šæ–¹ï¼‰
        label_y = y1 - 5
        if label_y - text_height - 5 < 0:
            label_y = y1 + text_height + 5
        
        # ç»˜åˆ¶æ ‡ç­¾èƒŒæ™¯
        cv2.rectangle(vis_image, 
                     (x1, label_y - text_height - 5), 
                     (x1 + text_width + 5, label_y + baseline),
                     color, -1)
        
        # ç»˜åˆ¶æ ‡ç­¾æ–‡å­—
        cv2.putText(vis_image, text, (x1 + 2, label_y - 2),
                   font, font_scale, (255, 255, 255), font_thickness)
    
    # æ·»åŠ æ ‡é¢˜ï¼ˆå­—å·å°ä¸€å€ï¼‰
    title = f"Strategy: {strategy_name} | Regions: {len(regions)}"
    cv2.putText(vis_image, title, (10, 20),
               cv2.FONT_HERSHEY_SIMPLEX, 0.4, (255, 255, 255), 1)
    cv2.putText(vis_image, title, (10, 20),
               cv2.FONT_HERSHEY_SIMPLEX, 0.4, (0, 0, 0), 1)
    
    # ä¿å­˜å›¾åƒ
    cv2.imwrite(output_path, vis_image)
    print(f"âœ… å¯è§†åŒ–ç»“æœå·²ä¿å­˜åˆ°: {output_path}")


def main():
    parser = argparse.ArgumentParser(description='RemoteCLIP åŒºåŸŸå¯è§†åŒ–')
    parser.add_argument('--strategy', type=str, default='multi_threshold_saliency',
                        choices=['layered', 'pyramid', 'multi_threshold_saliency', 'all'],
                        help='é‡‡æ ·ç­–ç•¥ (allè¡¨ç¤ºè¿è¡Œæ‰€æœ‰ç­–ç•¥)')
    parser.add_argument('--model', type=str, default='RN50',
                        choices=['RN50', 'ViT-B-32', 'ViT-L-14'],
                        help='æ¨¡å‹é€‰æ‹©')
    parser.add_argument('--image', type=str, default='assets/airport.jpg',
                        help='è¾“å…¥å›¾åƒè·¯å¾„')
    parser.add_argument('--output-dir', type=str, default='extensions/outputs/visualizations',
                        help='è¾“å‡ºç›®å½•')
    parser.add_argument('--max-regions', type=int, default=50,
                        help='æœ€å¤§åŒºåŸŸæ•°')
    parser.add_argument('--top-k', type=int, default=10,
                        help='åˆ†æå‰Kä¸ªé‡è¦åŒºåŸŸ')
    
    args = parser.parse_args()
    
    print("=" * 70)
    print("RemoteCLIP åŒºåŸŸå¯è§†åŒ–å·¥å…·")
    print("=" * 70)
    
    # åˆ›å»ºè¾“å‡ºç›®å½•
    om = get_output_manager()
    if args.output_dir == 'extensions/outputs/visualizations':
        # ä½¿ç”¨é»˜è®¤è¾“å‡ºç®¡ç†å™¨è·¯å¾„
        args.output_dir = om.dirs['visualizations']
    os.makedirs(args.output_dir, exist_ok=True)
    
    # 1. åŠ è½½æ¨¡å‹
    model_name = args.model
    device = "cuda" if torch.cuda.is_available() else "cpu"
    
    print(f"\nğŸ”„ æ­£åœ¨åŠ è½½ {model_name} æ¨¡å‹...")
    model, _, preprocess = open_clip.create_model_and_transforms(model_name)
    tokenizer = open_clip.get_tokenizer(model_name)
    
    checkpoint_path = f"checkpoints/RemoteCLIP-{model_name}.pt"
    if not os.path.exists(checkpoint_path):
        print(f"âŒ æ‰¾ä¸åˆ°æ¨¡å‹æ–‡ä»¶: {checkpoint_path}")
        return
    
    ckpt = torch.load(checkpoint_path, map_location="cpu")
    model.load_state_dict(ckpt)
    model = model.to(device).eval()
    print(f"âœ… æ¨¡å‹å·²åŠ è½½åˆ° {device}")
    
    # 2. åŠ è½½å›¾åƒ
    if not os.path.exists(args.image):
        print(f"âŒ æ‰¾ä¸åˆ°å›¾åƒ: {args.image}")
        return
    
    pil_image = Image.open(args.image)
    cv_image = np.array(pil_image)
    print(f"âœ… å›¾åƒå·²åŠ è½½: {cv_image.shape}")
    
    # 3. å®šä¹‰æŸ¥è¯¢
    region_queries = [
        "airport", "runway", "airplane", "aircraft", 
        "building", "terminal", "parking lot", "road",
        "vegetation", "water"
    ]
    
    # é¢„ç¼–ç æ–‡æœ¬ç‰¹å¾
    region_text = tokenizer(region_queries)
    with torch.no_grad():
        region_text_features = model.encode_text(region_text.to(device))
        region_text_features /= region_text_features.norm(dim=-1, keepdim=True)
    
    # 4. å¤„ç†ç­–ç•¥
    strategies = ['multi_threshold_saliency', 'layered', 'pyramid'] if args.strategy == 'all' else [args.strategy]
    
    for strategy in strategies:
        print(f"\n{'='*70}")
        print(f"ğŸ“Š å¤„ç†ç­–ç•¥: {strategy}")
        print(f"{'='*70}")
        
        # åŒºåŸŸé‡‡æ ·
        regions = sample_regions(
            cv_image,
            strategy=strategy,
            max_regions=args.max_regions
        )
        
        if len(regions) == 0:
            print("âŒ æœªæ‰¾åˆ°åŒºåŸŸ")
            continue
        
        # åˆ†ææ¯ä¸ªåŒºåŸŸ
        region_labels = {}
        print(f"\nåˆ†æå‰ {min(args.top_k, len(regions))} ä¸ªåŒºåŸŸ...")
        
        for idx, region in enumerate(regions[:args.top_k]):
            x1, y1, x2, y2 = region['bbox']
            x1, y1 = max(0, x1), max(0, y1)
            x2, y2 = min(cv_image.shape[1], x2), min(cv_image.shape[0], y2)
            
            # è£å‰ªåŒºåŸŸ
            cropped = cv_image[y1:y2, x1:x2]
            if cropped.size == 0 or cropped.shape[0] < 10 or cropped.shape[1] < 10:
                continue
            
            cropped_pil = Image.fromarray(cropped)
            cropped_tensor = preprocess(cropped_pil).unsqueeze(0)
            
            # æ¨ç†
            with torch.no_grad():
                crop_features = model.encode_image(cropped_tensor.to(device))
                crop_features /= crop_features.norm(dim=-1, keepdim=True)
                probs = (100.0 * crop_features @ region_text_features.T).softmax(dim=-1).cpu().numpy()[0]
            
            # ä¿å­˜æœ€ä½³åŒ¹é…
            best_idx = probs.argmax()
            best_label = region_queries[best_idx]
            best_confidence = probs[best_idx] * 100
            
            region_labels[idx] = (best_label, best_confidence)
            
            print(f"  åŒºåŸŸ {idx+1}: {best_label} ({best_confidence:.1f}%)")
        
        # 5. å¯è§†åŒ–å¹¶ä¿å­˜
        base_name = os.path.splitext(os.path.basename(args.image))[0]
        output_path = os.path.join(args.output_dir, f"{base_name}_{strategy}_visualization.jpg")
        
        draw_regions_on_image(
            args.image,
            regions[:args.top_k],
            output_path,
            region_labels,
            strategy_name=strategy
        )
    
    print(f"\n{'='*70}")
    print(f"âœ… æ‰€æœ‰å¯è§†åŒ–å®Œæˆï¼")
    print(f"   è¾“å‡ºç›®å½•: {args.output_dir}")
    print(f"{'='*70}")


if __name__ == "__main__":
    main()

