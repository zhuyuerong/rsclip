#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
RemoteCLIP åŒºåŸŸé‡‡æ ·ç­–ç•¥æ¨¡å—
æä¾›ä¸‰ç§é‡‡æ ·ç­–ç•¥ï¼š
1. å¤šé˜ˆå€¼åˆ†å±‚é‡‡æ · (Multi-Threshold Layered Sampling)
2. å¤šå°ºåº¦é‡‘å­—å¡”é‡‡æ · (Multi-Scale Pyramid Sampling)
3. å¤šé˜ˆå€¼æ˜¾è‘—æ€§é‡‡æ · (Multi-Threshold Saliency Sampling) - è½»é‡çº§
"""

import cv2
import numpy as np
from typing import List, Dict, Tuple, Optional


def compute_iou(box1: Tuple, box2: Tuple) -> float:
    """
    è®¡ç®—ä¸¤ä¸ªbboxçš„IoU
    
    å‚æ•°:
        box1, box2: (x1, y1, x2, y2)
    """
    x1_1, y1_1, x2_1, y2_1 = box1
    x1_2, y1_2, x2_2, y2_2 = box2
    
    xi1 = max(x1_1, x1_2)
    yi1 = max(y1_1, y1_2)
    xi2 = min(x2_1, x2_2)
    yi2 = min(y2_1, y2_2)
    
    inter_area = max(0, xi2 - xi1) * max(0, yi2 - yi1)
    
    box1_area = (x2_1 - x1_1) * (y2_1 - y1_1)
    box2_area = (x2_2 - x1_2) * (y2_2 - y1_2)
    union_area = box1_area + box2_area - inter_area
    
    return inter_area / union_area if union_area > 0 else 0


def non_max_suppression_regions(regions: List[Dict], iou_threshold: float = 0.5) -> List[Dict]:
    """
    NMSå»é™¤é«˜åº¦é‡å çš„åŒºåŸŸ
    
    å‚æ•°:
        regions: åŒºåŸŸåˆ—è¡¨
        iou_threshold: IoUé˜ˆå€¼
    """
    if len(regions) == 0:
        return []
    
    regions = sorted(regions, key=lambda x: x.get('score', 0), reverse=True)
    
    keep = []
    while len(regions) > 0:
        best = regions.pop(0)
        keep.append(best)
        
        regions = [
            r for r in regions 
            if compute_iou(best['bbox'], r['bbox']) < iou_threshold
        ]
    
    return keep


def fallback_grid_sampling(image: np.ndarray, used_area: np.ndarray, n_needed: int) -> List[Dict]:
    """
    å…œåº•çš„ç½‘æ ¼é‡‡æ ·ï¼Œå¡«è¡¥ç©ºç™½åŒºåŸŸ
    
    å‚æ•°:
        image: è¾“å…¥å›¾åƒ
        used_area: å·²ä½¿ç”¨åŒºåŸŸçš„mask
        n_needed: éœ€è¦çš„åŒºåŸŸæ•°é‡
    """
    h, w = image.shape[:2]
    grid_size = 256
    stride = grid_size // 2
    
    fallback_regions = []
    for y in range(0, h - grid_size, stride):
        for x in range(0, w - grid_size, stride):
            # æ£€æŸ¥è¿™ä¸ªåŒºåŸŸæ˜¯å¦å·²è¢«è¦†ç›–
            patch_area = used_area[y:y+grid_size, x:x+grid_size]
            if patch_area.mean() < 0.3:  # å°‘äº30%è¢«è¦†ç›–
                fallback_regions.append({
                    'bbox': (x, y, x+grid_size, y+grid_size),
                    'area': grid_size * grid_size,
                    'saliency': 0,
                    'priority': 'fallback',
                    'weight': 0.2,
                    'score': 0
                })
                
                if len(fallback_regions) >= n_needed:
                    break
        if len(fallback_regions) >= n_needed:
            break
    
    return fallback_regions


def compute_coverage_and_supplement(
    image: np.ndarray, 
    regions: List[Dict], 
    min_coverage: float = 0.5
) -> List[Dict]:
    """
    è®¡ç®—è¦†ç›–ç‡ï¼Œå¦‚æœä¸è¶³åˆ™è¡¥å……åŒºåŸŸ
    
    å‚æ•°:
        image: è¾“å…¥å›¾åƒ
        regions: å·²æœ‰åŒºåŸŸåˆ—è¡¨
        min_coverage: æœ€å°è¦†ç›–ç‡
    """
    # è®¡ç®—è¦†ç›–ç‡
    coverage_map = np.zeros(image.shape[:2], dtype=bool)
    for r in regions:
        x1, y1, x2, y2 = r['bbox']
        x1, y1 = max(0, x1), max(0, y1)
        x2, y2 = min(image.shape[1], x2), min(image.shape[0], y2)
        coverage_map[y1:y2, x1:x2] = True
    
    coverage_ratio = coverage_map.sum() / coverage_map.size
    print(f"   å›¾åƒè¦†ç›–ç‡: {coverage_ratio*100:.1f}%")
    
    if coverage_ratio < min_coverage:
        print(f"   âš ï¸  è¦†ç›–ç‡ä¸è¶³ï¼Œæ·»åŠ è¡¥å……åŒºåŸŸ...")
        n_supplement = max(10, int(len(regions) * 0.3))
        supplement_regions = fallback_grid_sampling(image, coverage_map, n_supplement)
        regions.extend(supplement_regions)
        print(f"   âœ“ è¡¥å……äº† {len(supplement_regions)} ä¸ªåŒºåŸŸ")
    
    return regions


# ==================== ç­–ç•¥1ï¼šå¤šé˜ˆå€¼åˆ†å±‚é‡‡æ · ====================

def multi_threshold_layered_sampling(
    image: np.ndarray, 
    min_regions: int = 50, 
    max_regions: int = 200
) -> List[Dict]:
    """
    ç­–ç•¥1: å¤šé˜ˆå€¼åˆ†å±‚é‡‡æ ·
    
    æ ¸å¿ƒæ€æƒ³ï¼š
    - ä½é˜ˆå€¼ä¿è¯è¦†ç›– â†’ é«˜å¬å›
    - åˆ†ä¼˜å…ˆçº§ â†’ å¯ä»¥æŒ‰é‡è¦æ€§å¤„ç†
    
    å‚æ•°:
        image: è¾“å…¥å›¾åƒ (RGBæˆ–BGRæ ¼å¼)
        min_regions: æœ€å°‘åŒºåŸŸæ•°
        max_regions: æœ€å¤šåŒºåŸŸæ•°
    
    è¿”å›:
        åŒºåŸŸåˆ—è¡¨ï¼Œæ¯ä¸ªåŒºåŸŸåŒ…å« bbox, area, saliency, priority, weight, score
    """
    print("\nğŸ” ç­–ç•¥1: å¤šé˜ˆå€¼åˆ†å±‚é‡‡æ ·")
    
    # è½¬æ¢ä¸ºç°åº¦å›¾
    if len(image.shape) == 3:
        gray = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)
    else:
        gray = image
    
    # 1. è®¡ç®—æ˜¾è‘—æ€§
    saliency = cv2.saliency.StaticSaliencySpectralResidual_create()
    success, saliency_map = saliency.computeSaliency(gray)
    saliency_map = (saliency_map * 255).astype(np.uint8)
    
    # å¢å¼ºå¯¹æ¯”åº¦ï¼ˆæå‡å¼±æ˜¾è‘—åŒºåŸŸçš„å¯è§æ€§ï¼‰
    saliency_map = cv2.equalizeHist(saliency_map)
    
    # 2. å¤šä¸ªé˜ˆå€¼åˆ†å±‚
    thresholds = [
        ('critical', 200, 1.0),   # æé«˜ä¼˜å…ˆçº§
        ('high', 150, 0.8),        # é«˜ä¼˜å…ˆçº§
        ('medium', 100, 0.6),      # ä¸­ä¼˜å…ˆçº§
        ('low', 50, 0.4),          # ä½ä¼˜å…ˆçº§ï¼ˆä¿è¯å¬å›ï¼‰
    ]
    
    all_regions = []
    used_area = np.zeros(image.shape[:2], dtype=bool)  # é¿å…é‡å¤
    
    for priority, threshold, weight in thresholds:
        # äºŒå€¼åŒ–
        _, binary = cv2.threshold(saliency_map, threshold, 255, cv2.THRESH_BINARY)
        
        # æ’é™¤å·²å¤„ç†åŒºåŸŸ
        binary = binary & (~used_area.astype(np.uint8) * 255)
        
        # å½¢æ€å­¦å¤„ç†
        kernel_size = 11 if priority == 'low' else 7
        kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (kernel_size, kernel_size))
        binary = cv2.morphologyEx(binary, cv2.MORPH_CLOSE, kernel)
        binary = cv2.morphologyEx(binary, cv2.MORPH_OPEN, kernel)
        
        # è¿é€šåŸŸåˆ†æ
        num_labels, labels, stats, centroids = cv2.connectedComponentsWithStats(
            binary, connectivity=8
        )
        
        # æ”¶é›†è¿™ä¸€å±‚çš„åŒºåŸŸ
        for i in range(1, num_labels):
            x, y, w, h, area = stats[i]
            
            # åŠ¨æ€é¢ç§¯é˜ˆå€¼ï¼ˆä½ä¼˜å…ˆçº§å…è®¸æ›´å°çš„åŒºåŸŸï¼‰
            min_area = 300 if priority == 'critical' else 200 if priority == 'high' else 100
            
            if area > min_area:
                mask = (labels == i)
                avg_saliency = saliency_map[mask].mean()
                
                all_regions.append({
                    'bbox': (x, y, x+w, y+h),
                    'area': area,
                    'saliency': avg_saliency,
                    'priority': priority,
                    'weight': weight,
                    'score': avg_saliency * weight * np.log(area + 1)
                })
                
                # æ ‡è®°ä¸ºå·²ä½¿ç”¨ï¼ˆåŠ paddingé¿å…ç´§é‚»ï¼‰
                padding = 10
                y1 = max(0, y - padding)
                y2 = min(image.shape[0], y + h + padding)
                x1 = max(0, x - padding)
                x2 = min(image.shape[1], x + w + padding)
                used_area[y1:y2, x1:x2] = True
    
    # 3. æ’åºä½†ä¸æˆªæ–­ï¼ˆä¿ç•™æ‰€æœ‰ï¼‰
    all_regions.sort(key=lambda x: x['score'], reverse=True)
    
    # 4. å¦‚æœåŒºåŸŸå¤ªå°‘ï¼Œè¡¥å……ç½‘æ ¼é‡‡æ ·
    if len(all_regions) < min_regions:
        print(f"   âš ï¸  åªæ‰¾åˆ°{len(all_regions)}ä¸ªåŒºåŸŸï¼Œè¡¥å……ç½‘æ ¼é‡‡æ ·...")
        all_regions.extend(
            fallback_grid_sampling(image, used_area, min_regions - len(all_regions))
        )
    
    # 5. å¦‚æœå¤ªå¤šï¼Œå¯ä»¥æˆªæ–­ï¼ˆä½†ä¿ç•™ä½ä¼˜å…ˆçº§çš„ï¼‰
    if len(all_regions) > max_regions:
        # ä¿è¯æ¯ä¸ªä¼˜å…ˆçº§éƒ½æœ‰ä»£è¡¨
        kept_regions = []
        for priority in ['critical', 'high', 'medium', 'low']:
            priority_regions = [r for r in all_regions if r.get('priority') == priority]
            kept_regions.extend(priority_regions[:max_regions//4])
        all_regions = kept_regions
    
    print(f"   âœ“ é‡‡æ ·å¾—åˆ° {len(all_regions)} ä¸ªåŒºåŸŸ")
    print(f"     - Critical: {sum(1 for r in all_regions if r.get('priority')=='critical')}")
    print(f"     - High: {sum(1 for r in all_regions if r.get('priority')=='high')}")
    print(f"     - Medium: {sum(1 for r in all_regions if r.get('priority')=='medium')}")
    print(f"     - Low: {sum(1 for r in all_regions if r.get('priority')=='low')}")
    
    return all_regions


# ==================== ç­–ç•¥2ï¼šå¤šå°ºåº¦é‡‘å­—å¡”é‡‡æ · ====================

def multi_scale_pyramid_sampling(
    image: np.ndarray, 
    scales: List[float] = [1.0, 0.5, 0.25],
    max_regions: int = 200
) -> List[Dict]:
    """
    ç­–ç•¥2: å¤šå°ºåº¦é‡‘å­—å¡”é‡‡æ ·
    
    æ ¸å¿ƒæ€æƒ³ï¼š
    - åŸå°ºåº¦ï¼šæ‰¾å¤§ç›®æ ‡
    - 0.5å€ï¼šæ‰¾ä¸­ç­‰ç›®æ ‡  
    - 0.25å€ï¼šæ‰¾å°ç›®æ ‡èšé›†åŒº
    
    å‚æ•°:
        image: è¾“å…¥å›¾åƒ
        scales: å°ºåº¦åˆ—è¡¨
        max_regions: æœ€å¤§åŒºåŸŸæ•°
    
    è¿”å›:
        åŒºåŸŸåˆ—è¡¨
    """
    print("\nğŸ” ç­–ç•¥2: å¤šå°ºåº¦é‡‘å­—å¡”é‡‡æ ·")
    
    all_regions = []
    
    for scale in scales:
        # ç¼©æ”¾å›¾åƒ
        h, w = image.shape[:2]
        scaled_h, scaled_w = int(h * scale), int(w * scale)
        
        if len(image.shape) == 3:
            scaled_img = cv2.resize(image, (scaled_w, scaled_h))
            gray = cv2.cvtColor(scaled_img, cv2.COLOR_RGB2GRAY)
        else:
            gray = cv2.resize(image, (scaled_w, scaled_h))
        
        # åœ¨å½“å‰å°ºåº¦ä¸‹é‡‡æ ·
        saliency = cv2.saliency.StaticSaliencySpectralResidual_create()
        _, saliency_map = saliency.computeSaliency(gray)
        saliency_map = (saliency_map * 255).astype(np.uint8)
        
        # è‡ªé€‚åº”é˜ˆå€¼ï¼ˆå°å°ºåº¦ç”¨æ›´ä½é˜ˆå€¼ï¼‰
        threshold = 100 if scale == 1.0 else 70 if scale == 0.5 else 40
        _, binary = cv2.threshold(saliency_map, threshold, 255, cv2.THRESH_BINARY)
        
        # å½¢æ€å­¦
        kernel_size = max(5, int(9 * scale))
        kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (kernel_size, kernel_size))
        binary = cv2.morphologyEx(binary, cv2.MORPH_CLOSE, kernel)
        
        # è¿é€šåŸŸ
        num_labels, labels, stats, centroids = cv2.connectedComponentsWithStats(binary)
        
        # æå–åŒºåŸŸå¹¶æ˜ å°„å›åŸå°ºåº¦
        for i in range(1, num_labels):
            x, y, w_box, h_box, area = stats[i]
            
            # å°ºåº¦ç›¸å…³çš„é¢ç§¯é˜ˆå€¼
            min_area = 200 * (scale ** 2)
            if area > min_area:
                # æ˜ å°„å›åŸå›¾åæ ‡
                x_orig = int(x / scale)
                y_orig = int(y / scale)
                w_orig = int(w_box / scale)
                h_orig = int(h_box / scale)
                
                mask = (labels == i)
                avg_saliency = saliency_map[mask].mean()
                
                all_regions.append({
                    'bbox': (x_orig, y_orig, x_orig + w_orig, y_orig + h_orig),
                    'area': w_orig * h_orig,
                    'saliency': avg_saliency,
                    'scale': scale,
                    'score': avg_saliency * np.log(area + 1)
                })
    
    # å»é‡ï¼ˆNMSï¼‰
    all_regions = non_max_suppression_regions(all_regions, iou_threshold=0.5)
    
    # æ’åº
    all_regions.sort(key=lambda x: x['score'], reverse=True)
    
    # é™åˆ¶æ•°é‡
    if len(all_regions) > max_regions:
        all_regions = all_regions[:max_regions]
    
    print(f"   âœ“ å¤šå°ºåº¦é‡‡æ ·å¾—åˆ° {len(all_regions)} ä¸ªåŒºåŸŸ")
    for scale in scales:
        count = sum(1 for r in all_regions if r.get('scale') == scale)
        print(f"     - Scale {scale}: {count} ä¸ªåŒºåŸŸ")
    
    # è¦†ç›–ç‡æ£€æŸ¥
    all_regions = compute_coverage_and_supplement(image, all_regions)
    
    return all_regions


# ==================== ç­–ç•¥3ï¼šå¤šé˜ˆå€¼æ˜¾è‘—æ€§é‡‡æ ·ï¼ˆè½»é‡çº§ï¼‰ ====================

def multi_threshold_saliency_sampling(
    image: np.ndarray,
    thresholds: List[float] = [0.1, 0.3, 0.5, 0.7],
    max_regions: int = 200,
    min_coverage: float = 0.5
) -> List[Dict]:
    """
    ç­–ç•¥3: å¤šé˜ˆå€¼æ˜¾è‘—æ€§é‡‡æ ·ï¼ˆè½»é‡çº§ï¼‰
    
    æ ¸å¿ƒæ€æƒ³ï¼š
    - åªè®¡ç®—ä¸€æ¬¡æ˜¾è‘—æ€§å›¾
    - ä½¿ç”¨å¤šä¸ªé˜ˆå€¼æ¨¡æ‹Ÿå¤šå°ºåº¦
    - é€šè¿‡é˜ˆå€¼æ•è·ä¸åŒæ˜¾è‘—æ€§æ°´å¹³çš„åŒºåŸŸ
    
    å…·ä½“æ­¥éª¤ï¼š
    1. è®¡ç®—åŸå›¾çš„æ˜¾è‘—æ€§å›¾
    2. ä½¿ç”¨ä¸€ç»„é˜ˆå€¼å¯¹æ˜¾è‘—æ€§å›¾äºŒå€¼åŒ–ï¼Œå¾—åˆ°å¤šä¸ªäºŒå€¼å›¾
    3. ä»æ¯ä¸ªäºŒå€¼å›¾ä¸­æå–è¿é€šåŸŸï¼Œå¹¶è®°å½•å…¶é˜ˆå€¼ï¼ˆå³æ˜¾è‘—æ€§æ°´å¹³ï¼‰
    4. åˆå¹¶æ‰€æœ‰è¿é€šåŸŸï¼Œå¹¶è®¡ç®—æ¯ä¸ªåŒºåŸŸçš„å¹³å‡æ˜¾è‘—æ€§
    5. ä½¿ç”¨NMSå»é‡ï¼Œç„¶åæŒ‰å¹³å‡æ˜¾è‘—æ€§æ’åº
    6. è¦†ç›–åº¦æ£€æŸ¥ï¼Œè¡¥å……æœªè¦†ç›–åŒºåŸŸ
    
    å‚æ•°:
        image: è¾“å…¥å›¾åƒ (RGBæˆ–BGRæ ¼å¼)
        thresholds: æ˜¾è‘—æ€§é˜ˆå€¼åˆ—è¡¨ (0-1ä¹‹é—´)
        max_regions: æœ€å¤§åŒºåŸŸæ•°
        min_coverage: æœ€å°è¦†ç›–ç‡
    
    è¿”å›:
        åŒºåŸŸåˆ—è¡¨
    """
    print("\nğŸ” ç­–ç•¥3: å¤šé˜ˆå€¼æ˜¾è‘—æ€§é‡‡æ ·ï¼ˆè½»é‡çº§ï¼‰")
    
    # è½¬æ¢ä¸ºç°åº¦å›¾
    if len(image.shape) == 3:
        gray = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)
    else:
        gray = image
    
    # 1. è®¡ç®—æ˜¾è‘—æ€§å›¾ï¼ˆåªè®¡ç®—ä¸€æ¬¡ï¼‰
    print("   è®¡ç®—æ˜¾è‘—æ€§å›¾...")
    saliency = cv2.saliency.StaticSaliencySpectralResidual_create()
    success, saliency_map = saliency.computeSaliency(gray)
    
    if not success:
        print("   âŒ æ˜¾è‘—æ€§è®¡ç®—å¤±è´¥")
        return []
    
    # å½’ä¸€åŒ–åˆ° 0-1
    saliency_map = saliency_map.astype(np.float32)
    saliency_map = (saliency_map - saliency_map.min()) / (saliency_map.max() - saliency_map.min() + 1e-8)
    
    # 2. ä½¿ç”¨å¤šä¸ªé˜ˆå€¼æå–åŒºåŸŸ
    all_regions = []
    
    for threshold in thresholds:
        # äºŒå€¼åŒ–
        binary = (saliency_map > threshold).astype(np.uint8) * 255
        
        # å½¢æ€å­¦å¤„ç†ï¼ˆæ ¹æ®é˜ˆå€¼è°ƒæ•´kernelå¤§å°ï¼‰
        kernel_size = 5 if threshold > 0.5 else 9
        kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (kernel_size, kernel_size))
        binary = cv2.morphologyEx(binary, cv2.MORPH_CLOSE, kernel)
        binary = cv2.morphologyEx(binary, cv2.MORPH_OPEN, kernel)
        
        # 3. è¿é€šåŸŸåˆ†æ
        num_labels, labels, stats, centroids = cv2.connectedComponentsWithStats(
            binary, connectivity=8
        )
        
        # æå–åŒºåŸŸ
        for i in range(1, num_labels):
            x, y, w, h, area = stats[i]
            
            # åŠ¨æ€é¢ç§¯é˜ˆå€¼ï¼ˆé«˜é˜ˆå€¼å…è®¸æ›´å°åŒºåŸŸï¼‰
            min_area = 500 if threshold < 0.3 else 300 if threshold < 0.5 else 150
            
            if area > min_area:
                # 4. è®¡ç®—è¯¥åŒºåŸŸçš„å¹³å‡æ˜¾è‘—æ€§ï¼ˆä½¿ç”¨åŸæ˜¾è‘—æ€§å›¾ï¼‰
                mask = (labels == i)
                avg_saliency = saliency_map[mask].mean()
                
                all_regions.append({
                    'bbox': (x, y, x+w, y+h),
                    'area': area,
                    'saliency': avg_saliency,
                    'threshold': threshold,
                    'score': avg_saliency * np.log(area + 1)
                })
    
    print(f"   âœ“ æå–åˆ° {len(all_regions)} ä¸ªåˆå§‹åŒºåŸŸ")
    
    # 5. NMSå»é‡
    print("   æ‰§è¡ŒNMSå»é‡...")
    all_regions = non_max_suppression_regions(all_regions, iou_threshold=0.5)
    
    # æŒ‰å¹³å‡æ˜¾è‘—æ€§æ’åº
    all_regions.sort(key=lambda x: x['score'], reverse=True)
    
    # é™åˆ¶æ•°é‡
    if len(all_regions) > max_regions:
        all_regions = all_regions[:max_regions]
    
    print(f"   âœ“ NMSåä¿ç•™ {len(all_regions)} ä¸ªåŒºåŸŸ")
    for threshold in thresholds:
        count = sum(1 for r in all_regions if r.get('threshold') == threshold)
        print(f"     - Threshold {threshold}: {count} ä¸ªåŒºåŸŸ")
    
    # 6. è¦†ç›–åº¦æ£€æŸ¥å’Œè¡¥å……
    all_regions = compute_coverage_and_supplement(image, all_regions, min_coverage)
    
    return all_regions


# ==================== ç»Ÿä¸€æ¥å£ ====================

def sample_regions(
    image: np.ndarray,
    strategy: str = "multi_threshold_saliency",
    **kwargs
) -> List[Dict]:
    """
    ç»Ÿä¸€çš„åŒºåŸŸé‡‡æ ·æ¥å£
    
    å‚æ•°:
        image: è¾“å…¥å›¾åƒ
        strategy: é‡‡æ ·ç­–ç•¥
            - "layered": å¤šé˜ˆå€¼åˆ†å±‚é‡‡æ ·
            - "pyramid": å¤šå°ºåº¦é‡‘å­—å¡”é‡‡æ ·
            - "multi_threshold_saliency": å¤šé˜ˆå€¼æ˜¾è‘—æ€§é‡‡æ ·ï¼ˆé»˜è®¤ï¼Œè½»é‡çº§ï¼‰
        **kwargs: ä¼ é€’ç»™å…·ä½“ç­–ç•¥çš„å‚æ•°
    
    è¿”å›:
        åŒºåŸŸåˆ—è¡¨
    """
    if strategy == "layered":
        return multi_threshold_layered_sampling(image, **kwargs)
    elif strategy == "pyramid":
        return multi_scale_pyramid_sampling(image, **kwargs)
    elif strategy == "multi_threshold_saliency":
        return multi_threshold_saliency_sampling(image, **kwargs)
    else:
        raise ValueError(f"Unknown strategy: {strategy}")


if __name__ == "__main__":
    # æµ‹è¯•ä»£ç 
    print("æµ‹è¯•é‡‡æ ·ç­–ç•¥...")
    
    # åˆ›å»ºæµ‹è¯•å›¾åƒ
    test_image = np.random.randint(0, 255, (512, 512, 3), dtype=np.uint8)
    
    # æµ‹è¯•ä¸‰ç§ç­–ç•¥
    for strategy in ["layered", "pyramid", "multi_threshold_saliency"]:
        print(f"\n{'='*70}")
        print(f"æµ‹è¯•ç­–ç•¥: {strategy}")
        print(f"{'='*70}")
        
        regions = sample_regions(test_image, strategy=strategy, max_regions=50)
        print(f"æœ€ç»ˆå¾—åˆ° {len(regions)} ä¸ªåŒºåŸŸ\n")

