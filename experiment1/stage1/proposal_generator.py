#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Stage1: å€™é€‰æ¡†ç”Ÿæˆæ¨¡å—
åŸºäºé‡‡æ ·åŒºåŸŸç”Ÿæˆå€™é€‰æ£€æµ‹æ¡†
"""

import cv2
import numpy as np
from typing import List, Dict, Tuple, Optional
import torch
import open_clip
from PIL import Image
import os
import sys

# æ·»åŠ çˆ¶ç›®å½•åˆ°è·¯å¾„ä»¥å¯¼å…¥åŸæœ‰æ¨¡å—
sys.path.append(os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__)))))


class ProposalGenerator:
    """å€™é€‰æ¡†ç”Ÿæˆå™¨"""
    
    def __init__(self, model_name: str = 'RN50', device: str = 'cuda'):
        """
        åˆå§‹åŒ–å€™é€‰æ¡†ç”Ÿæˆå™¨
        
        å‚æ•°:
            model_name: RemoteCLIPæ¨¡å‹åç§°
            device: è®¡ç®—è®¾å¤‡
        """
        self.device = device if torch.cuda.is_available() else 'cpu'
        self.model_name = model_name
        self.model = None
        self.preprocess = None
        self.tokenizer = None
        
        # åˆå§‹åŒ–æ¨¡å‹
        self._load_model()
    
    def _load_model(self):
        """åŠ è½½RemoteCLIPæ¨¡å‹"""
        print(f"ğŸ”„ åŠ è½½RemoteCLIPæ¨¡å‹: {self.model_name}")
        
        self.model, _, self.preprocess = open_clip.create_model_and_transforms(self.model_name)
        self.tokenizer = open_clip.get_tokenizer(self.model_name)
        
        checkpoint_path = f"checkpoints/RemoteCLIP-{self.model_name}.pt"
        ckpt = torch.load(checkpoint_path, map_location="cpu")
        self.model.load_state_dict(ckpt)
        self.model = self.model.to(self.device).eval()
        
        print(f"âœ… æ¨¡å‹å·²åŠ è½½åˆ° {self.device}")
    
    def generate_proposals_from_regions(self, image: np.ndarray, regions: List[Dict]) -> List[Dict]:
        """
        ä»é‡‡æ ·åŒºåŸŸç”Ÿæˆå€™é€‰æ¡†
        
        å‚æ•°:
            image: è¾“å…¥å›¾åƒ
            regions: é‡‡æ ·åŒºåŸŸåˆ—è¡¨
        
        è¿”å›:
            å€™é€‰æ¡†åˆ—è¡¨
        """
        print(f"\nğŸ”§ ä» {len(regions)} ä¸ªåŒºåŸŸç”Ÿæˆå€™é€‰æ¡†...")
        
        proposals = []
        
        for idx, region in enumerate(regions):
            # è·å–åŒºåŸŸè¾¹ç•Œæ¡†
            x1, y1, x2, y2 = region['bbox']
            
            # ç¡®ä¿åæ ‡åœ¨æœ‰æ•ˆèŒƒå›´å†…
            x1, y1 = max(0, int(x1)), max(0, int(y1))
            x2, y2 = min(image.shape[1], int(x2)), min(image.shape[0], int(y2))
            
            # è£å‰ªåŒºåŸŸ
            crop = image[y1:y2, x1:x2]
            
            # æ£€æŸ¥æœ‰æ•ˆæ€§
            if crop.size == 0 or crop.shape[0] < 10 or crop.shape[1] < 10:
                continue
            
            # åˆ›å»ºå€™é€‰æ¡†
            proposal = {
                'proposal_id': idx,
                'bbox': (x1, y1, x2, y2),
                'region_info': region,
                'crop': crop,
                'area': (x2 - x1) * (y2 - y1),
                'aspect_ratio': (x2 - x1) / (y2 - y1) if (y2 - y1) > 0 else 1.0,
                'center': ((x1 + x2) // 2, (y1 + y2) // 2),
                'confidence': 0.0,  # å°†åœ¨åç»­é˜¶æ®µè®¡ç®—
                'features': None,   # å°†åœ¨åç»­é˜¶æ®µè®¡ç®—
                'category': 'unknown'  # å°†åœ¨åˆ†ç±»é˜¶æ®µç¡®å®š
            }
            
            proposals.append(proposal)
        
        print(f"âœ… ç”Ÿæˆ {len(proposals)} ä¸ªå€™é€‰æ¡†")
        
        return proposals
    
    def refine_proposals(self, proposals: List[Dict], 
                        min_area: int = 100,
                        max_area: int = 50000,
                        min_aspect_ratio: float = 0.1,
                        max_aspect_ratio: float = 10.0) -> List[Dict]:
        """
        ç²¾åŒ–å€™é€‰æ¡†
        
        å‚æ•°:
            proposals: å€™é€‰æ¡†åˆ—è¡¨
            min_area: æœ€å°é¢ç§¯
            max_area: æœ€å¤§é¢ç§¯
            min_aspect_ratio: æœ€å°å®½é«˜æ¯”
            max_aspect_ratio: æœ€å¤§å®½é«˜æ¯”
        
        è¿”å›:
            ç²¾åŒ–åçš„å€™é€‰æ¡†åˆ—è¡¨
        """
        print(f"\nğŸ”§ ç²¾åŒ–å€™é€‰æ¡†...")
        
        refined_proposals = []
        
        for proposal in proposals:
            area = proposal['area']
            aspect_ratio = proposal['aspect_ratio']
            
            # é¢ç§¯è¿‡æ»¤
            if area < min_area or area > max_area:
                continue
            
            # å®½é«˜æ¯”è¿‡æ»¤
            if aspect_ratio < min_aspect_ratio or aspect_ratio > max_aspect_ratio:
                continue
            
            refined_proposals.append(proposal)
        
        print(f"âœ… ç²¾åŒ–åä¿ç•™: {len(proposals)} -> {len(refined_proposals)} ä¸ªå€™é€‰æ¡†")
        
        return refined_proposals
    
    def extract_proposal_features(self, proposals: List[Dict]) -> List[Dict]:
        """
        æå–å€™é€‰æ¡†ç‰¹å¾
        
        å‚æ•°:
            proposals: å€™é€‰æ¡†åˆ—è¡¨
        
        è¿”å›:
            å¸¦ç‰¹å¾çš„å€™é€‰æ¡†åˆ—è¡¨
        """
        print(f"\nğŸ”§ æå– {len(proposals)} ä¸ªå€™é€‰æ¡†çš„ç‰¹å¾...")
        
        for proposal in proposals:
            crop = proposal['crop']
            
            # è½¬æ¢ä¸ºPILå›¾åƒå¹¶é¢„å¤„ç†
            crop_pil = Image.fromarray(crop)
            crop_tensor = self.preprocess(crop_pil).unsqueeze(0)
            
            # æå–ç‰¹å¾
            with torch.no_grad():
                features = self.model.encode_image(crop_tensor.to(self.device))
                features /= features.norm(dim=-1, keepdim=True)
                
                proposal['features'] = features.cpu().numpy()
        
        print(f"âœ… ç‰¹å¾æå–å®Œæˆ")
        
        return proposals
    
    def compute_proposal_similarities(self, proposals: List[Dict], 
                                    query_classes: List[str]) -> List[Dict]:
        """
        è®¡ç®—å€™é€‰æ¡†ä¸æŸ¥è¯¢ç±»åˆ«çš„ç›¸ä¼¼åº¦
        
        å‚æ•°:
            proposals: å€™é€‰æ¡†åˆ—è¡¨
            query_classes: æŸ¥è¯¢ç±»åˆ«åˆ—è¡¨
        
        è¿”å›:
            å¸¦ç›¸ä¼¼åº¦çš„å€™é€‰æ¡†åˆ—è¡¨
        """
        print(f"\nğŸ”§ è®¡ç®—ä¸ {len(query_classes)} ä¸ªç±»åˆ«çš„ç›¸ä¼¼åº¦...")
        
        # ç¼–ç æŸ¥è¯¢ç±»åˆ«
        query_tokens = self.tokenizer(query_classes)
        with torch.no_grad():
            query_features = self.model.encode_text(query_tokens.to(self.device))
            query_features /= query_features.norm(dim=-1, keepdim=True)
        
        # è®¡ç®—ç›¸ä¼¼åº¦
        for proposal in proposals:
            if proposal['features'] is None:
                continue
            
            features = torch.from_numpy(proposal['features']).to(self.device)
            
            # è®¡ç®—ç›¸ä¼¼åº¦
            similarities = (features @ query_features.T).squeeze(0).cpu().numpy()
            
            # æ‰¾åˆ°æœ€ç›¸ä¼¼çš„ç±»åˆ«
            best_idx = similarities.argmax()
            best_class = query_classes[best_idx]
            best_similarity = similarities[best_idx]
            
            proposal['similarities'] = similarities
            proposal['best_class'] = best_class
            proposal['best_similarity'] = best_similarity
            proposal['confidence'] = best_similarity
        
        print(f"âœ… ç›¸ä¼¼åº¦è®¡ç®—å®Œæˆ")
        
        return proposals
    
    def generate_proposals_pipeline(self, image: np.ndarray, regions: List[Dict],
                                  query_classes: List[str] = None,
                                  **kwargs) -> List[Dict]:
        """
        å®Œæ•´çš„å€™é€‰æ¡†ç”Ÿæˆæµæ°´çº¿
        
        å‚æ•°:
            image: è¾“å…¥å›¾åƒ
            regions: é‡‡æ ·åŒºåŸŸåˆ—è¡¨
            query_classes: æŸ¥è¯¢ç±»åˆ«åˆ—è¡¨
            **kwargs: å…¶ä»–å‚æ•°
        
        è¿”å›:
            å®Œæ•´çš„å€™é€‰æ¡†åˆ—è¡¨
        """
        print(f"\nğŸš€ å¼€å§‹å€™é€‰æ¡†ç”Ÿæˆæµæ°´çº¿...")
        
        # 1. ä»åŒºåŸŸç”Ÿæˆå€™é€‰æ¡†
        proposals = self.generate_proposals_from_regions(image, regions)
        
        # 2. ç²¾åŒ–å€™é€‰æ¡†
        proposals = self.refine_proposals(proposals, **kwargs)
        
        # 3. æå–ç‰¹å¾
        proposals = self.extract_proposal_features(proposals)
        
        # 4. è®¡ç®—ç›¸ä¼¼åº¦ï¼ˆå¦‚æœæä¾›äº†æŸ¥è¯¢ç±»åˆ«ï¼‰
        if query_classes:
            proposals = self.compute_proposal_similarities(proposals, query_classes)
        
        print(f"âœ… å€™é€‰æ¡†ç”Ÿæˆæµæ°´çº¿å®Œæˆï¼Œå¾—åˆ° {len(proposals)} ä¸ªå€™é€‰æ¡†")
        
        return proposals
    
    def get_proposal_statistics(self, proposals: List[Dict]) -> Dict:
        """
        è·å–å€™é€‰æ¡†ç»Ÿè®¡ä¿¡æ¯
        
        å‚æ•°:
            proposals: å€™é€‰æ¡†åˆ—è¡¨
        
        è¿”å›:
            ç»Ÿè®¡ä¿¡æ¯å­—å…¸
        """
        if not proposals:
            return {}
        
        areas = [p['area'] for p in proposals]
        aspect_ratios = [p['aspect_ratio'] for p in proposals]
        confidences = [p.get('confidence', 0) for p in proposals]
        
        stats = {
            'total_proposals': len(proposals),
            'area_stats': {
                'mean': np.mean(areas),
                'std': np.std(areas),
                'min': np.min(areas),
                'max': np.max(areas)
            },
            'aspect_ratio_stats': {
                'mean': np.mean(aspect_ratios),
                'std': np.std(aspect_ratios),
                'min': np.min(aspect_ratios),
                'max': np.max(aspect_ratios)
            },
            'confidence_stats': {
                'mean': np.mean(confidences),
                'std': np.std(confidences),
                'min': np.min(confidences),
                'max': np.max(confidences)
            }
        }
        
        return stats


def main():
    """æµ‹è¯•å€™é€‰æ¡†ç”Ÿæˆå™¨"""
    print("=" * 70)
    print("æµ‹è¯•å€™é€‰æ¡†ç”Ÿæˆå™¨")
    print("=" * 70)
    
    # æµ‹è¯•å›¾åƒ
    test_image_path = "assets/airport.jpg"
    if not os.path.exists(test_image_path):
        print(f"âŒ æµ‹è¯•å›¾åƒä¸å­˜åœ¨: {test_image_path}")
        return
    
    # åŠ è½½æµ‹è¯•å›¾åƒ
    image = cv2.imread(test_image_path)
    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
    
    # åˆ›å»ºæ¨¡æ‹ŸåŒºåŸŸ
    h, w = image.shape[:2]
    mock_regions = [
        {'bbox': (100, 100, 200, 200), 'score': 0.8, 'saliency': 0.7},
        {'bbox': (300, 200, 400, 300), 'score': 0.6, 'saliency': 0.5},
        {'bbox': (150, 250, 250, 350), 'score': 0.9, 'saliency': 0.8},
    ]
    
    # åˆ›å»ºå€™é€‰æ¡†ç”Ÿæˆå™¨
    generator = ProposalGenerator()
    
    # æŸ¥è¯¢ç±»åˆ«
    query_classes = ['airplane', 'building', 'runway', 'vehicle']
    
    # è¿è¡Œå®Œæ•´æµæ°´çº¿
    proposals = generator.generate_proposals_pipeline(
        image, 
        mock_regions, 
        query_classes
    )
    
    # è·å–ç»Ÿè®¡ä¿¡æ¯
    stats = generator.get_proposal_statistics(proposals)
    print(f"\nğŸ“Š å€™é€‰æ¡†ç»Ÿè®¡:")
    print(f"  æ€»å€™é€‰æ¡†æ•°: {stats.get('total_proposals', 0)}")
    print(f"  é¢ç§¯èŒƒå›´: {stats.get('area_stats', {}).get('min', 0)} - {stats.get('area_stats', {}).get('max', 0)}")
    print(f"  ç½®ä¿¡åº¦èŒƒå›´: {stats.get('confidence_stats', {}).get('min', 0):.3f} - {stats.get('confidence_stats', {}).get('max', 0):.3f}")
    
    # æ˜¾ç¤ºæ¯ä¸ªå€™é€‰æ¡†çš„ä¿¡æ¯
    print(f"\nğŸ“‹ å€™é€‰æ¡†è¯¦æƒ…:")
    for i, proposal in enumerate(proposals):
        print(f"  å€™é€‰æ¡† {i+1}:")
        print(f"    ä½ç½®: {proposal['bbox']}")
        print(f"    é¢ç§¯: {proposal['area']}")
        print(f"    æœ€ä½³ç±»åˆ«: {proposal.get('best_class', 'unknown')}")
        print(f"    ç½®ä¿¡åº¦: {proposal.get('confidence', 0):.3f}")
    
    print("\nâœ… å€™é€‰æ¡†ç”Ÿæˆå™¨æµ‹è¯•å®Œæˆ!")


if __name__ == "__main__":
    main()
