#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
RemoteCLIP å¢å¼ºç¤ºä¾‹
ä½¿ç”¨æœºåœºå›¾ç‰‡è¿›è¡Œå›¾åƒ-æ–‡æœ¬åŒ¹é…æ¼”ç¤º
æ”¯æŒï¼šå…¨å›¾åŒ¹é… + å¤šç§åŒºåŸŸé‡‡æ ·ç­–ç•¥
"""

import torch
import open_clip
from PIL import Image
import os
import numpy as np
import argparse

# å¯¼å…¥é‡‡æ ·ç­–ç•¥æ¨¡å—
from sampling import sample_regions

def main():
    # è§£æå‘½ä»¤è¡Œå‚æ•°
    parser = argparse.ArgumentParser(description='RemoteCLIP åŒºåŸŸé‡‡æ ·æ¼”ç¤º')
    parser.add_argument('--strategy', type=str, default='multi_threshold_saliency',
                        choices=['layered', 'pyramid', 'multi_threshold_saliency'],
                        help='é‡‡æ ·ç­–ç•¥: layered(åˆ†å±‚), pyramid(é‡‘å­—å¡”), multi_threshold_saliency(å¤šé˜ˆå€¼)')
    parser.add_argument('--model', type=str, default='RN50',
                        choices=['RN50', 'ViT-B-32', 'ViT-L-14'],
                        help='æ¨¡å‹é€‰æ‹©')
    parser.add_argument('--image', type=str, default='assets/airport.jpg',
                        help='å›¾åƒè·¯å¾„')
    parser.add_argument('--max-regions', type=int, default=50,
                        help='æœ€å¤§åŒºåŸŸæ•°')
    parser.add_argument('--top-k', type=int, default=10,
                        help='åˆ†æå‰Kä¸ªé‡è¦åŒºåŸŸ')
    parser.add_argument('--no-region-sampling', action='store_true',
                        help='ç¦ç”¨åŒºåŸŸé‡‡æ ·ï¼Œåªè¿›è¡Œå…¨å›¾åŒ¹é…')
    
    args = parser.parse_args()
    
    print("=" * 70)
    print("RemoteCLIP å¢å¼ºç¤ºä¾‹ - å›¾åƒ-æ–‡æœ¬åŒ¹é…")
    print("=" * 70)
    
    # 1. æ£€æŸ¥ç¯å¢ƒ
    print(f"\nğŸ“‹ ç¯å¢ƒä¿¡æ¯:")
    print(f"   PyTorchç‰ˆæœ¬: {torch.__version__}")
    print(f"   CUDAå¯ç”¨: {torch.cuda.is_available()}")
    print(f"   OpenCLIPç‰ˆæœ¬: {open_clip.__version__}")
    print(f"\nğŸ“‹ è¿è¡Œé…ç½®:")
    print(f"   æ¨¡å‹: {args.model}")
    print(f"   å›¾åƒ: {args.image}")
    print(f"   é‡‡æ ·ç­–ç•¥: {args.strategy}")
    print(f"   æœ€å¤§åŒºåŸŸæ•°: {args.max_regions}")
    print(f"   åˆ†æåŒºåŸŸæ•°: {args.top_k}")
    
    # 2. åŠ è½½æ¨¡å‹
    model_name = args.model
    print(f"\nğŸ”„ æ­£åœ¨åŠ è½½ {model_name} æ¨¡å‹...")
    
    model, _, preprocess = open_clip.create_model_and_transforms(model_name)
    tokenizer = open_clip.get_tokenizer(model_name)
    
    # åŠ è½½é¢„è®­ç»ƒæƒé‡
    checkpoint_path = f"checkpoints/RemoteCLIP-{model_name}.pt"
    if not os.path.exists(checkpoint_path):
        print(f"âŒ é”™è¯¯: æ‰¾ä¸åˆ°æ¨¡å‹æ–‡ä»¶ {checkpoint_path}")
        print("è¯·ç¡®ä¿æƒé‡æ–‡ä»¶åœ¨checkpointsç›®å½•ä¸‹")
        return
    
    ckpt = torch.load(checkpoint_path, map_location="cpu")
    message = model.load_state_dict(ckpt)
    print(f"âœ… æ¨¡å‹åŠ è½½çŠ¶æ€: {message}")
    
    # å°†æ¨¡å‹ç§»åˆ°GPUå¹¶è®¾ç½®ä¸ºè¯„ä¼°æ¨¡å¼
    device = "cuda" if torch.cuda.is_available() else "cpu"
    model = model.to(device).eval()
    print(f"âœ… æ¨¡å‹å·²åŠ è½½åˆ° {device} è®¾å¤‡")
    
    # 3. åŠ è½½å›¾ç‰‡
    image_path = args.image
    print(f"\nğŸ–¼ï¸  åŠ è½½å›¾ç‰‡: {image_path}")
    
    if not os.path.exists(image_path):
        print(f"âŒ é”™è¯¯: æ‰¾ä¸åˆ°å›¾ç‰‡æ–‡ä»¶ {image_path}")
        return
    
    pil_image = Image.open(image_path)
    cv_image = np.array(pil_image)  # è½¬æ¢ä¸ºnumpyæ•°ç»„ä¾›é‡‡æ ·ä½¿ç”¨
    print(f"âœ… å›¾ç‰‡å°ºå¯¸: {pil_image.size}")
    
    # 4. å…¨å›¾åŒ¹é…
    print(f"\n" + "=" * 70)
    print("ğŸ“Š æ­¥éª¤1: å…¨å›¾åŒ¹é…")
    print("=" * 70)
    
    text_queries = [
        "A busy airport with many airplanes.",
        "Satellite view of Hohai university.",
        "A building next to a lake.",
        "Many people in a stadium.",
        "A cute cat",
    ]
    
    print(f"\næ–‡æœ¬æŸ¥è¯¢åˆ—è¡¨:")
    for i, query in enumerate(text_queries, 1):
        print(f"   {i}. {query}")
    
    text = tokenizer(text_queries)
    image_tensor = preprocess(pil_image).unsqueeze(0)
    
    with torch.no_grad():
        # ç¼–ç å›¾åƒå’Œæ–‡æœ¬
        image_features = model.encode_image(image_tensor.to(device))
        text_features = model.encode_text(text.to(device))
        
        # L2å½’ä¸€åŒ–
        image_features /= image_features.norm(dim=-1, keepdim=True)
        text_features /= text_features.norm(dim=-1, keepdim=True)
        
        # è®¡ç®—ç›¸ä¼¼åº¦æ¦‚ç‡
        text_probs = (100.0 * image_features @ text_features.T).softmax(dim=-1).cpu().numpy()[0]
    
    # æ˜¾ç¤ºç»“æœ
    print(f"\nğŸ¯ å…¨å›¾åŒ¹é…ç»“æœ:")
    for query, prob in zip(text_queries, text_probs):
        bar_length = int(prob * 50)
        bar = "â–ˆ" * bar_length + "â–‘" * (50 - bar_length)
        print(f"{query:<45} {prob * 100:5.1f}% {bar}")
    
    best_match_idx = text_probs.argmax()
    print(f"\nğŸ† æœ€ä½³åŒ¹é…: {text_queries[best_match_idx]} ({text_probs[best_match_idx] * 100:.2f}%)")
    
    # 5. åŒºåŸŸé‡‡æ ·åŒ¹é…
    if not args.no_region_sampling:
        print(f"\n" + "=" * 70)
        print("ğŸ“Š æ­¥éª¤2: åŒºåŸŸé‡‡æ ·åŒ¹é…")
        print("=" * 70)
        
        # ä½¿ç”¨é€‰å®šçš„ç­–ç•¥è¿›è¡ŒåŒºåŸŸé‡‡æ ·
        regions = sample_regions(
            cv_image, 
            strategy=args.strategy,
            max_regions=args.max_regions
        )
        
        if len(regions) == 0:
            print("âŒ æœªæ‰¾åˆ°ä»»ä½•åŒºåŸŸ")
        else:
            # å®šä¹‰åŒºåŸŸçº§åˆ«çš„æŸ¥è¯¢
            region_queries = [
                "airport", "runway", "airplane", "aircraft", 
                "building", "terminal", "parking lot", "road",
                "vegetation", "water"
            ]
            
            print(f"\nåˆ†æå‰ {min(args.top_k, len(regions))} ä¸ªé‡è¦åŒºåŸŸ...")
            
            region_text = tokenizer(region_queries)
            with torch.no_grad():
                region_text_features = model.encode_text(region_text.to(device))
                region_text_features /= region_text_features.norm(dim=-1, keepdim=True)
            
            # å¯¹æ¯ä¸ªåŒºåŸŸè¿›è¡ŒåŒ¹é…
            for idx, region in enumerate(regions[:args.top_k]):
                x1, y1, x2, y2 = region['bbox']
                
                # ç¡®ä¿åæ ‡åœ¨æœ‰æ•ˆèŒƒå›´å†…
                x1, y1 = max(0, x1), max(0, y1)
                x2, y2 = min(cv_image.shape[1], x2), min(cv_image.shape[0], y2)
                
                # è£å‰ªåŒºåŸŸ
                cropped = cv_image[y1:y2, x1:x2]
                if cropped.size == 0 or cropped.shape[0] < 10 or cropped.shape[1] < 10:
                    continue
                
                cropped_pil = Image.fromarray(cropped)
                cropped_tensor = preprocess(cropped_pil).unsqueeze(0)
                
                with torch.no_grad():
                    crop_features = model.encode_image(cropped_tensor.to(device))
                    crop_features /= crop_features.norm(dim=-1, keepdim=True)
                    
                    probs = (100.0 * crop_features @ region_text_features.T).softmax(dim=-1).cpu().numpy()[0]
                
                # è·å–top3ç»“æœ
                top3_indices = probs.argsort()[-3:][::-1]
                
                print(f"\nåŒºåŸŸ {idx+1} [{x1},{y1},{x2},{y2}] - ä¼˜å…ˆçº§: {region.get('priority', region.get('threshold', 'N/A'))}")
                print(f"  Top3åŒ¹é…:")
                for rank, top_idx in enumerate(top3_indices, 1):
                    print(f"    {rank}. {region_queries[top_idx]}: {probs[top_idx]*100:.1f}%")
    
    # 6. å®Œæˆ
    print(f"\n" + "=" * 70)
    print(f"âœ… æ¼”ç¤ºå®Œæˆï¼")
    print("=" * 70)
    print(f"\nğŸ’¡ ä½¿ç”¨æç¤º:")
    print(f"   python demo_simple.py --help                    # æŸ¥çœ‹æ‰€æœ‰å‚æ•°")
    print(f"   python demo_simple.py --strategy pyramid        # ä½¿ç”¨é‡‘å­—å¡”é‡‡æ ·")
    print(f"   python demo_simple.py --strategy layered        # ä½¿ç”¨åˆ†å±‚é‡‡æ ·")
    print(f"   python demo_simple.py --model ViT-B-32          # ä½¿ç”¨ViT-B-32æ¨¡å‹")
    print(f"   python demo_simple.py --image your_image.jpg    # ä½¿ç”¨è‡ªå®šä¹‰å›¾ç‰‡")
    print(f"   python demo_simple.py --no-region-sampling      # åªè¿›è¡Œå…¨å›¾åŒ¹é…")

if __name__ == "__main__":
    main()

