#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ç›®æ ‡æ£€æµ‹è„šæœ¬ï¼ˆé¥æ„Ÿé€šç”¨ï¼‰
è¾“å…¥ç›®æ ‡ç±»åˆ«ï¼ˆå¦‚shipï¼‰ï¼Œè¾“å‡ºå¾—åˆ†æœ€é«˜çš„é‚£ä¸€ä¸ªæ¡†
"""

import torch
import open_clip
from PIL import Image
import numpy as np
import cv2
import argparse
from typing import List, Dict

from sampling import sample_regions
from wordnet_vocabulary import get_expansion_words, get_synonyms, WORDNET_REMOTE_SENSING_CLASSES
from output_manager import get_output_manager


def detect_target_with_contrastive_learning(
    image_path: str,
    target_class: str,
    model_name: str = 'RN50',
    strategy: str = 'multi_threshold_saliency',
    max_regions: int = 50,
    output_path: str = None
):
    """
    ä½¿ç”¨å¯¹æ¯”å­¦ä¹ æ£€æµ‹ç›®æ ‡ç±»åˆ«
    
    æµç¨‹:
    1. åŒºåŸŸé‡‡æ · â†’ å€™é€‰æ¡†
    2. å®šä¹‰ç±»åˆ«:
       - ç›®æ ‡ç±» + è¿‘ä¹‰è¯ â†’ æ­£æ ·æœ¬å‚è€ƒ
       - å…¶ä»–100ç±» â†’ è´Ÿæ ·æœ¬å‚è€ƒ
    3. æ‰¹é‡æ¨ç† â†’ ç›¸ä¼¼åº¦çŸ©é˜µ
    4. å¯¹æ¯”å­¦ä¹  â†’ æ‰¾å‡ºä¸ç›®æ ‡ç±»æœ€ç›¸ä¼¼çš„æ¡†
    5. è¾“å‡ºå¾—åˆ†æœ€é«˜çš„1ä¸ªæ¡†
    
    å‚æ•°:
        image_path: è¾“å…¥å›¾åƒ
        target_class: ç›®æ ‡ç±»åˆ«ï¼ˆå¦‚ "ship", "airplane"ç­‰ï¼‰
        model_name: æ¨¡å‹åç§°
        strategy: é‡‡æ ·ç­–ç•¥
        max_regions: æœ€å¤§åŒºåŸŸæ•°
        output_path: è¾“å‡ºå›¾åƒè·¯å¾„
    """
    print("=" * 70)
    print(f"ğŸ¯ ç›®æ ‡æ£€æµ‹: {target_class}")
    print("=" * 70)
    
    device = "cuda" if torch.cuda.is_available() else "cpu"
    
    # 1. åŠ è½½æ¨¡å‹
    print(f"\nğŸ”„ åŠ è½½RemoteCLIPæ¨¡å‹: {model_name}")
    model, _, preprocess = open_clip.create_model_and_transforms(model_name)
    tokenizer = open_clip.get_tokenizer(model_name)
    
    checkpoint_path = f"checkpoints/RemoteCLIP-{model_name}.pt"
    ckpt = torch.load(checkpoint_path, map_location="cpu")
    model.load_state_dict(ckpt)
    model = model.to(device).eval()
    print(f"âœ… æ¨¡å‹å·²åŠ è½½")
    
    # 2. åŠ è½½å›¾åƒ
    pil_image = Image.open(image_path)
    cv_image = np.array(pil_image)
    print(f"âœ… å›¾åƒå·²åŠ è½½: {cv_image.shape}")
    
    # 3. å®šä¹‰ç±»åˆ«ä½“ç³»
    # æ­£æ ·æœ¬å‚è€ƒï¼šç›®æ ‡ç±» + è¿‘ä¹‰è¯
    target_synonyms = get_synonyms(target_class)
    if target_synonyms:
        positive_classes = [target_class] + target_synonyms[:4]  # ç›®æ ‡ç±»+æœ€å¤š4ä¸ªè¿‘ä¹‰è¯
    else:
        positive_classes = [target_class]
    
    # è´Ÿæ ·æœ¬å‚è€ƒï¼š100ä¸ªåŸºç¡€ç±»åˆ«ï¼ˆæ’é™¤ç›®æ ‡ç±»åŠå…¶è¿‘ä¹‰è¯ï¼‰
    negative_classes = [c for c in WORDNET_REMOTE_SENSING_CLASSES 
                        if c not in positive_classes]
    
    # æ‰€æœ‰ç±»åˆ«
    all_classes = positive_classes + negative_classes
    n_positive = len(positive_classes)
    
    print(f"\nğŸ“‹ ç±»åˆ«è®¾ç½®:")
    print(f"   ç›®æ ‡ç±»: {target_class}")
    print(f"   æ­£æ ·æœ¬å‚è€ƒ ({n_positive}ä¸ª): {positive_classes}")
    print(f"   è´Ÿæ ·æœ¬å‚è€ƒ: {len(negative_classes)}ä¸ªåŸºç¡€ç±»åˆ«")
    print(f"   æ€»ç±»åˆ«æ•°: {len(all_classes)}")
    
    # 4. é¢„ç¼–ç æ–‡æœ¬ç‰¹å¾
    print(f"\nğŸ”„ ç¼–ç æ–‡æœ¬ç‰¹å¾...")
    text = tokenizer(all_classes)
    with torch.no_grad():
        text_features = model.encode_text(text.to(device))
        text_features /= text_features.norm(dim=-1, keepdim=True)
    
    # åˆ†ç¦»æ­£è´Ÿæ ·æœ¬ç‰¹å¾
    positive_text_features = text_features[:n_positive]  # å‰nä¸ªæ˜¯æ­£æ ·æœ¬
    negative_text_features = text_features[n_positive:]  # åé¢æ˜¯è´Ÿæ ·æœ¬
    
    print(f"âœ… æ–‡æœ¬ç‰¹å¾å·²ç¼–ç ")
    
    # 5. åŒºåŸŸé‡‡æ ·
    print(f"\nğŸ” Step 1: åŒºåŸŸé‡‡æ ·")
    regions = sample_regions(cv_image, strategy=strategy, max_regions=max_regions)
    print(f"âœ… æå–åˆ° {len(regions)} ä¸ªå€™é€‰åŒºåŸŸ")
    
    # 6. å¯¹æ¯ä¸ªåŒºåŸŸè®¡ç®—å¯¹æ¯”å¾—åˆ†
    print(f"\nğŸ”„ Step 2: å¯¹æ¯”å­¦ä¹ è¯„åˆ†...")
    results = []
    
    for idx, region in enumerate(regions):
        x1, y1, x2, y2 = region['bbox']
        x1, y1 = max(0, x1), max(0, y1)
        x2, y2 = min(cv_image.shape[1], x2), min(cv_image.shape[0], y2)
        
        # è£å‰ª
        crop = cv_image[y1:y2, x1:x2]
        if crop.size == 0 or crop.shape[0] < 10 or crop.shape[1] < 10:
            continue
        
        crop_pil = Image.fromarray(crop)
        crop_tensor = preprocess(crop_pil).unsqueeze(0)
        
        # ç¼–ç 
        with torch.no_grad():
            image_features = model.encode_image(crop_tensor.to(device))
            image_features /= image_features.norm(dim=-1, keepdim=True)
            
            # è®¡ç®—ä¸æ‰€æœ‰ç±»åˆ«çš„ç›¸ä¼¼åº¦
            similarities = (image_features @ text_features.T).squeeze(0).cpu().numpy()
            
            # æ­£æ ·æœ¬å¾—åˆ†ï¼ˆä¸ç›®æ ‡ç±»+è¿‘ä¹‰è¯çš„ç›¸ä¼¼åº¦ï¼‰
            positive_scores = similarities[:n_positive]
            positive_avg = positive_scores.mean()
            positive_max = positive_scores.max()
            best_positive_idx = positive_scores.argmax()
            
            # è´Ÿæ ·æœ¬å¾—åˆ†ï¼ˆä¸å…¶ä»–ç±»åˆ«çš„ç›¸ä¼¼åº¦ï¼‰
            negative_scores = similarities[n_positive:]
            negative_avg = negative_scores.mean()
            negative_max = negative_scores.max()
            
            # æ”¹è¿›çš„å¯¹æ¯”å¾—åˆ†è®¡ç®—
            # 1. åŸºç¡€å¯¹æ¯”å¾—åˆ†
            base_contrast = positive_avg - negative_avg
            
            # 2. åŠ å¼ºèƒŒæ™¯é™åˆ†ï¼šè´Ÿæ ·æœ¬æœ€é«˜åˆ†ä¹Ÿè¦æƒ©ç½š
            background_penalty = negative_max * 0.5  # è´Ÿæ ·æœ¬æœ€é«˜åˆ†çš„ä¸€åŠä½œä¸ºæƒ©ç½š
            
            # 3. æœ€ç»ˆå¯¹æ¯”å¾—åˆ† = åŸºç¡€å¯¹æ¯” - èƒŒæ™¯æƒ©ç½š
            contrast_score = base_contrast - background_penalty
            
            # 4. æ›´ä¸¥æ ¼çš„åˆ¤æ–­æ¡ä»¶
            is_target = (positive_avg > negative_avg) and (positive_max > negative_max)
        
        if is_target:  # åªä¿ç•™ç›®æ ‡ç±»åˆ«çš„æ¡†
            results.append({
                'bbox': (x1, y1, x2, y2),
                'region_idx': idx,
                'target_class': positive_classes[best_positive_idx],
                'target_score': float(positive_max),
                'positive_avg': float(positive_avg),
                'negative_avg': float(negative_avg),
                'contrast_score': float(contrast_score),
                'saliency': region.get('saliency', 0),
                'priority': region.get('priority', 'N/A')
            })
    
    # 7. æŒ‰å¯¹æ¯”å¾—åˆ†æ’åºï¼Œåªä¿ç•™æœ€ä½³çš„1ä¸ª
    results.sort(key=lambda x: x['contrast_score'], reverse=True)
    
    print(f"âœ… æ‰¾åˆ° {len(results)} ä¸ª{target_class}å€™é€‰æ¡†")
    
    if len(results) == 0:
        print(f"âŒ æœªæ‰¾åˆ°{target_class}ç±»åˆ«çš„æ¡†")
        return None
    
    # 8. è¾“å‡ºæœ€ä½³ç»“æœ
    best_result = results[0]
    
    print(f"\n{'='*70}")
    print(f"ğŸ† æœ€ä½³{target_class}æ¡†é€‰ç»“æœ")
    print(f"{'='*70}")
    print(f"  ğŸ“ ä½ç½®: {best_result['bbox']}")
    print(f"  ğŸ·ï¸  åŒ¹é…ç±»åˆ«: {best_result['target_class']}")
    print(f"  ğŸ“Š ç›®æ ‡ç±»å¾—åˆ†: {best_result['target_score']:.3f}")
    print(f"  ğŸ“Š æ­£æ ·æœ¬å¹³å‡: {best_result['positive_avg']:.3f}")
    print(f"  ğŸ“Š è´Ÿæ ·æœ¬å¹³å‡: {best_result['negative_avg']:.3f}")
    print(f"  ğŸ’¯ å¯¹æ¯”å¾—åˆ†: {best_result['contrast_score']:.3f}")
    print(f"  â­ æ˜¾è‘—æ€§: {best_result['saliency']:.1f}")
    
    # 9. å¯è§†åŒ–ï¼ˆåªç”»1ä¸ªæ¡†ï¼‰
    if output_path:
        vis_image = cv2.imread(image_path)
        x1, y1, x2, y2 = best_result['bbox']
        
        # ç¡®ä¿åæ ‡æ˜¯æ•´æ•°
        x1, y1, x2, y2 = int(x1), int(y1), int(x2), int(y2)
        
        print(f"\nğŸ” ç»˜åˆ¶æ¡†é€‰: ({x1}, {y1}) -> ({x2}, {y2})")
        
        # ç»˜åˆ¶æ›´æ˜æ˜¾çš„æ¡†ï¼ˆçº¢è‰²ï¼Œå¾ˆç²—çš„çº¿ï¼‰
        cv2.rectangle(vis_image, (x1, y1), (x2, y2), (0, 0, 255), 5)  # çº¢è‰²ï¼Œ5åƒç´ ç²—
        
        # ç»˜åˆ¶å†…æ¡†ï¼ˆç™½è‰²ï¼Œç»†çº¿ï¼‰
        cv2.rectangle(vis_image, (x1+2, y1+2), (x2-2, y2-2), (255, 255, 255), 2)
        
        # æ ‡ç­¾
        label1 = f"{best_result['target_class']}: {best_result['target_score']:.1%}"
        label2 = f"Contrast: {best_result['contrast_score']:.3f}"
        
        font = cv2.FONT_HERSHEY_SIMPLEX
        
        # æ ‡ç­¾1ï¼ˆçº¢è‰²èƒŒæ™¯ï¼‰
        (tw, th), _ = cv2.getTextSize(label1, font, 0.8, 2)
        cv2.rectangle(vis_image, (x1, y1-th-15), (x1+tw+15, y1), (0, 0, 255), -1)
        cv2.putText(vis_image, label1, (x1+8, y1-8), font, 0.8, (255, 255, 255), 2)
        
        # æ ‡ç­¾2ï¼ˆçº¢è‰²èƒŒæ™¯ï¼‰
        (tw2, th2), _ = cv2.getTextSize(label2, font, 0.6, 2)
        cv2.rectangle(vis_image, (x1, y2+5), (x1+tw2+15, y2+th2+15), (0, 0, 255), -1)
        cv2.putText(vis_image, label2, (x1+8, y2+th2+12), font, 0.6, (255, 255, 255), 2)
        
        # æ ‡é¢˜
        title = f"Target: {target_class} (Best Detection)"
        cv2.putText(vis_image, title, (10, 40), font, 1.0, (255, 255, 255), 4)
        cv2.putText(vis_image, title, (10, 40), font, 1.0, (0, 255, 0), 2)
        
        cv2.imwrite(output_path, vis_image)
        print(f"\nâœ… å¯è§†åŒ–å·²ä¿å­˜: {output_path}")
    
    return best_result


def main():
    parser = argparse.ArgumentParser(description='é¥æ„Ÿç›®æ ‡æ£€æµ‹ï¼ˆé€šç”¨ï¼‰')
    parser.add_argument('--image', type=str, required=True,
                        help='è¾“å…¥å›¾åƒè·¯å¾„')
    parser.add_argument('--target', type=str, required=True,
                        help='ç›®æ ‡ç±»åˆ«ï¼ˆå¦‚: ship, airplane, buildingç­‰ï¼‰')
    parser.add_argument('--model', type=str, default='RN50',
                        choices=['RN50', 'ViT-B-32', 'ViT-L-14'],
                        help='æ¨¡å‹é€‰æ‹©')
    parser.add_argument('--strategy', type=str, default='multi_threshold_saliency',
                        choices=['layered', 'pyramid', 'multi_threshold_saliency'],
                        help='é‡‡æ ·ç­–ç•¥')
    parser.add_argument('--max-regions', type=int, default=50,
                        help='æœ€å¤§åŒºåŸŸæ•°')
    parser.add_argument('--output', type=str, default=None,
                        help='è¾“å‡ºå›¾åƒè·¯å¾„')
    
    args = parser.parse_args()
    
    # è‡ªåŠ¨ç”Ÿæˆè¾“å‡ºè·¯å¾„
    if args.output is None:
        om = get_output_manager()
        args.output = om.get_detection_result_path(args.target, args.model)
    
    # è¿è¡Œæ£€æµ‹
    result = detect_target_with_contrastive_learning(
        image_path=args.image,
        target_class=args.target,
        model_name=args.model,
        strategy=args.strategy,
        max_regions=args.max_regions,
        output_path=args.output
    )
    
    print(f"\n{'='*70}")
    print("âœ… æ£€æµ‹å®Œæˆï¼")
    print(f"{'='*70}")


if __name__ == "__main__":
    main()

