#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Stage2: WordNetå¢å¼ºæ¨¡å—
åŸºäºåŸæœ‰wordnet_vocabulary.pyï¼Œä¸“é—¨ç”¨äºå®éªŒä¸­çš„è¯æ±‡å¢å¼º
"""

import os
import sys
from typing import List, Dict, Optional

# æ·»åŠ çˆ¶ç›®å½•åˆ°è·¯å¾„ä»¥å¯¼å…¥åŸæœ‰æ¨¡å—
sys.path.append(os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__)))))

from wordnet_vocabulary import (
    WORDNET_REMOTE_SENSING_CLASSES, 
    get_synonyms, 
    get_expansion_words,
    get_hypernyms,
    build_vocabulary,
    get_full_class_list
)


class WordNetEnhancer:
    """WordNetå¢å¼ºå™¨"""
    
    def __init__(self):
        """åˆå§‹åŒ–WordNetå¢å¼ºå™¨"""
        self.base_classes = WORDNET_REMOTE_SENSING_CLASSES.copy()
        self.enhanced_vocabulary = {}
        self.expansion_cache = {}
    
    def enhance_class_with_synonyms(self, target_class: str, max_synonyms: int = 5) -> List[str]:
        """
        ä½¿ç”¨åŒä¹‰è¯å¢å¼ºç±»åˆ«
        
        å‚æ•°:
            target_class: ç›®æ ‡ç±»åˆ«
            max_synonyms: æœ€å¤§åŒä¹‰è¯æ•°é‡
        
        è¿”å›:
            å¢å¼ºåçš„ç±»åˆ«åˆ—è¡¨
        """
        print(f"\nğŸ”§ ä½¿ç”¨åŒä¹‰è¯å¢å¼ºç±»åˆ«: {target_class}")
        
        # è·å–åŒä¹‰è¯
        synonyms = get_synonyms(target_class)
        
        # é™åˆ¶åŒä¹‰è¯æ•°é‡
        enhanced_synonyms = synonyms[:max_synonyms]
        
        # æ„å»ºå¢å¼ºç±»åˆ«åˆ—è¡¨
        enhanced_classes = [target_class] + enhanced_synonyms
        
        print(f"âœ… å¢å¼ºç±»åˆ«: {enhanced_classes}")
        
        return enhanced_classes
    
    def enhance_class_with_expansion(self, target_class: str, num_expansion: int = 5) -> List[str]:
        """
        ä½¿ç”¨æ‰©å±•è¯å¢å¼ºç±»åˆ«
        
        å‚æ•°:
            target_class: ç›®æ ‡ç±»åˆ«
            num_expansion: æ‰©å±•è¯æ•°é‡
        
        è¿”å›:
            å¢å¼ºåçš„ç±»åˆ«åˆ—è¡¨
        """
        print(f"\nğŸ”§ ä½¿ç”¨æ‰©å±•è¯å¢å¼ºç±»åˆ«: {target_class}")
        
        # è·å–æ‰©å±•è¯
        expansion_words = get_expansion_words(target_class, num_words=num_expansion)
        
        # æ„å»ºå¢å¼ºç±»åˆ«åˆ—è¡¨
        enhanced_classes = [target_class] + expansion_words
        
        print(f"âœ… å¢å¼ºç±»åˆ«: {enhanced_classes}")
        
        return enhanced_classes
    
    def enhance_class_with_hierarchy(self, target_class: str) -> List[str]:
        """
        ä½¿ç”¨å±‚æ¬¡ç»“æ„å¢å¼ºç±»åˆ«
        
        å‚æ•°:
            target_class: ç›®æ ‡ç±»åˆ«
        
        è¿”å›:
            å¢å¼ºåçš„ç±»åˆ«åˆ—è¡¨
        """
        print(f"\nğŸ”§ ä½¿ç”¨å±‚æ¬¡ç»“æ„å¢å¼ºç±»åˆ«: {target_class}")
        
        # è·å–ä¸Šä½è¯
        hypernyms = get_hypernyms(target_class)
        
        # æ„å»ºå¢å¼ºç±»åˆ«åˆ—è¡¨
        enhanced_classes = [target_class] + hypernyms
        
        print(f"âœ… å¢å¼ºç±»åˆ«: {enhanced_classes}")
        
        return enhanced_classes
    
    def create_enhanced_vocabulary(self, target_classes: List[str], 
                                 enhancement_methods: List[str] = None) -> Dict[str, List[str]]:
        """
        åˆ›å»ºå¢å¼ºè¯è¡¨
        
        å‚æ•°:
            target_classes: ç›®æ ‡ç±»åˆ«åˆ—è¡¨
            enhancement_methods: å¢å¼ºæ–¹æ³•åˆ—è¡¨
        
        è¿”å›:
            å¢å¼ºè¯è¡¨å­—å…¸
        """
        if enhancement_methods is None:
            enhancement_methods = ['synonyms', 'expansion', 'hierarchy']
        
        print(f"\nğŸ”§ åˆ›å»ºå¢å¼ºè¯è¡¨ (ç›®æ ‡ç±»åˆ«: {len(target_classes)}ä¸ª, æ–¹æ³•: {enhancement_methods})")
        
        enhanced_vocab = {}
        
        for target_class in target_classes:
            enhanced_classes = [target_class]
            
            # åº”ç”¨ä¸åŒçš„å¢å¼ºæ–¹æ³•
            if 'synonyms' in enhancement_methods:
                synonyms = get_synonyms(target_class)
                enhanced_classes.extend(synonyms[:3])  # æœ€å¤š3ä¸ªåŒä¹‰è¯
            
            if 'expansion' in enhancement_methods:
                expansion_words = get_expansion_words(target_class, num_words=3)
                enhanced_classes.extend(expansion_words)
            
            if 'hierarchy' in enhancement_methods:
                hypernyms = get_hypernyms(target_class)
                enhanced_classes.extend(hypernyms)
            
            # å»é‡å¹¶ä¿æŒé¡ºåº
            enhanced_classes = list(dict.fromkeys(enhanced_classes))
            
            enhanced_vocab[target_class] = enhanced_classes
            
            print(f"  {target_class}: {len(enhanced_classes)} ä¸ªå¢å¼ºè¯")
        
        print(f"âœ… å¢å¼ºè¯è¡¨åˆ›å»ºå®Œæˆ")
        
        return enhanced_vocab
    
    def build_comprehensive_vocabulary(self, target_classes: List[str],
                                     include_base_classes: bool = True,
                                     enhancement_methods: List[str] = None) -> List[str]:
        """
        æ„å»ºç»¼åˆè¯è¡¨
        
        å‚æ•°:
            target_classes: ç›®æ ‡ç±»åˆ«åˆ—è¡¨
            include_base_classes: æ˜¯å¦åŒ…å«åŸºç¡€ç±»åˆ«
            enhancement_methods: å¢å¼ºæ–¹æ³•åˆ—è¡¨
        
        è¿”å›:
            ç»¼åˆè¯è¡¨åˆ—è¡¨
        """
        print(f"\nğŸ”§ æ„å»ºç»¼åˆè¯è¡¨...")
        
        # è·å–å¢å¼ºè¯è¡¨
        enhanced_vocab = self.create_enhanced_vocabulary(target_classes, enhancement_methods)
        
        # æ„å»ºç»¼åˆè¯è¡¨
        comprehensive_vocab = []
        
        # æ·»åŠ å¢å¼ºçš„ç›®æ ‡ç±»åˆ«
        for target_class, enhanced_classes in enhanced_vocab.items():
            comprehensive_vocab.extend(enhanced_classes)
        
        # æ·»åŠ åŸºç¡€ç±»åˆ«ï¼ˆå¦‚æœéœ€è¦ï¼‰
        if include_base_classes:
            # æ’é™¤å·²ç»åœ¨å¢å¼ºè¯è¡¨ä¸­çš„ç±»åˆ«
            enhanced_classes_set = set(comprehensive_vocab)
            base_classes = [c for c in self.base_classes if c not in enhanced_classes_set]
            comprehensive_vocab.extend(base_classes)
        
        # å»é‡å¹¶ä¿æŒé¡ºåº
        comprehensive_vocab = list(dict.fromkeys(comprehensive_vocab))
        
        print(f"âœ… ç»¼åˆè¯è¡¨æ„å»ºå®Œæˆ: {len(comprehensive_vocab)} ä¸ªç±»åˆ«")
        
        return comprehensive_vocab
    
    def get_vocabulary_statistics(self, vocabulary: List[str]) -> Dict:
        """
        è·å–è¯è¡¨ç»Ÿè®¡ä¿¡æ¯
        
        å‚æ•°:
            vocabulary: è¯è¡¨åˆ—è¡¨
        
        è¿”å›:
            ç»Ÿè®¡ä¿¡æ¯å­—å…¸
        """
        stats = {
            'total_classes': len(vocabulary),
            'base_classes_count': len(set(vocabulary) & set(self.base_classes)),
            'enhanced_classes_count': len(vocabulary) - len(set(vocabulary) & set(self.base_classes))
        }
        
        # æŒ‰ç±»åˆ«åˆ†ç»„ç»Ÿè®¡
        category_stats = {}
        for cls in vocabulary:
            # ç®€å•çš„ç±»åˆ«åˆ†ç»„é€»è¾‘
            if any(word in cls for word in ['building', 'house', 'apartment']):
                category = 'buildings'
            elif any(word in cls for word in ['ship', 'boat', 'vessel']):
                category = 'vessels'
            elif any(word in cls for word in ['airplane', 'aircraft', 'helicopter']):
                category = 'aircraft'
            elif any(word in cls for word in ['road', 'highway', 'street']):
                category = 'transportation'
            elif any(word in cls for word in ['forest', 'tree', 'vegetation']):
                category = 'vegetation'
            elif any(word in cls for word in ['water', 'lake', 'river']):
                category = 'water'
            else:
                category = 'other'
            
            category_stats[category] = category_stats.get(category, 0) + 1
        
        stats['category_distribution'] = category_stats
        
        return stats
    
    def save_enhanced_vocabulary(self, vocabulary: List[str], 
                               target_classes: List[str],
                               output_path: str):
        """
        ä¿å­˜å¢å¼ºè¯è¡¨
        
        å‚æ•°:
            vocabulary: è¯è¡¨åˆ—è¡¨
            target_classes: ç›®æ ‡ç±»åˆ«åˆ—è¡¨
            output_path: è¾“å‡ºè·¯å¾„
        """
        print(f"\nğŸ’¾ ä¿å­˜å¢å¼ºè¯è¡¨åˆ°: {output_path}")
        
        os.makedirs(os.path.dirname(output_path), exist_ok=True)
        
        with open(output_path, 'w', encoding='utf-8') as f:
            f.write("# Enhanced WordNet Vocabulary\n")
            f.write(f"# Generated for target classes: {', '.join(target_classes)}\n")
            f.write(f"# Total classes: {len(vocabulary)}\n\n")
            
            for i, cls in enumerate(vocabulary, 1):
                f.write(f"{i:3d}. {cls}\n")
        
        print(f"âœ… å¢å¼ºè¯è¡¨å·²ä¿å­˜")
    
    def load_enhanced_vocabulary(self, input_path: str) -> List[str]:
        """
        åŠ è½½å¢å¼ºè¯è¡¨
        
        å‚æ•°:
            input_path: è¾“å…¥è·¯å¾„
        
        è¿”å›:
            è¯è¡¨åˆ—è¡¨
        """
        print(f"\nğŸ“– åŠ è½½å¢å¼ºè¯è¡¨: {input_path}")
        
        if not os.path.exists(input_path):
            raise FileNotFoundError(f"è¯è¡¨æ–‡ä»¶ä¸å­˜åœ¨: {input_path}")
        
        vocabulary = []
        
        with open(input_path, 'r', encoding='utf-8') as f:
            for line in f:
                line = line.strip()
                if line and not line.startswith('#'):
                    # æå–ç±»åˆ«åç§°ï¼ˆå»æ‰åºå·ï¼‰
                    if '. ' in line:
                        cls = line.split('. ', 1)[1]
                    else:
                        cls = line
                    vocabulary.append(cls)
        
        print(f"âœ… åŠ è½½è¯è¡¨: {len(vocabulary)} ä¸ªç±»åˆ«")
        
        return vocabulary


def main():
    """æµ‹è¯•WordNetå¢å¼ºå™¨"""
    print("=" * 70)
    print("æµ‹è¯•WordNetå¢å¼ºå™¨")
    print("=" * 70)
    
    # åˆ›å»ºWordNetå¢å¼ºå™¨
    enhancer = WordNetEnhancer()
    
    # æµ‹è¯•ç›®æ ‡ç±»åˆ«
    target_classes = ['ship', 'airplane', 'building']
    
    # æµ‹è¯•ä¸åŒå¢å¼ºæ–¹æ³•
    print(f"\n{'='*50}")
    print("æµ‹è¯•åŒä¹‰è¯å¢å¼º")
    print(f"{'='*50}")
    
    for target_class in target_classes:
        enhanced = enhancer.enhance_class_with_synonyms(target_class, max_synonyms=3)
        print(f"{target_class}: {enhanced}")
    
    print(f"\n{'='*50}")
    print("æµ‹è¯•æ‰©å±•è¯å¢å¼º")
    print(f"{'='*50}")
    
    for target_class in target_classes:
        enhanced = enhancer.enhance_class_with_expansion(target_class, num_expansion=3)
        print(f"{target_class}: {enhanced}")
    
    print(f"\n{'='*50}")
    print("æµ‹è¯•å±‚æ¬¡ç»“æ„å¢å¼º")
    print(f"{'='*50}")
    
    for target_class in target_classes:
        enhanced = enhancer.enhance_class_with_hierarchy(target_class)
        print(f"{target_class}: {enhanced}")
    
    # æµ‹è¯•ç»¼åˆå¢å¼º
    print(f"\n{'='*50}")
    print("æµ‹è¯•ç»¼åˆå¢å¼º")
    print(f"{'='*50}")
    
    comprehensive_vocab = enhancer.build_comprehensive_vocabulary(
        target_classes, 
        include_base_classes=True,
        enhancement_methods=['synonyms', 'expansion']
    )
    
    # è·å–ç»Ÿè®¡ä¿¡æ¯
    stats = enhancer.get_vocabulary_statistics(comprehensive_vocab)
    print(f"\nğŸ“Š è¯è¡¨ç»Ÿè®¡:")
    print(f"  æ€»ç±»åˆ«æ•°: {stats['total_classes']}")
    print(f"  åŸºç¡€ç±»åˆ«æ•°: {stats['base_classes_count']}")
    print(f"  å¢å¼ºç±»åˆ«æ•°: {stats['enhanced_classes_count']}")
    print(f"  ç±»åˆ«åˆ†å¸ƒ: {stats['category_distribution']}")
    
    # ä¿å­˜è¯è¡¨
    output_path = "outputs/enhanced_vocabulary.txt"
    enhancer.save_enhanced_vocabulary(comprehensive_vocab, target_classes, output_path)
    
    print("\nâœ… WordNetå¢å¼ºå™¨æµ‹è¯•å®Œæˆ!")


if __name__ == "__main__":
    main()
