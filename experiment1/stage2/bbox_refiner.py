#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Stage2: è¾¹ç•Œæ¡†å¾®è°ƒæ¨¡å—
åŸºäºåŸæœ‰bbox_refinement.pyï¼Œä¸“é—¨ç”¨äºå®éªŒä¸­çš„è¾¹ç•Œæ¡†ä¼˜åŒ–
"""

import torch
import numpy as np
import cv2
from typing import List, Dict, Tuple, Optional
import os
import sys

# æ·»åŠ çˆ¶ç›®å½•åˆ°è·¯å¾„ä»¥å¯¼å…¥åŸæœ‰æ¨¡å—
sys.path.append(os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__)))))

from bbox_refinement import BBoxRefinement, compute_saliency_map


class ExperimentBBoxRefiner:
    """å®éªŒç”¨è¾¹ç•Œæ¡†å¾®è°ƒå™¨"""
    
    def __init__(self, model, preprocess, device: str = 'cuda'):
        """
        åˆå§‹åŒ–è¾¹ç•Œæ¡†å¾®è°ƒå™¨
        
        å‚æ•°:
            model: RemoteCLIPæ¨¡å‹
            preprocess: å›¾åƒé¢„å¤„ç†å‡½æ•°
            device: è®¡ç®—è®¾å¤‡
        """
        self.device = device if torch.cuda.is_available() else 'cpu'
        self.bbox_refiner = BBoxRefinement(model, preprocess, self.device)
    
    def refine_proposals(self, image: np.ndarray, proposals: List[Dict],
                        saliency_map: np.ndarray = None,
                        refinement_method: str = 'both') -> List[Dict]:
        """
        å¾®è°ƒå€™é€‰æ¡†
        
        å‚æ•°:
            image: è¾“å…¥å›¾åƒ
            proposals: å€™é€‰æ¡†åˆ—è¡¨
            saliency_map: æ˜¾è‘—æ€§å›¾
            refinement_method: å¾®è°ƒæ–¹æ³•
        
        è¿”å›:
            å¾®è°ƒåçš„å€™é€‰æ¡†åˆ—è¡¨
        """
        print(f"\nğŸ”§ å¾®è°ƒ {len(proposals)} ä¸ªå€™é€‰æ¡† (æ–¹æ³•: {refinement_method})...")
        
        if saliency_map is None:
            print("  è®¡ç®—æ˜¾è‘—æ€§å›¾...")
            saliency_map = compute_saliency_map(image)
        
        refined_proposals = []
        
        for i, proposal in enumerate(proposals):
            bbox = proposal['bbox']
            
            # åˆ›å»ºæ­£è´Ÿæ ·æœ¬åŸå‹ï¼ˆç®€åŒ–ç‰ˆï¼‰
            positive_prototype = torch.randn(1, 512).to(self.device)  # æ¨¡æ‹Ÿæ­£æ ·æœ¬ç‰¹å¾
            negative_prototype = torch.randn(1, 512).to(self.device)  # æ¨¡æ‹Ÿè´Ÿæ ·æœ¬ç‰¹å¾
            
            try:
                # æ‰§è¡Œè¾¹ç•Œæ¡†å¾®è°ƒ
                refine_result = self.bbox_refiner.refine_bbox_hybrid(
                    image=image,
                    bbox=bbox,
                    saliency_map=saliency_map,
                    positive_prototype=positive_prototype,
                    negative_prototype=negative_prototype,
                    method=refinement_method
                )
                
                # æ›´æ–°å€™é€‰æ¡†ä¿¡æ¯
                proposal['refined_bbox'] = refine_result['bbox']
                proposal['refinement_applied'] = refine_result.get('refined', False)
                proposal['refinement_score'] = refine_result.get('composite_score', 0.0)
                proposal['refinement_method'] = refinement_method
                
                if 'scale' in refine_result:
                    proposal['refinement_scale'] = refine_result['scale']
                
                if 'iterations' in refine_result:
                    proposal['refinement_iterations'] = refine_result['iterations']
                
                refined_proposals.append(proposal)
                
            except Exception as e:
                print(f"  è­¦å‘Š: å€™é€‰æ¡† {i} å¾®è°ƒå¤±è´¥: {e}")
                # ä¿ç•™åŸå§‹å€™é€‰æ¡†
                proposal['refined_bbox'] = bbox
                proposal['refinement_applied'] = False
                proposal['refinement_score'] = proposal.get('score', 0.0)
                proposal['refinement_method'] = refinement_method
                refined_proposals.append(proposal)
        
        print(f"âœ… è¾¹ç•Œæ¡†å¾®è°ƒå®Œæˆ")
        
        return refined_proposals
    
    def refine_proposals_with_multiple_methods(self, image: np.ndarray, proposals: List[Dict],
                                             methods: List[str] = None) -> Dict[str, List[Dict]]:
        """
        ä½¿ç”¨å¤šç§æ–¹æ³•å¾®è°ƒå€™é€‰æ¡†
        
        å‚æ•°:
            image: è¾“å…¥å›¾åƒ
            proposals: å€™é€‰æ¡†åˆ—è¡¨
            methods: å¾®è°ƒæ–¹æ³•åˆ—è¡¨
        
        è¿”å›:
            æ¯ç§æ–¹æ³•çš„å¾®è°ƒç»“æœå­—å…¸
        """
        if methods is None:
            methods = ['position', 'scale', 'both', 'boundary']
        
        print(f"\nğŸ”§ ä½¿ç”¨å¤šç§æ–¹æ³•å¾®è°ƒå€™é€‰æ¡†: {methods}")
        
        saliency_map = compute_saliency_map(image)
        all_results = {}
        
        for method in methods:
            print(f"\n  ä½¿ç”¨æ–¹æ³•: {method}")
            
            # ä¸ºæ¯ç§æ–¹æ³•åˆ›å»ºç‹¬ç«‹çš„å€™é€‰æ¡†å‰¯æœ¬
            method_proposals = []
            for proposal in proposals:
                method_proposal = proposal.copy()
                method_proposals.append(method_proposal)
            
            # æ‰§è¡Œå¾®è°ƒ
            refined_proposals = self.refine_proposals(
                image, method_proposals, saliency_map, method
            )
            
            all_results[method] = refined_proposals
        
        return all_results
    
    def evaluate_refinement_quality(self, proposals: List[Dict]) -> Dict:
        """
        è¯„ä¼°å¾®è°ƒè´¨é‡
        
        å‚æ•°:
            proposals: å¾®è°ƒåçš„å€™é€‰æ¡†åˆ—è¡¨
        
        è¿”å›:
            è´¨é‡è¯„ä¼°ç»“æœ
        """
        if not proposals:
            return {}
        
        refined_count = sum(1 for p in proposals if p.get('refinement_applied', False))
        total_count = len(proposals)
        
        refinement_scores = [p.get('refinement_score', 0) for p in proposals]
        
        quality_metrics = {
            'total_proposals': total_count,
            'refined_proposals': refined_count,
            'refinement_rate': refined_count / total_count if total_count > 0 else 0,
            'average_refinement_score': np.mean(refinement_scores) if refinement_scores else 0,
            'max_refinement_score': np.max(refinement_scores) if refinement_scores else 0,
            'min_refinement_score': np.min(refinement_scores) if refinement_scores else 0
        }
        
        return quality_metrics
    
    def compare_refinement_methods(self, refinement_results: Dict[str, List[Dict]]) -> Dict:
        """
        æ¯”è¾ƒä¸åŒå¾®è°ƒæ–¹æ³•çš„æ•ˆæœ
        
        å‚æ•°:
            refinement_results: ä¸åŒæ–¹æ³•çš„å¾®è°ƒç»“æœ
        
        è¿”å›:
            æ–¹æ³•æ¯”è¾ƒç»“æœ
        """
        print(f"\nğŸ“Š æ¯”è¾ƒå¾®è°ƒæ–¹æ³•æ•ˆæœ...")
        
        comparison_results = {}
        
        for method, proposals in refinement_results.items():
            quality_metrics = self.evaluate_refinement_quality(proposals)
            comparison_results[method] = quality_metrics
        
        # æ‰¾å‡ºæœ€ä½³æ–¹æ³•
        best_method = max(comparison_results.keys(), 
                         key=lambda x: comparison_results[x]['average_refinement_score'])
        
        comparison_results['best_method'] = best_method
        
        print(f"âœ… æœ€ä½³å¾®è°ƒæ–¹æ³•: {best_method}")
        
        return comparison_results
    
    def get_refinement_statistics(self, proposals: List[Dict]) -> Dict:
        """
        è·å–å¾®è°ƒç»Ÿè®¡ä¿¡æ¯
        
        å‚æ•°:
            proposals: å€™é€‰æ¡†åˆ—è¡¨
        
        è¿”å›:
            ç»Ÿè®¡ä¿¡æ¯å­—å…¸
        """
        if not proposals:
            return {}
        
        # ç»Ÿè®¡å¾®è°ƒæ–¹æ³•åˆ†å¸ƒ
        method_counts = {}
        for proposal in proposals:
            method = proposal.get('refinement_method', 'unknown')
            method_counts[method] = method_counts.get(method, 0) + 1
        
        # ç»Ÿè®¡å¾®è°ƒåº”ç”¨æƒ…å†µ
        applied_count = sum(1 for p in proposals if p.get('refinement_applied', False))
        
        # ç»Ÿè®¡åˆ†æ•°åˆ†å¸ƒ
        scores = [p.get('refinement_score', 0) for p in proposals]
        
        stats = {
            'total_proposals': len(proposals),
            'refinement_applied_count': applied_count,
            'refinement_rate': applied_count / len(proposals) if proposals else 0,
            'method_distribution': method_counts,
            'score_stats': {
                'mean': np.mean(scores) if scores else 0,
                'std': np.std(scores) if scores else 0,
                'min': np.min(scores) if scores else 0,
                'max': np.max(scores) if scores else 0
            }
        }
        
        return stats


def main():
    """æµ‹è¯•è¾¹ç•Œæ¡†å¾®è°ƒå™¨"""
    print("=" * 70)
    print("æµ‹è¯•è¾¹ç•Œæ¡†å¾®è°ƒå™¨")
    print("=" * 70)
    
    # æµ‹è¯•å›¾åƒ
    test_image_path = "assets/airport.jpg"
    if not os.path.exists(test_image_path):
        print(f"âŒ æµ‹è¯•å›¾åƒä¸å­˜åœ¨: {test_image_path}")
        return
    
    # åŠ è½½æµ‹è¯•å›¾åƒ
    image = cv2.imread(test_image_path)
    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
    
    # åˆ›å»ºæ¨¡æ‹Ÿå€™é€‰æ¡†
    mock_proposals = [
        {
            'proposal_id': 0,
            'bbox': (100, 100, 200, 200),
            'score': 0.8,
            'predicted_class': 'airplane'
        },
        {
            'proposal_id': 1,
            'bbox': (300, 200, 400, 300),
            'score': 0.6,
            'predicted_class': 'building'
        },
        {
            'proposal_id': 2,
            'bbox': (150, 250, 250, 350),
            'score': 0.9,
            'predicted_class': 'vehicle'
        }
    ]
    
    # åˆ›å»ºæ¨¡æ‹Ÿæ¨¡å‹å’Œé¢„å¤„ç†å‡½æ•°
    class MockModel:
        def encode_image(self, x):
            return torch.randn(x.shape[0], 512)
    
    class MockPreprocess:
        def __call__(self, x):
            return torch.randn(1, 3, 224, 224)
    
    mock_model = MockModel()
    mock_preprocess = MockPreprocess()
    
    # åˆ›å»ºè¾¹ç•Œæ¡†å¾®è°ƒå™¨
    refiner = ExperimentBBoxRefiner(mock_model, mock_preprocess)
    
    # æµ‹è¯•å•æ–¹æ³•å¾®è°ƒ
    print(f"\n{'='*50}")
    print("æµ‹è¯•å•æ–¹æ³•å¾®è°ƒ")
    print(f"{'='*50}")
    
    refined_proposals = refiner.refine_proposals(
        image, mock_proposals, refinement_method='both'
    )
    
    # è·å–ç»Ÿè®¡ä¿¡æ¯
    stats = refiner.get_refinement_statistics(refined_proposals)
    print(f"\nğŸ“Š å¾®è°ƒç»Ÿè®¡:")
    print(f"  æ€»å€™é€‰æ¡†æ•°: {stats['total_proposals']}")
    print(f"  å¾®è°ƒåº”ç”¨æ•°: {stats['refinement_applied_count']}")
    print(f"  å¾®è°ƒç‡: {stats['refinement_rate']:.2%}")
    print(f"  å¹³å‡åˆ†æ•°: {stats['score_stats']['mean']:.3f}")
    
    # æµ‹è¯•å¤šæ–¹æ³•å¾®è°ƒ
    print(f"\n{'='*50}")
    print("æµ‹è¯•å¤šæ–¹æ³•å¾®è°ƒ")
    print(f"{'='*50}")
    
    methods = ['position', 'scale', 'both']
    multi_results = refiner.refine_proposals_with_multiple_methods(
        image, mock_proposals, methods
    )
    
    # æ¯”è¾ƒæ–¹æ³•æ•ˆæœ
    comparison = refiner.compare_refinement_methods(multi_results)
    print(f"\nğŸ“Š æ–¹æ³•æ¯”è¾ƒ:")
    for method, metrics in comparison.items():
        if method != 'best_method':
            print(f"  {method}: å¾®è°ƒç‡={metrics['refinement_rate']:.2%}, å¹³å‡åˆ†æ•°={metrics['average_refinement_score']:.3f}")
    
    print(f"\nğŸ† æœ€ä½³æ–¹æ³•: {comparison['best_method']}")
    
    print("\nâœ… è¾¹ç•Œæ¡†å¾®è°ƒå™¨æµ‹è¯•å®Œæˆ!")


if __name__ == "__main__":
    main()
