#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Stage2: ç›®æ ‡æ£€æµ‹æ¨¡å—
åŸºäºåŸæœ‰target_detection.pyï¼Œä¸“é—¨ç”¨äºå®éªŒä¸­çš„ç›®æ ‡æ£€æµ‹
"""

import torch
import open_clip
from PIL import Image
import numpy as np
import cv2
import argparse
from typing import List, Dict
import os
import sys

# æ·»åŠ çˆ¶ç›®å½•åˆ°è·¯å¾„ä»¥å¯¼å…¥åŸæœ‰æ¨¡å—
sys.path.append(os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__)))))

from sampling import sample_regions
from wordnet_vocabulary import get_expansion_words, get_synonyms, WORDNET_REMOTE_SENSING_CLASSES


class ExperimentTargetDetector:
    """å®éªŒç”¨ç›®æ ‡æ£€æµ‹å™¨"""
    
    def __init__(self, model_name: str = 'RN50', device: str = 'cuda'):
        """
        åˆå§‹åŒ–ç›®æ ‡æ£€æµ‹å™¨
        
        å‚æ•°:
            model_name: RemoteCLIPæ¨¡å‹åç§°
            device: è®¡ç®—è®¾å¤‡
        """
        self.device = device if torch.cuda.is_available() else 'cpu'
        self.model_name = model_name
        self.model = None
        self.preprocess = None
        self.tokenizer = None
        
        # åˆå§‹åŒ–æ¨¡å‹
        self._load_model()
    
    def _load_model(self):
        """åŠ è½½RemoteCLIPæ¨¡å‹"""
        print(f"ğŸ”„ åŠ è½½RemoteCLIPæ¨¡å‹: {self.model_name}")
        
        self.model, _, self.preprocess = open_clip.create_model_and_transforms(self.model_name)
        self.tokenizer = open_clip.get_tokenizer(self.model_name)
        
        checkpoint_path = f"checkpoints/RemoteCLIP-{self.model_name}.pt"
        ckpt = torch.load(checkpoint_path, map_location="cpu")
        self.model.load_state_dict(ckpt)
        self.model = self.model.to(self.device).eval()
        
        print(f"âœ… æ¨¡å‹å·²åŠ è½½åˆ° {self.device}")
    
    def detect_target_with_contrastive_learning(self, image: np.ndarray, target_class: str,
                                              strategy: str = 'multi_threshold_saliency',
                                              max_regions: int = 50) -> List[Dict]:
        """
        ä½¿ç”¨å¯¹æ¯”å­¦ä¹ æ£€æµ‹ç›®æ ‡ç±»åˆ«
        
        å‚æ•°:
            image: è¾“å…¥å›¾åƒ
            target_class: ç›®æ ‡ç±»åˆ«
            strategy: é‡‡æ ·ç­–ç•¥
            max_regions: æœ€å¤§åŒºåŸŸæ•°
        
        è¿”å›:
            æ£€æµ‹ç»“æœåˆ—è¡¨
        """
        print(f"\nğŸ¯ ç›®æ ‡æ£€æµ‹: {target_class}")
        
        # 1. å®šä¹‰ç±»åˆ«ä½“ç³»
        target_synonyms = get_synonyms(target_class)
        if target_synonyms:
            positive_classes = [target_class] + target_synonyms[:4]
        else:
            positive_classes = [target_class]
        
        negative_classes = [c for c in WORDNET_REMOTE_SENSING_CLASSES 
                          if c not in positive_classes]
        
        all_classes = positive_classes + negative_classes
        n_positive = len(positive_classes)
        
        print(f"ğŸ“‹ ç±»åˆ«è®¾ç½®: æ­£æ ·æœ¬{n_positive}ä¸ª, è´Ÿæ ·æœ¬{len(negative_classes)}ä¸ª")
        
        # 2. é¢„ç¼–ç æ–‡æœ¬ç‰¹å¾
        text = self.tokenizer(all_classes)
        with torch.no_grad():
            text_features = self.model.encode_text(text.to(self.device))
            text_features /= text_features.norm(dim=-1, keepdim=True)
        
        positive_text_features = text_features[:n_positive]
        negative_text_features = text_features[n_positive:]
        
        # 3. åŒºåŸŸé‡‡æ ·
        regions = sample_regions(image, strategy=strategy, max_regions=max_regions)
        print(f"âœ… æå–åˆ° {len(regions)} ä¸ªå€™é€‰åŒºåŸŸ")
        
        # 4. å¯¹æ¯”å­¦ä¹ æ£€æµ‹
        results = []
        
        for idx, region in enumerate(regions):
            x1, y1, x2, y2 = region['bbox']
            x1, y1 = max(0, x1), max(0, y1)
            x2, y2 = min(image.shape[1], x2), min(image.shape[0], y2)
            
            # è£å‰ª
            crop = image[y1:y2, x1:x2]
            if crop.size == 0 or crop.shape[0] < 10 or crop.shape[1] < 10:
                continue
            
            crop_pil = Image.fromarray(crop)
            crop_tensor = self.preprocess(crop_pil).unsqueeze(0)
            
            # ç¼–ç 
            with torch.no_grad():
                image_features = self.model.encode_image(crop_tensor.to(self.device))
                image_features /= image_features.norm(dim=-1, keepdim=True)
                
                # è®¡ç®—ç›¸ä¼¼åº¦
                similarities = (image_features @ text_features.T).squeeze(0).cpu().numpy()
                
                # æ­£è´Ÿæ ·æœ¬å¾—åˆ†
                positive_scores = similarities[:n_positive]
                positive_avg = positive_scores.mean()
                positive_max = positive_scores.max()
                best_positive_idx = positive_scores.argmax()
                
                negative_scores = similarities[n_positive:]
                negative_avg = negative_scores.mean()
                negative_max = negative_scores.max()
                
                # å¯¹æ¯”å¾—åˆ†
                base_contrast = positive_avg - negative_avg
                background_penalty = negative_max * 0.5
                contrast_score = base_contrast - background_penalty
                
                # åˆ¤æ–­æ¡ä»¶
                is_target = (positive_avg > negative_avg) and (positive_max > negative_max)
            
            if is_target:
                results.append({
                    'bbox': (x1, y1, x2, y2),
                    'region_idx': idx,
                    'target_class': positive_classes[best_positive_idx],
                    'target_score': float(positive_max),
                    'positive_avg': float(positive_avg),
                    'negative_avg': float(negative_avg),
                    'contrast_score': float(contrast_score),
                    'saliency': region.get('saliency', 0),
                    'priority': region.get('priority', 'N/A')
                })
        
        # æŒ‰å¯¹æ¯”å¾—åˆ†æ’åº
        results.sort(key=lambda x: x['contrast_score'], reverse=True)
        
        print(f"âœ… æ‰¾åˆ° {len(results)} ä¸ª{target_class}å€™é€‰æ¡†")
        
        return results
    
    def detect_multiple_targets(self, image: np.ndarray, target_classes: List[str],
                              strategy: str = 'multi_threshold_saliency',
                              max_regions: int = 50) -> Dict[str, List[Dict]]:
        """
        æ£€æµ‹å¤šä¸ªç›®æ ‡ç±»åˆ«
        
        å‚æ•°:
            image: è¾“å…¥å›¾åƒ
            target_classes: ç›®æ ‡ç±»åˆ«åˆ—è¡¨
            strategy: é‡‡æ ·ç­–ç•¥
            max_regions: æœ€å¤§åŒºåŸŸæ•°
        
        è¿”å›:
            æ¯ä¸ªç±»åˆ«çš„æ£€æµ‹ç»“æœå­—å…¸
        """
        print(f"\nğŸ¯ å¤šç›®æ ‡æ£€æµ‹: {target_classes}")
        
        all_results = {}
        
        for target_class in target_classes:
            results = self.detect_target_with_contrastive_learning(
                image, target_class, strategy, max_regions
            )
            all_results[target_class] = results
        
        return all_results
    
    def get_detection_statistics(self, results: List[Dict]) -> Dict:
        """
        è·å–æ£€æµ‹ç»Ÿè®¡ä¿¡æ¯
        
        å‚æ•°:
            results: æ£€æµ‹ç»“æœåˆ—è¡¨
        
        è¿”å›:
            ç»Ÿè®¡ä¿¡æ¯å­—å…¸
        """
        if not results:
            return {}
        
        contrast_scores = [r['contrast_score'] for r in results]
        target_scores = [r['target_score'] for r in results]
        
        stats = {
            'total_detections': len(results),
            'contrast_score_stats': {
                'mean': np.mean(contrast_scores),
                'std': np.std(contrast_scores),
                'min': np.min(contrast_scores),
                'max': np.max(contrast_scores)
            },
            'target_score_stats': {
                'mean': np.mean(target_scores),
                'std': np.std(target_scores),
                'min': np.min(target_scores),
                'max': np.max(target_scores)
            }
        }
        
        return stats


def main():
    """æµ‹è¯•ç›®æ ‡æ£€æµ‹å™¨"""
    print("=" * 70)
    print("æµ‹è¯•å®éªŒç›®æ ‡æ£€æµ‹å™¨")
    print("=" * 70)
    
    # æµ‹è¯•å›¾åƒ
    test_image_path = "assets/airport.jpg"
    if not os.path.exists(test_image_path):
        print(f"âŒ æµ‹è¯•å›¾åƒä¸å­˜åœ¨: {test_image_path}")
        return
    
    # åŠ è½½æµ‹è¯•å›¾åƒ
    image = cv2.imread(test_image_path)
    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
    
    # åˆ›å»ºç›®æ ‡æ£€æµ‹å™¨
    detector = ExperimentTargetDetector()
    
    # æµ‹è¯•å•ç›®æ ‡æ£€æµ‹
    target_class = "airplane"
    results = detector.detect_target_with_contrastive_learning(
        image, target_class, max_regions=30
    )
    
    # è·å–ç»Ÿè®¡ä¿¡æ¯
    stats = detector.get_detection_statistics(results)
    print(f"\nğŸ“Š æ£€æµ‹ç»Ÿè®¡:")
    print(f"  æ€»æ£€æµ‹æ•°: {stats.get('total_detections', 0)}")
    print(f"  å¯¹æ¯”åˆ†æ•°èŒƒå›´: {stats.get('contrast_score_stats', {}).get('min', 0):.3f} - {stats.get('contrast_score_stats', {}).get('max', 0):.3f}")
    
    # æ˜¾ç¤ºæ£€æµ‹ç»“æœ
    print(f"\nğŸ“‹ æ£€æµ‹ç»“æœ:")
    for i, result in enumerate(results[:5]):  # æ˜¾ç¤ºå‰5ä¸ª
        print(f"  æ£€æµ‹ {i+1}:")
        print(f"    ä½ç½®: {result['bbox']}")
        print(f"    ç±»åˆ«: {result['target_class']}")
        print(f"    å¯¹æ¯”åˆ†æ•°: {result['contrast_score']:.3f}")
    
    # æµ‹è¯•å¤šç›®æ ‡æ£€æµ‹
    print(f"\n{'='*50}")
    print("æµ‹è¯•å¤šç›®æ ‡æ£€æµ‹")
    print(f"{'='*50}")
    
    target_classes = ["airplane", "building", "runway"]
    all_results = detector.detect_multiple_targets(
        image, target_classes, max_regions=30
    )
    
    for target_class, class_results in all_results.items():
        print(f"\n{target_class}: {len(class_results)} ä¸ªæ£€æµ‹ç»“æœ")
    
    print("\nâœ… ç›®æ ‡æ£€æµ‹å™¨æµ‹è¯•å®Œæˆ!")


if __name__ == "__main__":
    main()
