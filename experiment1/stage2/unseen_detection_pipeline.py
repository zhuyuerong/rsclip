#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æœªè§ç›®æ ‡æ£€æµ‹Pipeline
åŸºäºRemoteCLIPå’Œå¯¹æ¯”å­¦ä¹ çš„æœªè§ç›®æ ‡æ£€æµ‹ç³»ç»Ÿ
"""

import torch
import torch.nn.functional as F
import open_clip
from PIL import Image
import numpy as np
import cv2
from typing import List, Dict, Tuple
import argparse

from sampling import sample_regions
from wordnet_vocabulary import get_full_class_list, get_expansion_words, WORDNET_80_CLASSES
from bbox_refinement import BBoxRefinement, compute_saliency_map


class UnseenDetectionPipeline:
    """æœªè§ç›®æ ‡æ£€æµ‹Pipeline"""
    
    def __init__(self, model_name='RN50', device='cuda'):
        """
        åˆå§‹åŒ–Pipeline
        
        å‚æ•°:
            model_name: RemoteCLIPæ¨¡å‹åç§°
            device: è®¡ç®—è®¾å¤‡
        """
        self.device = device if torch.cuda.is_available() else 'cpu'
        self.model_name = model_name
        
        # åŠ è½½æ¨¡å‹
        print(f"ğŸ”„ åŠ è½½RemoteCLIPæ¨¡å‹: {model_name}")
        self.model, _, self.preprocess = open_clip.create_model_and_transforms(model_name)
        self.tokenizer = open_clip.get_tokenizer(model_name)
        
        checkpoint_path = f"checkpoints/RemoteCLIP-{model_name}.pt"
        ckpt = torch.load(checkpoint_path, map_location="cpu")
        self.model.load_state_dict(ckpt)
        self.model = self.model.to(self.device).eval()
        print(f"âœ… æ¨¡å‹å·²åŠ è½½åˆ° {self.device}")
        
        # è¯è¡¨å’Œç‰¹å¾ç¼“å­˜
        self.class_list = None
        self.text_features = None
        self.expansion_indices = None
        
        # æ¡†å¾®è°ƒå™¨
        self.bbox_refiner = BBoxRefinement(self.model, self.preprocess, self.device)
        
    def setup_vocabulary(self, unseen_class=None):
        """
        è®¾ç½®è¯è¡¨
        
        å‚æ•°:
            unseen_class: æœªè§ç±»åˆ«åç§°ï¼ˆå¯é€‰ï¼‰
        """
        # æ„å»ºå®Œæ•´ç±»åˆ«åˆ—è¡¨
        self.class_list = get_full_class_list(unseen_class)
        
        if unseen_class:
            # è®°å½•æ‰©å±•è¯çš„ç´¢å¼•
            expansion_words = get_expansion_words(unseen_class, num_words=5)
            self.expansion_indices = [
                self.class_list.index(word) for word in expansion_words
            ]
            print(f"ğŸ“Š è¯è¡¨è®¾ç½®: 80åŸºç¡€ç±» + 5æ‰©å±•è¯ + 1æœªè§ç±» = {len(self.class_list)}ç±»")
            print(f"   æœªè§ç±»åˆ«: {unseen_class}")
            print(f"   æ‰©å±•è¯: {expansion_words}")
        else:
            self.expansion_indices = None
            print(f"ğŸ“Š è¯è¡¨è®¾ç½®: {len(self.class_list)}ä¸ªåŸºç¡€ç±»åˆ«")
        
        # é¢„ç¼–ç æ–‡æœ¬ç‰¹å¾
        print("ğŸ”„ ç¼–ç æ–‡æœ¬ç‰¹å¾...")
        text_tokens = self.tokenizer(self.class_list)
        with torch.no_grad():
            self.text_features = self.model.encode_text(text_tokens.to(self.device))
            self.text_features /= self.text_features.norm(dim=-1, keepdim=True)
        print("âœ… æ–‡æœ¬ç‰¹å¾ç¼–ç å®Œæˆ")
    
    def step1_density_map(self, image, strategy='multi_threshold_saliency', max_regions=50):
        """
        Step 1: å¯†åº¦å›¾è®¡ç®— â†’ å€™é€‰åŒºåŸŸ
        
        å‚æ•°:
            image: è¾“å…¥å›¾åƒ (numpy array, RGB)
            strategy: é‡‡æ ·ç­–ç•¥
            max_regions: æœ€å¤§åŒºåŸŸæ•°
        
        è¿”å›:
            å€™é€‰åŒºåŸŸåˆ—è¡¨
        """
        print("\n" + "="*70)
        print("ğŸ“ Step 1: å¯†åº¦å›¾è®¡ç®— â†’ å€™é€‰åŒºåŸŸ")
        print("="*70)
        
        regions = sample_regions(image, strategy=strategy, max_regions=max_regions)
        print(f"âœ… æå–åˆ° {len(regions)} ä¸ªå€™é€‰åŒºåŸŸ")
        
        return regions
    
    def step2_intelligent_crop(self, image, regions):
        """
        Step 2: æ™ºèƒ½åˆ‡å‰² â†’ é¢„é€‰æ¡†ï¼ˆcropsï¼‰
        
        å‚æ•°:
            image: åŸå§‹å›¾åƒ
            regions: å€™é€‰åŒºåŸŸåˆ—è¡¨
        
        è¿”å›:
            cropsåˆ—è¡¨å’Œå¯¹åº”çš„åŒºåŸŸä¿¡æ¯
        """
        print("\n" + "="*70)
        print("ğŸ“ Step 2: æ™ºèƒ½åˆ‡å‰² â†’ é¢„é€‰æ¡†")
        print("="*70)
        
        crops = []
        valid_regions = []
        
        for idx, region in enumerate(regions):
            x1, y1, x2, y2 = region['bbox']
            
            # ç¡®ä¿åæ ‡æœ‰æ•ˆ
            x1, y1 = max(0, x1), max(0, y1)
            x2, y2 = min(image.shape[1], x2), min(image.shape[0], y2)
            
            # è£å‰ªåŒºåŸŸ
            crop = image[y1:y2, x1:x2]
            
            # æ£€æŸ¥æœ‰æ•ˆæ€§
            if crop.size == 0 or crop.shape[0] < 10 or crop.shape[1] < 10:
                continue
            
            crops.append(crop)
            valid_regions.append(region)
        
        print(f"âœ… ç”Ÿæˆ {len(crops)} ä¸ªæœ‰æ•ˆé¢„é€‰æ¡†")
        return crops, valid_regions
    
    def step3_batch_inference(self, crops):
        """
        Step 3: RemoteCLIPæ‰¹é‡æ¨ç†
        
        å‚æ•°:
            crops: é¢„é€‰æ¡†åˆ—è¡¨
        
        è¿”å›:
            ç›¸ä¼¼åº¦çŸ©é˜µ [N Ã— 86]
        """
        print("\n" + "="*70)
        print("ğŸ“ Step 3: RemoteCLIPæ‰¹é‡æ¨ç†")
        print("="*70)
        
        if self.text_features is None:
            raise ValueError("è¯·å…ˆè°ƒç”¨ setup_vocabulary() è®¾ç½®è¯è¡¨")
        
        similarity_matrix = []
        
        print(f"ğŸ”„ å¤„ç† {len(crops)} ä¸ªcrops...")
        for idx, crop in enumerate(crops):
            # è½¬æ¢ä¸ºPILå›¾åƒå¹¶é¢„å¤„ç†
            crop_pil = Image.fromarray(crop)
            crop_tensor = self.preprocess(crop_pil).unsqueeze(0)
            
            # ç¼–ç å›¾åƒ
            with torch.no_grad():
                image_features = self.model.encode_image(crop_tensor.to(self.device))
                image_features /= image_features.norm(dim=-1, keepdim=True)
                
                # è®¡ç®—ç›¸ä¼¼åº¦
                similarities = (image_features @ self.text_features.T).squeeze(0)
                similarity_matrix.append(similarities.cpu().numpy())
            
            if (idx + 1) % 10 == 0:
                print(f"   å·²å¤„ç†: {idx + 1}/{len(crops)}")
        
        similarity_matrix = np.array(similarity_matrix)  # [N, 86]
        print(f"âœ… ç›¸ä¼¼åº¦çŸ©é˜µ: {similarity_matrix.shape}")
        
        return similarity_matrix
    
    def step4_initial_filtering(self, similarity_matrix, score_threshold=0.15):
        """
        Step 4: åˆæ­¥ç­›é€‰
        
        å‚æ•°:
            similarity_matrix: ç›¸ä¼¼åº¦çŸ©é˜µ [N Ã— 86]
            score_threshold: æœ€ä½åˆ†æ•°é˜ˆå€¼
        
        è¿”å›:
            æ­£æ ·æœ¬ç´¢å¼•ã€è´Ÿæ ·æœ¬ç´¢å¼•ã€æœ‰æ•ˆç´¢å¼•
        """
        print("\n" + "="*70)
        print("ğŸ“ Step 4: åˆæ­¥ç­›é€‰")
        print("="*70)
        
        N = similarity_matrix.shape[0]
        
        # 4.1 ç§»é™¤ä½åˆ†å™ªå£°
        max_scores = similarity_matrix.max(axis=1)
        valid_mask = max_scores > score_threshold
        valid_indices = np.where(valid_mask)[0]
        
        print(f"ğŸ“Š å™ªå£°è¿‡æ»¤:")
        print(f"   - åŸå§‹æ ·æœ¬æ•°: {N}")
        print(f"   - åˆ†æ•°é˜ˆå€¼: {score_threshold}")
        print(f"   - ä¿ç•™æ ·æœ¬æ•°: {len(valid_indices)}")
        print(f"   - ç§»é™¤å™ªå£°: {N - len(valid_indices)}")
        
        # 4.2 æ ¹æ®æ‰©å±•è¯æ ‡æ³¨æ­£è´Ÿæ ·æœ¬
        positive_samples = []
        negative_samples = []
        
        if self.expansion_indices is not None:
            # è®¡ç®—æ‰©å±•è¯çš„å¹³å‡åˆ†æ•°
            expansion_scores = similarity_matrix[:, self.expansion_indices].mean(axis=1)
            
            # è®¡ç®—åŸºç¡€ç±»åˆ«çš„æœ€å¤§åˆ†æ•°
            base_scores = similarity_matrix[:, :80].max(axis=1)
            
            # æ­£æ ·æœ¬: æ‰©å±•è¯åˆ†æ•° > åŸºç¡€ç±»åˆ«åˆ†æ•°
            for idx in valid_indices:
                if expansion_scores[idx] > base_scores[idx]:
                    positive_samples.append(idx)
                else:
                    negative_samples.append(idx)
            
            print(f"\nğŸ“Š æ­£è´Ÿæ ·æœ¬æ ‡æ³¨:")
            print(f"   - æ­£æ ·æœ¬ï¼ˆå¯èƒ½çš„æœªè§ç›®æ ‡ï¼‰: {len(positive_samples)}")
            print(f"   - è´Ÿæ ·æœ¬ï¼ˆå¹²æ‰°èƒŒæ™¯ï¼‰: {len(negative_samples)}")
        else:
            print("\nâš ï¸  æœªè®¾ç½®æœªè§ç±»åˆ«ï¼Œè·³è¿‡æ­£è´Ÿæ ·æœ¬æ ‡æ³¨")
        
        return positive_samples, negative_samples, valid_indices
    
    def step5_contrastive_refinement(self, image, crops, regions, similarity_matrix, 
                                     positive_samples, negative_samples,
                                     refine_bbox=True, refine_method='both'):
        """
        Step 5: å¯¹æ¯”å­¦ä¹ ç²¾åŒ– + æ¡†å¾®è°ƒ
        
        å‚æ•°:
            image: åŸå§‹å›¾åƒ
            crops: é¢„é€‰æ¡†åˆ—è¡¨
            regions: åŒºåŸŸä¿¡æ¯
            similarity_matrix: ç›¸ä¼¼åº¦çŸ©é˜µ
            positive_samples: æ­£æ ·æœ¬ç´¢å¼•
            negative_samples: è´Ÿæ ·æœ¬ç´¢å¼•
            refine_bbox: æ˜¯å¦è¿›è¡Œæ¡†å¾®è°ƒ
            refine_method: æ¡†å¾®è°ƒæ–¹æ³• ('position', 'scale', 'both', 'boundary')
        
        è¿”å›:
            ç²¾åŒ–åçš„æ£€æµ‹ç»“æœ
        """
        print("\n" + "="*70)
        print("ğŸ“ Step 5: å¯¹æ¯”å­¦ä¹ ç²¾åŒ–" + (" + æ¡†å¾®è°ƒ" if refine_bbox else ""))
        print("="*70)
        
        if len(positive_samples) == 0:
            print("âš ï¸  æ²¡æœ‰æ­£æ ·æœ¬ï¼Œè·³è¿‡ç²¾åŒ–")
            return []
        
        # 5.1 æ„å»ºæ­£è´Ÿæ ·æœ¬åŸå‹
        print("ğŸ”„ æ„å»ºç‰¹å¾åŸå‹...")
        
        # ç¼–ç æ­£æ ·æœ¬ç‰¹å¾
        positive_features = []
        for idx in positive_samples:
            crop_pil = Image.fromarray(crops[idx])
            crop_tensor = self.preprocess(crop_pil).unsqueeze(0)
            with torch.no_grad():
                feat = self.model.encode_image(crop_tensor.to(self.device))
                feat /= feat.norm(dim=-1, keepdim=True)
                positive_features.append(feat)
        
        positive_prototype = torch.cat(positive_features).mean(dim=0, keepdim=True)
        
        # ç¼–ç è´Ÿæ ·æœ¬ç‰¹å¾ï¼ˆå¦‚æœæœ‰ï¼‰
        if len(negative_samples) > 0:
            negative_features = []
            for idx in negative_samples[:min(10, len(negative_samples))]:  # é™åˆ¶æ•°é‡
                crop_pil = Image.fromarray(crops[idx])
                crop_tensor = self.preprocess(crop_pil).unsqueeze(0)
                with torch.no_grad():
                    feat = self.model.encode_image(crop_tensor.to(self.device))
                    feat /= feat.norm(dim=-1, keepdim=True)
                    negative_features.append(feat)
            
            negative_prototype = torch.cat(negative_features).mean(dim=0, keepdim=True)
        else:
            negative_prototype = None
        
        print("âœ… åŸå‹æ„å»ºå®Œæˆ")
        
        # 5.2 è®¡ç®—æ˜¾è‘—æ€§å›¾ï¼ˆç”¨äºæ¡†å¾®è°ƒï¼‰
        if refine_bbox:
            print("ğŸ”„ è®¡ç®—æ˜¾è‘—æ€§å›¾ç”¨äºæ¡†å¾®è°ƒ...")
            saliency_map = compute_saliency_map(image)
        else:
            saliency_map = None
        
        # 5.3 é‡æ–°è¯„åˆ† + æ¡†å¾®è°ƒ
        print(f"ğŸ”„ åŸºäºåŸå‹é‡æ–°è¯„åˆ†" + (" + æ¡†å¾®è°ƒ..." if refine_bbox else "..."))
        refined_results = []
        
        for idx in positive_samples:
            crop_pil = Image.fromarray(crops[idx])
            crop_tensor = self.preprocess(crop_pil).unsqueeze(0)
            
            with torch.no_grad():
                feat = self.model.encode_image(crop_tensor.to(self.device))
                feat /= feat.norm(dim=-1, keepdim=True)
                
                # ä¸æ­£åŸå‹çš„ç›¸ä¼¼åº¦
                pos_sim = (feat @ positive_prototype.T).item()
                
                # ä¸è´ŸåŸå‹çš„ç›¸ä¼¼åº¦
                if negative_prototype is not None:
                    neg_sim = (feat @ negative_prototype.T).item()
                    # å¯¹æ¯”åˆ†æ•°
                    contrast_score = pos_sim - neg_sim
                else:
                    contrast_score = pos_sim
                
                # è·å–æœ€å¯èƒ½çš„ç±»åˆ«
                similarities = similarity_matrix[idx]
                top_class_idx = similarities.argmax()
                top_class = self.class_list[top_class_idx]
                top_score = similarities[top_class_idx]
                
                # æ‰©å±•è¯å¹³å‡åˆ†æ•°
                if self.expansion_indices:
                    expansion_score = similarities[self.expansion_indices].mean()
                else:
                    expansion_score = 0.0
                
                # æ¡†å¾®è°ƒ
                initial_bbox = regions[idx]['bbox']
                if refine_bbox and saliency_map is not None:
                    refine_result = self.bbox_refiner.refine_bbox_hybrid(
                        image=image,
                        bbox=initial_bbox,
                        saliency_map=saliency_map,
                        positive_prototype=positive_prototype,
                        negative_prototype=negative_prototype,
                        method=refine_method
                    )
                    refined_bbox = refine_result['bbox']
                    bbox_refined = refine_result.get('refined', False)
                    saliency_score = refine_result.get('saliency_score', 0.0)
                    composite_score = refine_result.get('composite_score', contrast_score)
                else:
                    refined_bbox = initial_bbox
                    bbox_refined = False
                    saliency_score = 0.0
                    composite_score = contrast_score
                
                refined_results.append({
                    'index': idx,
                    'bbox': refined_bbox,
                    'initial_bbox': initial_bbox,
                    'bbox_refined': bbox_refined,
                    'top_class': top_class,
                    'top_score': float(top_score),
                    'expansion_score': float(expansion_score),
                    'contrast_score': float(contrast_score),
                    'saliency_score': float(saliency_score),
                    'composite_score': float(composite_score),
                    'is_unseen': contrast_score > 0.1  # ç®€å•é˜ˆå€¼åˆ¤æ–­
                })
        
        # æŒ‰ç»¼åˆåˆ†æ•°æ’åº
        refined_results.sort(key=lambda x: x['composite_score'], reverse=True)
        
        # ç»Ÿè®¡æ¡†å¾®è°ƒæƒ…å†µ
        if refine_bbox:
            n_refined = sum(1 for r in refined_results if r['bbox_refined'])
            print(f"   æ¡†å¾®è°ƒç»Ÿè®¡: {n_refined}/{len(refined_results)} ä¸ªæ¡†è¢«ä¼˜åŒ–")
        
        print(f"âœ… ç²¾åŒ–å®Œæˆï¼Œå¾—åˆ° {len(refined_results)} ä¸ªå€™é€‰ç›®æ ‡")
        
        return refined_results
    
    def run_pipeline(self, image, unseen_class=None, strategy='multi_threshold_saliency',
                     max_regions=50, score_threshold=0.15, top_k=10,
                     refine_bbox=True, refine_method='both'):
        """
        è¿è¡Œå®Œæ•´Pipeline
        
        å‚æ•°:
            image: è¾“å…¥å›¾åƒ (numpy array, RGB)
            unseen_class: æœªè§ç±»åˆ«åç§°
            strategy: é‡‡æ ·ç­–ç•¥
            max_regions: æœ€å¤§åŒºåŸŸæ•°
            score_threshold: åˆ†æ•°é˜ˆå€¼
            top_k: è¿”å›å‰Kä¸ªç»“æœ
            refine_bbox: æ˜¯å¦è¿›è¡Œæ¡†å¾®è°ƒ
            refine_method: æ¡†å¾®è°ƒæ–¹æ³• ('position', 'scale', 'both', 'boundary')
        
        è¿”å›:
            æ£€æµ‹ç»“æœåˆ—è¡¨
        """
        print("\n" + "="*70)
        print("ğŸš€ æœªè§ç›®æ ‡æ£€æµ‹Pipeline")
        print("="*70)
        
        # è®¾ç½®è¯è¡¨
        self.setup_vocabulary(unseen_class)
        
        # Step 1: å¯†åº¦å›¾è®¡ç®—
        regions = self.step1_density_map(image, strategy, max_regions)
        
        # Step 2: æ™ºèƒ½åˆ‡å‰²
        crops, valid_regions = self.step2_intelligent_crop(image, regions)
        
        # Step 3: æ‰¹é‡æ¨ç†
        similarity_matrix = self.step3_batch_inference(crops)
        
        # Step 4: åˆæ­¥ç­›é€‰
        positive_samples, negative_samples, valid_indices = self.step4_initial_filtering(
            similarity_matrix, score_threshold
        )
        
        # Step 5: å¯¹æ¯”å­¦ä¹ ç²¾åŒ– + æ¡†å¾®è°ƒ
        refined_results = self.step5_contrastive_refinement(
            image, crops, valid_regions, similarity_matrix, 
            positive_samples, negative_samples,
            refine_bbox=refine_bbox, refine_method=refine_method
        )
        
        # è¿”å›Top-Kç»“æœ
        final_results = refined_results[:top_k]
        
        print("\n" + "="*70)
        print(f"âœ… Pipelineå®Œæˆ! è¿”å›å‰{len(final_results)}ä¸ªç»“æœ")
        print("="*70)
        
        return final_results


def main():
    parser = argparse.ArgumentParser(description='æœªè§ç›®æ ‡æ£€æµ‹Pipeline')
    parser.add_argument('--image', type=str, default='assets/airport.jpg',
                        help='è¾“å…¥å›¾åƒè·¯å¾„')
    parser.add_argument('--unseen-class', type=str, default=None,
                        help='æœªè§ç±»åˆ«åç§°ï¼ˆä¾‹å¦‚: wind turbineï¼‰')
    parser.add_argument('--model', type=str, default='RN50',
                        choices=['RN50', 'ViT-B-32', 'ViT-L-14'],
                        help='æ¨¡å‹é€‰æ‹©')
    parser.add_argument('--strategy', type=str, default='multi_threshold_saliency',
                        choices=['layered', 'pyramid', 'multi_threshold_saliency'],
                        help='é‡‡æ ·ç­–ç•¥')
    parser.add_argument('--max-regions', type=int, default=50,
                        help='æœ€å¤§åŒºåŸŸæ•°')
    parser.add_argument('--threshold', type=float, default=0.15,
                        help='åˆ†æ•°é˜ˆå€¼')
    parser.add_argument('--top-k', type=int, default=10,
                        help='è¿”å›å‰Kä¸ªç»“æœ')
    parser.add_argument('--refine-bbox', action='store_true', default=True,
                        help='å¯ç”¨æ¡†å¾®è°ƒï¼ˆé»˜è®¤å¯ç”¨ï¼‰')
    parser.add_argument('--no-refine-bbox', action='store_true',
                        help='ç¦ç”¨æ¡†å¾®è°ƒ')
    parser.add_argument('--refine-method', type=str, default='both',
                        choices=['position', 'scale', 'both', 'boundary'],
                        help='æ¡†å¾®è°ƒæ–¹æ³•: position(ä½ç½®), scale(å°ºå¯¸), both(ä¸¤è€…), boundary(è¾¹ç•Œ)')
    
    args = parser.parse_args()
    
    # åŠ è½½å›¾åƒ
    image = cv2.imread(args.image)
    if image is None:
        print(f"âŒ æ— æ³•åŠ è½½å›¾åƒ: {args.image}")
        return
    
    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
    
    # åˆ›å»ºPipeline
    pipeline = UnseenDetectionPipeline(model_name=args.model)
    
    # è¿è¡ŒPipeline
    refine_bbox = args.refine_bbox and not args.no_refine_bbox
    
    results = pipeline.run_pipeline(
        image=image,
        unseen_class=args.unseen_class,
        strategy=args.strategy,
        max_regions=args.max_regions,
        score_threshold=args.threshold,
        top_k=args.top_k,
        refine_bbox=refine_bbox,
        refine_method=args.refine_method
    )
    
    # æ‰“å°ç»“æœ
    print("\n" + "="*70)
    print("ğŸ¯ æ£€æµ‹ç»“æœ")
    print("="*70)
    
    for i, result in enumerate(results, 1):
        print(f"\nç»“æœ {i}:")
        if result.get('bbox_refined', False):
            print(f"  åˆå§‹ä½ç½®: {result['initial_bbox']}")
            print(f"  ä¼˜åŒ–ä½ç½®: {result['bbox']} âœ“")
        else:
            print(f"  ä½ç½®: {result['bbox']}")
        print(f"  æœ€å¯èƒ½ç±»åˆ«: {result['top_class']} ({result['top_score']:.3f})")
        print(f"  æ‰©å±•è¯åˆ†æ•°: {result['expansion_score']:.3f}")
        print(f"  å¯¹æ¯”åˆ†æ•°: {result['contrast_score']:.3f}")
        print(f"  æ˜¾è‘—æ€§åˆ†æ•°: {result.get('saliency_score', 0.0):.3f}")
        print(f"  ç»¼åˆåˆ†æ•°: {result.get('composite_score', 0.0):.3f}")
        print(f"  æ˜¯å¦æœªè§ç›®æ ‡: {'âœ“' if result['is_unseen'] else 'âœ—'}")


if __name__ == "__main__":
    main()

