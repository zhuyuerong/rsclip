# -*- coding: utf-8 -*-
"""
CLIP Surgeryæ¨¡å‹æ„å»ºå·¥å…·
"""
from torch import nn
from .clip_model import CLIP
from .clip_surgery_model import CLIPSurgery
from typing import Optional, Tuple
import torch
import os


def build_model(model_name: str,
                checkpoint_path: str,
                device: str = "cuda") -> Tuple[nn.Module, callable]:
    """
    æ„å»ºCLIPæˆ–CLIPSurgeryæ¨¡å‹
    
    Args:
        model_name: æ¨¡å‹æ¶æ„
            - "clip": åŸå§‹CLIPæ¶æ„ï¼ˆæ— VVæ³¨æ„åŠ›ï¼‰
            - "surgeryclip": Surgeryæ¶æ„ï¼ˆæœ‰VVæ³¨æ„åŠ›ï¼‰
        checkpoint_path: æƒé‡æ–‡ä»¶è·¯å¾„ï¼ˆå¿…é¡»ï¼‰
        device: è®¾å¤‡
    
    Returns:
        model: CLIPæ¨¡å‹
        preprocess: é¢„å¤„ç†å‡½æ•°
    """
    if not os.path.exists(checkpoint_path):
        raise FileNotFoundError(f"æƒé‡æ–‡ä»¶ä¸å­˜åœ¨: {checkpoint_path}")
    
    print(f"ğŸ“¥ åŠ è½½æƒé‡: {checkpoint_path}")
    
    # åŠ è½½æƒé‡æ–‡ä»¶
    state_dict = _load_checkpoint(checkpoint_path)
    
    # æ ¹æ®model_nameæ„å»ºå¯¹åº”æ¶æ„
    if model_name == "clip":
        print("ğŸ”§ ä½¿ç”¨CLIPæ¶æ„ï¼ˆæ— VVæ³¨æ„åŠ›ï¼‰")
        model = _build_clip_model(state_dict)
    elif model_name == "surgeryclip":
        print("ğŸ”§ ä½¿ç”¨Surgeryæ¶æ„ï¼ˆæœ‰VVæ³¨æ„åŠ›ï¼‰")
        model = _build_surgery_model(state_dict)
    else:
        raise ValueError(f"æœªçŸ¥çš„model_name: {model_name}ï¼Œåº”è¯¥æ˜¯ 'clip' æˆ– 'surgeryclip'")
    
    # ç§»åŠ¨åˆ°è®¾å¤‡
    model = model.to(device)
    model.eval()
    
    # åˆ›å»ºé¢„å¤„ç†å‡½æ•°
    from .clip import _transform
    preprocess = _transform(model.visual.input_resolution)
    
    print("âœ… æ¨¡å‹åŠ è½½æˆåŠŸ")
    
    return model, preprocess


def _load_checkpoint(checkpoint_path: str) -> dict:
    """åŠ è½½æƒé‡æ–‡ä»¶ï¼Œè¿”å›state_dict"""
    try:
        # å…ˆå°è¯•ä½œä¸ºTorchScriptåŠ è½½ï¼ˆCLIPå®˜æ–¹æƒé‡é€šå¸¸æ˜¯è¿™ç§æ ¼å¼ï¼‰
        try:
            jit_model = torch.jit.load(checkpoint_path, map_location='cpu')
            # å¦‚æœTorchScriptæ¨¡å‹æœ‰state_dictæ–¹æ³•ï¼Œæå–state_dict
            if hasattr(jit_model, 'state_dict'):
                try:
                    return jit_model.state_dict()
                except:
                    # å¦‚æœæ— æ³•æå–state_dictï¼Œå°è¯•å…¶ä»–æ–¹æ³•
                    pass
        except (RuntimeError, Exception):
            # ä¸æ˜¯TorchScriptæ ¼å¼ï¼Œç»§ç»­å°è¯•å…¶ä»–æ ¼å¼
            pass
        
        # å°è¯•ç›´æ¥åŠ è½½ä¸ºstate_dictæˆ–åŒ…å«state_dictçš„å­—å…¸
        try:
            checkpoint = torch.load(checkpoint_path, map_location='cpu', weights_only=False)
        except TypeError:
            # æ—§ç‰ˆæœ¬PyTorchä¸æ”¯æŒweights_only
            checkpoint = torch.load(checkpoint_path, map_location='cpu')
        
        # æå–state_dict
        if isinstance(checkpoint, dict):
            if 'state_dict' in checkpoint:
                return checkpoint['state_dict']
            elif 'model' in checkpoint:
                return checkpoint['model']
            else:
                # å‡è®¾checkpointæœ¬èº«å°±æ˜¯state_dict
                return checkpoint
        else:
            raise ValueError("æ— æ³•ä»checkpointä¸­æå–state_dict")
    
    except Exception as e:
        raise RuntimeError(f"åŠ è½½æƒé‡å¤±è´¥: {e}")


def _build_clip_model(state_dict: dict) -> CLIP:
    """ä»state_dictæ„å»ºCLIPæ¨¡å‹ï¼ˆæ— VVæ³¨æ„åŠ›ï¼‰"""
    model_config = _extract_model_config(state_dict)
    
    model = CLIP(
        embed_dim=model_config['embed_dim'],
        image_resolution=model_config['image_resolution'],
        vision_layers=model_config['vision_layers'],
        vision_width=model_config['vision_width'],
        vision_patch_size=model_config['vision_patch_size'],
        context_length=model_config['context_length'],
        vocab_size=model_config['vocab_size'],
        transformer_width=model_config['transformer_width'],
        transformer_heads=model_config['transformer_heads'],
        transformer_layers=model_config['transformer_layers']
    )
    
    # åˆ é™¤ä¸éœ€è¦çš„é”®
    for key in ["input_resolution", "context_length", "vocab_size"]:
        if key in state_dict:
            del state_dict[key]
    
    # åŠ è½½æƒé‡
    model.load_state_dict(state_dict)
    
    return model


def _build_surgery_model(state_dict: dict) -> CLIPSurgery:
    """ä»state_dictæ„å»ºCLIPSurgeryæ¨¡å‹ï¼ˆæœ‰VVæ³¨æ„åŠ›ï¼‰"""
    model_config = _extract_model_config(state_dict)
    
    model = CLIPSurgery(
        embed_dim=model_config['embed_dim'],
        image_resolution=model_config['image_resolution'],
        vision_layers=model_config['vision_layers'],
        vision_width=model_config['vision_width'],
        vision_patch_size=model_config['vision_patch_size'],
        context_length=model_config['context_length'],
        vocab_size=model_config['vocab_size'],
        transformer_width=model_config['transformer_width'],
        transformer_heads=model_config['transformer_heads'],
        transformer_layers=model_config['transformer_layers']
    )
    
    # åˆ é™¤ä¸éœ€è¦çš„é”®
    for key in ["input_resolution", "context_length", "vocab_size"]:
        if key in state_dict:
            del state_dict[key]
    
    # åŠ è½½æƒé‡
    model.load_state_dict(state_dict)
    
    return model


def _extract_model_config(state_dict: dict) -> dict:
    """ä»state_dictä¸­æå–æ¨¡å‹é…ç½®"""
    # åˆ¤æ–­æ˜¯ViTè¿˜æ˜¯ResNet
    vit = "visual.proj" in state_dict
    
    if vit:
        # ViTé…ç½®
        vision_width = state_dict["visual.conv1.weight"].shape[0]
        vision_layers = len([k for k in state_dict.keys() 
                            if k.startswith("visual.") and k.endswith(".attn.in_proj_weight")])
        vision_patch_size = state_dict["visual.conv1.weight"].shape[-1]
        grid_size = round((state_dict["visual.positional_embedding"].shape[0] - 1) ** 0.5)
        image_resolution = vision_patch_size * grid_size
    else:
        # ResNeté…ç½®
        counts = [len(set(k.split(".")[2] for k in state_dict 
                         if k.startswith(f"visual.layer{b}"))) 
                 for b in [1, 2, 3, 4]]
        vision_layers = tuple(counts)
        vision_width = state_dict["visual.layer1.0.conv1.weight"].shape[0]
        output_width = round((state_dict["visual.attnpool.positional_embedding"].shape[0] - 1) ** 0.5)
        vision_patch_size = None
        image_resolution = output_width * 32
    
    # æ–‡æœ¬ç¼–ç å™¨é…ç½®
    embed_dim = state_dict["text_projection"].shape[1]
    context_length = state_dict["positional_embedding"].shape[0]
    vocab_size = state_dict["token_embedding.weight"].shape[0]
    transformer_width = state_dict["ln_final.weight"].shape[0]
    transformer_heads = transformer_width // 64
    transformer_layers = len(set(k.split(".")[2] for k in state_dict 
                                 if k.startswith("transformer.resblocks")))
    
    return {
        'embed_dim': embed_dim,
        'image_resolution': image_resolution,
        'vision_layers': vision_layers,
        'vision_width': vision_width,
        'vision_patch_size': vision_patch_size,
        'context_length': context_length,
        'vocab_size': vocab_size,
        'transformer_width': transformer_width,
        'transformer_heads': transformer_heads,
        'transformer_layers': transformer_layers
    }
