#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
è¯Šæ–­è¯„ä¼°é—®é¢˜ï¼šè®­ç»ƒæŸå¤±ä¸‹é™ä½†mAP=0
æ£€æŸ¥ï¼šåæ ‡ç³»ç»Ÿã€ç½®ä¿¡åº¦ã€IoUã€ç±»åˆ«æ˜ å°„
"""

import torch
import argparse
from pathlib import Path
import yaml
from tqdm import tqdm
import numpy as np
from collections import defaultdict
import matplotlib.pyplot as plt
from PIL import Image
import torchvision.transforms as transforms

import sys
sys.path.insert(0, str(Path(__file__).parent))
sys.path.insert(0, str(Path(__file__).parent.parent.parent))

from models.improved_direct_detection_detector import create_improved_direct_detection_detector
from datasets.dior_detection import get_detection_dataloader
from utils.class_split import ALL_CLASSES

def compute_iou(box1, box2):
    """è®¡ç®—IoU"""
    # box: [x1, y1, x2, y2]
    x1_min, y1_min, x1_max, y1_max = box1
    x2_min, y2_min, x2_max, y2_max = box2
    
    inter_x1 = max(x1_min, x2_min)
    inter_y1 = max(y1_min, y2_min)
    inter_x2 = min(x1_max, x2_max)
    inter_y2 = min(y1_max, y2_max)
    
    if inter_x2 <= inter_x1 or inter_y2 <= inter_y1:
        return 0.0
    
    inter_area = (inter_x2 - inter_x1) * (inter_y2 - inter_y1)
    box1_area = (x1_max - x1_min) * (y1_max - y1_min)
    box2_area = (x2_max - x2_min) * (y2_max - y2_min)
    union_area = box1_area + box2_area - inter_area
    
    if union_area == 0:
        return 0.0
    
    return inter_area / union_area

def diagnose_model(model, dataloader, device, conf_threshold=0.1, nms_threshold=0.5, num_samples=10):
    """è¯Šæ–­æ¨¡å‹è¾“å‡º"""
    model.eval()
    
    # ç»Ÿè®¡ä¿¡æ¯
    stats = {
        'num_detections_before_conf': [],
        'num_detections_after_conf': [],
        'num_detections_after_nms': [],
        'confidences': [],
        'pred_box_coords': {'x1': [], 'y1': [], 'x2': [], 'y2': []},
        'gt_box_coords': {'x1': [], 'y1': [], 'x2': [], 'y2': []},
        'ious': [],
        'class_predictions': [],
        'gt_classes': []
    }
    
    sample_detections = []  # ä¿å­˜å‰å‡ ä¸ªæ ·æœ¬çš„è¯¦ç»†ä¿¡æ¯
    
    print("\n" + "="*80)
    print("ğŸ” å¼€å§‹è¯Šæ–­...")
    print("="*80)
    
    with torch.no_grad():
        for batch_idx, batch in enumerate(tqdm(dataloader, desc="è¯Šæ–­ä¸­")):
            if batch_idx >= num_samples:
                break
                
            images = batch['images'].to(device)
            text_queries = batch['text_queries']
            gt_boxes = batch['boxes']  # List of tensors
            gt_labels = batch['labels']  # List of tensors
            
            B = images.shape[0]
            
            # ===== æ£€æŸ¥1: æ¨¡å‹åŸå§‹è¾“å‡º =====
            outputs = model(images, text_queries)
            boxes_raw = outputs['boxes']  # [B, C, H, W, 4]
            confidences_raw = outputs['confidences']  # [B, C, H, W]
            
            # ç»Ÿè®¡ç½®ä¿¡åº¦åˆ†å¸ƒ
            conf_values = confidences_raw.cpu().numpy().flatten()
            stats['confidences'].extend(conf_values.tolist())
            
            # ===== æ£€æŸ¥2: ç½®ä¿¡åº¦è¿‡æ»¤å‰ =====
            num_before_conf = (confidences_raw > conf_threshold).sum().item()
            stats['num_detections_before_conf'].append(num_before_conf)
            
            # ===== æ£€æŸ¥3: ä½¿ç”¨inferenceæ–¹æ³• =====
            detections = model.inference(
                images, text_queries,
                conf_threshold=conf_threshold,
                nms_threshold=nms_threshold
            )
            
            # ç»Ÿè®¡æ¯ä¸ªå›¾åƒçš„æ£€æµ‹æ•°é‡
            for b in range(B):
                img_detections = detections[b]
                stats['num_detections_after_nms'].append(len(img_detections))
                
                # æ”¶é›†é¢„æµ‹æ¡†åæ ‡
                for det in img_detections:
                    box = det['box']
                    if isinstance(box, torch.Tensor):
                        box = box.cpu().numpy()
                    elif isinstance(box, (list, tuple)):
                        box = np.array(box)
                    else:
                        box = np.array([box['xmin'], box['ymin'], box['xmax'], box['ymax']])
                    
                    x1, y1, x2, y2 = box
                    stats['pred_box_coords']['x1'].append(x1)
                    stats['pred_box_coords']['y1'].append(y1)
                    stats['pred_box_coords']['x2'].append(x2)
                    stats['pred_box_coords']['y2'].append(y2)
                    stats['class_predictions'].append(det['class'])
                
                # æ”¶é›†GTæ¡†åæ ‡
                gt_boxes_b = gt_boxes[b]
                gt_labels_b = gt_labels[b]
                
                if isinstance(gt_boxes_b, torch.Tensor):
                    gt_boxes_b = gt_boxes_b.cpu().numpy()
                else:
                    gt_boxes_b = np.array(gt_boxes_b)
                
                for gt_box, gt_label in zip(gt_boxes_b, gt_labels_b):
                    if len(gt_box) >= 4:
                        x1, y1, x2, y2 = gt_box[:4]
                        stats['gt_box_coords']['x1'].append(x1)
                        stats['gt_box_coords']['y1'].append(y1)
                        stats['gt_box_coords']['x2'].append(x2)
                        stats['gt_box_coords']['y2'].append(y2)
                        stats['gt_classes'].append(gt_label.item() if isinstance(gt_label, torch.Tensor) else gt_label)
                
                # è®¡ç®—IoU
                if len(img_detections) > 0 and len(gt_boxes_b) > 0:
                    for det in img_detections:
                        box_pred = det['box']
                        if isinstance(box_pred, torch.Tensor):
                            box_pred = box_pred.cpu().numpy()
                        elif isinstance(box_pred, (list, tuple)):
                            box_pred = np.array(box_pred)
                        else:
                            box_pred = np.array([box_pred['xmin'], box_pred['ymin'], box_pred['xmax'], box_pred['ymax']])
                        
                        max_iou = 0.0
                        for gt_box in gt_boxes_b:
                            if len(gt_box) >= 4:
                                iou = compute_iou(box_pred, gt_box[:4])
                                max_iou = max(max_iou, iou)
                        stats['ious'].append(max_iou)
                
                # ä¿å­˜æ ·æœ¬ä¿¡æ¯
                if batch_idx < 5:
                    sample_detections.append({
                        'image_idx': batch_idx * B + b,
                        'detections': img_detections,
                        'gt_boxes': gt_boxes_b,
                        'gt_labels': gt_labels_b,
                        'image': images[b].cpu()
                    })
    
    # ===== æ‰“å°è¯Šæ–­ç»“æœ =====
    print("\n" + "="*80)
    print("ğŸ“Š è¯Šæ–­ç»“æœ")
    print("="*80)
    
    # æ£€æŸ¥1: é¢„æµ‹æ¡†æ•°é‡
    print("\nã€æ£€æŸ¥1ã€‘é¢„æµ‹æ¡†æ•°é‡ç»Ÿè®¡")
    print(f"  ç½®ä¿¡åº¦è¿‡æ»¤å‰ (>{conf_threshold}): {np.mean(stats['num_detections_before_conf']):.1f} ä¸ª/å›¾")
    print(f"  NMSå: {np.mean(stats['num_detections_after_nms']):.1f} ä¸ª/å›¾")
    print(f"  æ€»æ£€æµ‹æ•°: {sum(stats['num_detections_after_nms'])} ä¸ª")
    
    if np.mean(stats['num_detections_after_nms']) == 0:
        print("  âš ï¸  è­¦å‘Š: NMSåæ²¡æœ‰æ£€æµ‹æ¡†ï¼")
    
    # æ£€æŸ¥2: ç½®ä¿¡åº¦åˆ†å¸ƒ
    print("\nã€æ£€æŸ¥2ã€‘ç½®ä¿¡åº¦åˆ†å¸ƒ")
    if len(stats['confidences']) > 0:
        conf_arr = np.array(stats['confidences'])
        print(f"  æœ€å¤§å€¼: {conf_arr.max():.4f}")
        print(f"  å¹³å‡å€¼: {conf_arr.mean():.4f}")
        print(f"  ä¸­ä½æ•°: {np.median(conf_arr):.4f}")
        print(f"  æ ‡å‡†å·®: {conf_arr.std():.4f}")
        print(f"  >0.1çš„æ•°é‡: {(conf_arr > 0.1).sum()} ({100*(conf_arr > 0.1).mean():.2f}%)")
        print(f"  >0.01çš„æ•°é‡: {(conf_arr > 0.01).sum()} ({100*(conf_arr > 0.01).mean():.2f}%)")
        print(f"  >0.001çš„æ•°é‡: {(conf_arr > 0.001).sum()} ({100*(conf_arr > 0.001).mean():.2f}%)")
        
        if conf_arr.max() < conf_threshold:
            print(f"  âš ï¸  è­¦å‘Š: æœ€å¤§ç½®ä¿¡åº¦({conf_arr.max():.4f}) < é˜ˆå€¼({conf_threshold})ï¼")
    else:
        print("  âš ï¸  è­¦å‘Š: æ²¡æœ‰ç½®ä¿¡åº¦æ•°æ®ï¼")
    
    # æ£€æŸ¥3: åæ ‡èŒƒå›´
    print("\nã€æ£€æŸ¥3ã€‘åæ ‡èŒƒå›´æ£€æŸ¥")
    if len(stats['pred_box_coords']['x1']) > 0:
        pred_x1 = np.array(stats['pred_box_coords']['x1'])
        pred_y1 = np.array(stats['pred_box_coords']['y1'])
        pred_x2 = np.array(stats['pred_box_coords']['x2'])
        pred_y2 = np.array(stats['pred_box_coords']['y2'])
        
        print("  é¢„æµ‹æ¡†åæ ‡:")
        print(f"    x1: min={pred_x1.min():.4f}, max={pred_x1.max():.4f}, mean={pred_x1.mean():.4f}")
        print(f"    y1: min={pred_y1.min():.4f}, max={pred_y1.max():.4f}, mean={pred_y1.mean():.4f}")
        print(f"    x2: min={pred_x2.min():.4f}, max={pred_x2.max():.4f}, mean={pred_x2.mean():.4f}")
        print(f"    y2: min={pred_y2.min():.4f}, max={pred_y2.max():.4f}, mean={pred_y2.mean():.4f}")
        
        # æ£€æŸ¥æ˜¯å¦å½’ä¸€åŒ–
        if pred_x2.max() <= 1.0 and pred_y2.max() <= 1.0:
            print("  âœ… é¢„æµ‹æ¡†ä¼¼ä¹æ˜¯å½’ä¸€åŒ–åæ ‡ [0,1]")
        elif pred_x2.max() > 100:
            print("  âœ… é¢„æµ‹æ¡†ä¼¼ä¹æ˜¯åƒç´ åæ ‡")
        else:
            print("  âš ï¸  é¢„æµ‹æ¡†åæ ‡èŒƒå›´å¼‚å¸¸")
    else:
        print("  âš ï¸  è­¦å‘Š: æ²¡æœ‰é¢„æµ‹æ¡†æ•°æ®ï¼")
    
    if len(stats['gt_box_coords']['x1']) > 0:
        gt_x1 = np.array(stats['gt_box_coords']['x1'])
        gt_y1 = np.array(stats['gt_box_coords']['y1'])
        gt_x2 = np.array(stats['gt_box_coords']['x2'])
        gt_y2 = np.array(stats['gt_box_coords']['y2'])
        
        print("  GTæ¡†åæ ‡:")
        print(f"    x1: min={gt_x1.min():.4f}, max={gt_x1.max():.4f}, mean={gt_x1.mean():.4f}")
        print(f"    y1: min={gt_y1.min():.4f}, max={gt_y1.max():.4f}, mean={gt_y1.mean():.4f}")
        print(f"    x2: min={gt_x2.min():.4f}, max={gt_x2.max():.4f}, mean={gt_x2.mean():.4f}")
        print(f"    y2: min={gt_y2.min():.4f}, max={gt_y2.max():.4f}, mean={gt_y2.mean():.4f}")
        
        # æ£€æŸ¥æ˜¯å¦å½’ä¸€åŒ–
        if gt_x2.max() <= 1.0 and gt_y2.max() <= 1.0:
            print("  âœ… GTæ¡†ä¼¼ä¹æ˜¯å½’ä¸€åŒ–åæ ‡ [0,1]")
        elif gt_x2.max() > 100:
            print("  âœ… GTæ¡†ä¼¼ä¹æ˜¯åƒç´ åæ ‡")
        else:
            print("  âš ï¸  GTæ¡†åæ ‡èŒƒå›´å¼‚å¸¸")
        
        # æ£€æŸ¥åæ ‡ç³»ç»Ÿæ˜¯å¦åŒ¹é…
        if len(stats['pred_box_coords']['x1']) > 0:
            pred_max = max(pred_x2.max(), pred_y2.max())
            gt_max = max(gt_x2.max(), gt_y2.max())
            if abs(pred_max - gt_max) > 10:
                print(f"  âš ï¸  è­¦å‘Š: åæ ‡ç³»ç»Ÿå¯èƒ½ä¸åŒ¹é…ï¼")
                print(f"    é¢„æµ‹æ¡†æœ€å¤§åæ ‡: {pred_max:.2f}")
                print(f"    GTæ¡†æœ€å¤§åæ ‡: {gt_max:.2f}")
    else:
        print("  âš ï¸  è­¦å‘Š: æ²¡æœ‰GTæ¡†æ•°æ®ï¼")
    
    # æ£€æŸ¥4: IoUåˆ†å¸ƒ
    print("\nã€æ£€æŸ¥4ã€‘IoUåˆ†å¸ƒ")
    if len(stats['ious']) > 0:
        ious_arr = np.array(stats['ious'])
        print(f"  æœ€å¤§IoU: {ious_arr.max():.4f}")
        print(f"  å¹³å‡IoU: {ious_arr.mean():.4f}")
        print(f"  ä¸­ä½æ•°IoU: {np.median(ious_arr):.4f}")
        print(f"  IoU>0.5çš„æ•°é‡: {(ious_arr > 0.5).sum()} ({100*(ious_arr > 0.5).mean():.2f}%)")
        print(f"  IoU>0.3çš„æ•°é‡: {(ious_arr > 0.3).sum()} ({100*(ious_arr > 0.3).mean():.2f}%)")
        print(f"  IoU>0.1çš„æ•°é‡: {(ious_arr > 0.1).sum()} ({100*(ious_arr > 0.1).mean():.2f}%)")
        
        if ious_arr.max() < 0.5:
            print(f"  âš ï¸  è­¦å‘Š: æœ€å¤§IoU({ious_arr.max():.4f}) < 0.5ï¼Œæ— æ³•è¾¾åˆ°mAP@0.5ï¼")
    else:
        print("  âš ï¸  è­¦å‘Š: æ²¡æœ‰IoUæ•°æ®ï¼ˆå¯èƒ½æ²¡æœ‰æ£€æµ‹æ¡†æˆ–GTæ¡†ï¼‰ï¼")
    
    # æ£€æŸ¥5: ç±»åˆ«åˆ†å¸ƒ
    print("\nã€æ£€æŸ¥5ã€‘ç±»åˆ«é¢„æµ‹åˆ†å¸ƒ")
    if len(stats['class_predictions']) > 0:
        class_pred_arr = np.array(stats['class_predictions'])
        unique, counts = np.unique(class_pred_arr, return_counts=True)
        print(f"  é¢„æµ‹çš„ç±»åˆ«ç´¢å¼•: {unique.tolist()}")
        print(f"  å„ç±»åˆ«æ•°é‡: {counts.tolist()}")
        print(f"  æ€»é¢„æµ‹æ•°: {len(class_pred_arr)}")
    else:
        print("  âš ï¸  è­¦å‘Š: æ²¡æœ‰ç±»åˆ«é¢„æµ‹æ•°æ®ï¼")
    
    if len(stats['gt_classes']) > 0:
        gt_class_arr = np.array(stats['gt_classes'])
        unique, counts = np.unique(gt_class_arr, return_counts=True)
        print(f"  GTç±»åˆ«ç´¢å¼•: {unique.tolist()}")
        print(f"  å„ç±»åˆ«æ•°é‡: {counts.tolist()}")
        print(f"  æ€»GTæ•°: {len(gt_class_arr)}")
    
    # ===== å¯è§†åŒ–æ ·æœ¬ =====
    print("\nã€æ£€æŸ¥6ã€‘å¯è§†åŒ–æ ·æœ¬ï¼ˆå‰5ä¸ªï¼‰")
    visualize_samples(sample_detections, output_dir='outputs/diagnosis_visualizations')
    
    print("\n" + "="*80)
    print("âœ… è¯Šæ–­å®Œæˆï¼")
    print("="*80)
    
    return stats

def visualize_samples(sample_detections, output_dir='outputs/diagnosis_visualizations'):
    """å¯è§†åŒ–æ ·æœ¬"""
    output_dir = Path(output_dir)
    output_dir.mkdir(parents=True, exist_ok=True)
    
    mean = torch.tensor([0.48145466, 0.4578275, 0.40821073])
    std = torch.tensor([0.26862954, 0.26130258, 0.27577711])
    denormalize = transforms.Normalize(
        mean=-mean / std,
        std=1 / std
    )
    
    for sample in sample_detections:
        img_idx = sample['image_idx']
        image = sample['image']
        detections = sample['detections']
        gt_boxes = sample['gt_boxes']
        gt_labels = sample['gt_labels']
        
        # åå½’ä¸€åŒ–å›¾åƒ
        image_denorm = denormalize(image)
        image_denorm = torch.clamp(image_denorm, 0, 1)
        image_np = image_denorm.permute(1, 2, 0).cpu().numpy()
        image_np = (image_np * 255).astype(np.uint8)
        
        fig, ax = plt.subplots(1, 1, figsize=(12, 12))
        ax.imshow(image_np)
        ax.set_title(f'Sample {img_idx}\nPred: {len(detections)} boxes, GT: {len(gt_boxes)} boxes', fontsize=12)
        
        # ç»˜åˆ¶GTæ¡†ï¼ˆçº¢è‰²ï¼‰
        for gt_box, gt_label in zip(gt_boxes, gt_labels):
            if len(gt_box) >= 4:
                x1, y1, x2, y2 = gt_box[:4]
                w = x2 - x1
                h = y2 - y1
                rect = plt.Rectangle((x1, y1), w, h, linewidth=2, edgecolor='red', facecolor='none')
                ax.add_patch(rect)
                label_name = ALL_CLASSES[gt_label] if gt_label < len(ALL_CLASSES) else f"class_{gt_label}"
                ax.text(x1, y1-5, f'GT: {label_name}', color='red', fontsize=8, weight='bold')
        
        # ç»˜åˆ¶é¢„æµ‹æ¡†ï¼ˆè“è‰²ï¼Œå‰10ä¸ªï¼‰
        for i, det in enumerate(detections[:10]):
            box = det['box']
            if isinstance(box, torch.Tensor):
                box = box.cpu().numpy()
            elif isinstance(box, (list, tuple)):
                box = np.array(box)
            else:
                box = np.array([box['xmin'], box['ymin'], box['xmax'], box['ymax']])
            
            x1, y1, x2, y2 = box
            w = x2 - x1
            h = y2 - y1
            conf = det.get('confidence', 0.0)
            class_idx = det.get('class', -1)
            class_name = det.get('class_name', f'class_{class_idx}')
            
            rect = plt.Rectangle((x1, y1), w, h, linewidth=1.5, edgecolor='blue', facecolor='none', linestyle='--')
            ax.add_patch(rect)
            ax.text(x1, y1+h+5, f'Pred: {class_name} ({conf:.3f})', color='blue', fontsize=8)
        
        ax.axis('off')
        plt.tight_layout()
        plt.savefig(output_dir / f'sample_{img_idx}.png', dpi=150, bbox_inches='tight')
        plt.close()
        
        print(f"  ä¿å­˜: {output_dir / f'sample_{img_idx}.png'}")
    
    print(f"  âœ… å¯è§†åŒ–å®Œæˆï¼Œä¿å­˜åœ¨: {output_dir}")

if __name__ == '__main__':
    parser = argparse.ArgumentParser()
    parser.add_argument('--checkpoint', type=str, required=True, help='æ¨¡å‹checkpointè·¯å¾„')
    parser.add_argument('--config', type=str, default='configs/improved_detector_config.yaml', help='é…ç½®æ–‡ä»¶')
    parser.add_argument('--split', type=str, default='val', choices=['train', 'val', 'test'])
    parser.add_argument('--conf_threshold', type=float, default=0.1, help='ç½®ä¿¡åº¦é˜ˆå€¼')
    parser.add_argument('--nms_threshold', type=float, default=0.5, help='NMSé˜ˆå€¼')
    parser.add_argument('--num_samples', type=int, default=10, help='è¯Šæ–­æ ·æœ¬æ•°')
    
    args = parser.parse_args()
    
    # åŠ è½½é…ç½®
    with open(args.config, 'r') as f:
        config = yaml.safe_load(f)
    
    device = torch.device(config.get('device', 'cuda'))
    
    # åˆ›å»ºæ¨¡å‹
    print("åˆ›å»ºæ¨¡å‹...")
    # å¤„ç†checkpointè·¯å¾„
    surgery_checkpoint = config.get('surgery_clip_checkpoint', 'checkpoints/RemoteCLIP-ViT-B-32.pt')
    if not Path(surgery_checkpoint).is_absolute():
        project_root = Path(__file__).parent.parent.parent.parent
        surgery_checkpoint = project_root / surgery_checkpoint
        surgery_checkpoint = str(surgery_checkpoint)
    
    model = create_improved_direct_detection_detector(
        surgery_clip_checkpoint=surgery_checkpoint,
        num_classes=config.get('num_classes', 20),
        cam_resolution=config.get('cam_resolution', 7),
        device=device,
        unfreeze_cam_last_layer=config.get('unfreeze_cam_last_layer', True)
    )
    
    # åŠ è½½checkpoint
    print(f"åŠ è½½checkpoint: {args.checkpoint}")
    checkpoint = torch.load(args.checkpoint, map_location=device)
    
    # å¤„ç†åŠ¨æ€attentionå±‚
    state_dict = checkpoint['model_state_dict']
    model_state_dict = model.state_dict()
    filtered_state_dict = {}
    
    for key, value in state_dict.items():
        if 'attn.in_proj_weight' in key or 'attn.in_proj_bias' in key:
            qkv_key = key.replace('in_proj_weight', 'qkv.weight').replace('in_proj_bias', 'qkv.bias')
            if qkv_key not in state_dict:
                continue
        elif 'attn.qkv.weight' in key or 'attn.qkv.bias' in key:
            if key in model_state_dict:
                filtered_state_dict[key] = value
        elif key in model_state_dict:
            if model_state_dict[key].shape == value.shape:
                filtered_state_dict[key] = value
    
    model.load_state_dict(filtered_state_dict, strict=False)
    print("âœ… æ¨¡å‹åŠ è½½æˆåŠŸ")
    
    # åŠ è½½æ•°æ®
    print(f"\nåŠ è½½{args.split}æ•°æ®é›†...")
    from datasets.dior_detection import get_detection_dataloader
    
    dataloader = get_detection_dataloader(
        root=config.get('dataset_root'),
        split=args.split if args.split != 'val' else 'trainval',  # val images are in trainval folder
        batch_size=4,
        num_workers=2,
        image_size=config.get('image_size', 224),
        augment=False,
        train_only_seen=False  # è¯„ä¼°æ‰€æœ‰ç±»åˆ«
    )
    
    # è¯Šæ–­
    stats = diagnose_model(
        model, dataloader, device,
        conf_threshold=args.conf_threshold,
        nms_threshold=args.nms_threshold,
        num_samples=args.num_samples
    )
    
    print("\nè¯Šæ–­å®Œæˆï¼è¯·æŸ¥çœ‹ä¸Šé¢çš„ç»Ÿè®¡ä¿¡æ¯å’Œå¯è§†åŒ–ç»“æœã€‚")

