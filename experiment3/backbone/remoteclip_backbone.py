#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
RemoteCLIPéª¨å¹²ç½‘ç»œ

åŸºäºOV-ADETRçš„è®¾è®¡ï¼Œä½¿ç”¨RemoteCLIPæ›¿ä»£CLIP

åŠŸèƒ½ï¼š
1. å›¾åƒç‰¹å¾æå–ï¼ˆRemoteCLIPå›¾åƒç¼–ç å™¨ï¼‰
2. æ–‡æœ¬ç‰¹å¾æå–ï¼ˆRemoteCLIPæ–‡æœ¬ç¼–ç å™¨ï¼‰
3. å›¾åƒ-æ–‡æœ¬å¯¹é½
"""

import torch
import torch.nn as nn
import open_clip
from typing import List, Tuple


class RemoteCLIPBackbone(nn.Module):
    """
    RemoteCLIPéª¨å¹²ç½‘ç»œ
    
    ç»“åˆRemoteCLIPçš„å›¾åƒå’Œæ–‡æœ¬ç¼–ç å™¨
    """
    
    def __init__(
        self,
        model_name: str = 'RN50',
        pretrained_path: str = 'checkpoints/RemoteCLIP-RN50.pt',
        freeze_backbone: bool = True,
        output_layers: List[int] = None
    ):
        """
        å‚æ•°:
            model_name: RemoteCLIPæ¨¡å‹åç§°
            pretrained_path: RemoteCLIPæƒé‡è·¯å¾„
            freeze_backbone: æ˜¯å¦å†»ç»“éª¨å¹²ç½‘ç»œ
            output_layers: è¾“å‡ºçš„ç‰¹å¾å±‚
        """
        super().__init__()
        
        self.model_name = model_name
        self.freeze_backbone = freeze_backbone
        
        # åŠ è½½RemoteCLIPæ¨¡å‹
        print(f"ğŸ”„ åŠ è½½RemoteCLIPéª¨å¹²ç½‘ç»œ: {model_name}")
        self.model, _, self.preprocess = open_clip.create_model_and_transforms(model_name)
        
        # åŠ è½½é¢„è®­ç»ƒæƒé‡
        if pretrained_path:
            print(f"ğŸ“¦ åŠ è½½RemoteCLIPæƒé‡: {pretrained_path}")
            ckpt = torch.load(pretrained_path, map_location='cpu')
            self.model.load_state_dict(ckpt)
            print(f"âœ… RemoteCLIPéª¨å¹²ç½‘ç»œåŠ è½½æˆåŠŸ")
        
        # åˆ†è¯å™¨
        self.tokenizer = open_clip.get_tokenizer(model_name)
        
        # å†»ç»“å‚æ•°
        if freeze_backbone:
            for param in self.model.parameters():
                param.requires_grad = False
            self.model.eval()
        
        # è¾“å‡ºå±‚é…ç½®
        if output_layers is None:
            if 'ViT' in model_name:
                self.output_layers = [6, 9, 12]  # ViTçš„å¤šå±‚è¾“å‡º
            else:
                self.output_layers = [2, 3, 4]   # ResNetçš„å¤šå±‚è¾“å‡º
        else:
            self.output_layers = output_layers
        
        # æ³¨å†Œhookæå–ä¸­é—´ç‰¹å¾
        self.features = {}
        self._register_hooks()
    
    def _register_hooks(self):
        """æ³¨å†Œhookä»¥æå–ä¸­é—´å±‚ç‰¹å¾"""
        def get_activation(name):
            def hook(module, input, output):
                self.features[name] = output
            return hook
        
        if 'ViT' in self.model_name:
            # ViTæ¶æ„
            if hasattr(self.model.visual, 'transformer'):
                for idx in self.output_layers:
                    if idx < len(self.model.visual.transformer.resblocks):
                        self.model.visual.transformer.resblocks[idx].register_forward_hook(
                            get_activation(f'layer_{idx}')
                        )
        else:
            # ResNetæ¶æ„
            if hasattr(self.model.visual, 'layer2'):
                self.model.visual.layer2.register_forward_hook(get_activation('layer_2'))
            if hasattr(self.model.visual, 'layer3'):
                self.model.visual.layer3.register_forward_hook(get_activation('layer_3'))
            if hasattr(self.model.visual, 'layer4'):
                self.model.visual.layer4.register_forward_hook(get_activation('layer_4'))
    
    def forward_image(self, images: torch.Tensor) -> List[torch.Tensor]:
        """
        å›¾åƒç‰¹å¾æå–
        
        å‚æ•°:
            images: (B, 3, H, W)
        
        è¿”å›:
            multi_level_feats: å¤šå±‚çº§ç‰¹å¾åˆ—è¡¨
        """
        self.features = {}
        
        # å‰å‘ä¼ æ’­
        with torch.set_grad_enabled(not self.freeze_backbone):
            _ = self.model.encode_image(images)
        
        # æå–å¤šå±‚çº§ç‰¹å¾
        multi_level_feats = []
        for layer_name in sorted(self.features.keys()):
            feat = self.features[layer_name]
            
            # å¤„ç†ä¸åŒæ¶æ„
            if 'ViT' in self.model_name and len(feat.shape) == 3:
                # ViT: (B, N+1, d) -> (B, N, d) -> (B, d, H, W)
                feat = feat[:, 1:, :]  # å»æ‰CLS token
                B, N, d = feat.shape
                H = W = int(N ** 0.5)
                feat = feat.reshape(B, H, W, d).permute(0, 3, 1, 2)
            
            multi_level_feats.append(feat)
        
        return multi_level_feats
    
    def forward_text(self, texts: List[str]) -> torch.Tensor:
        """
        æ–‡æœ¬ç‰¹å¾æå–
        
        å‚æ•°:
            texts: æ–‡æœ¬åˆ—è¡¨
        
        è¿”å›:
            text_features: (B, num_texts, d)
        """
        # åˆ†è¯
        text_tokens = self.tokenizer(texts).to(next(self.model.parameters()).device)
        
        # ç¼–ç 
        with torch.no_grad():
            text_features = self.model.encode_text(text_tokens)
            text_features = text_features / text_features.norm(dim=-1, keepdim=True)
        
        return text_features
    
    def forward(
        self,
        images: torch.Tensor,
        texts: List[str]
    ) -> Tuple[List[torch.Tensor], torch.Tensor]:
        """
        å®Œæ•´å‰å‘ä¼ æ’­
        
        å‚æ•°:
            images: å›¾åƒå¼ é‡
            texts: æ–‡æœ¬åˆ—è¡¨
        
        è¿”å›:
            img_feats: å¤šå±‚çº§å›¾åƒç‰¹å¾
            txt_feats: æ–‡æœ¬ç‰¹å¾
        """
        img_feats = self.forward_image(images)
        txt_feats = self.forward_text(texts)
        
        return img_feats, txt_feats


if __name__ == "__main__":
    print("=" * 70)
    print("æµ‹è¯•RemoteCLIPéª¨å¹²ç½‘ç»œ")
    print("=" * 70)
    
    # åˆ›å»ºéª¨å¹²ç½‘ç»œ
    backbone = RemoteCLIPBackbone(
        model_name='RN50',
        pretrained_path='checkpoints/RemoteCLIP-RN50.pt'
    )
    backbone = backbone.cuda().eval()
    
    # æµ‹è¯•æ•°æ®
    batch_size = 2
    images = torch.randn(batch_size, 3, 800, 800).cuda()
    texts = ["airplane", "ship", "harbor", "bridge"]
    
    # å‰å‘ä¼ æ’­
    with torch.no_grad():
        img_feats, txt_feats = backbone(images, texts)
    
    print(f"\nå›¾åƒç‰¹å¾:")
    for i, feat in enumerate(img_feats):
        print(f"  å±‚{i}: {feat.shape}")
    
    print(f"\næ–‡æœ¬ç‰¹å¾: {txt_feats.shape}")
    
    print("\nâœ… RemoteCLIPéª¨å¹²ç½‘ç»œæµ‹è¯•å®Œæˆï¼")

