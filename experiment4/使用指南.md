# å®éªŒ4 ä½¿ç”¨æŒ‡å—

## ç›®å½•
1. [å¿«é€Ÿå¼€å§‹](#å¿«é€Ÿå¼€å§‹)
2. [è¯¦ç»†ä½¿ç”¨](#è¯¦ç»†ä½¿ç”¨)
3. [å¸¸è§é—®é¢˜](#å¸¸è§é—®é¢˜)
4. [é«˜çº§ç”¨æ³•](#é«˜çº§ç”¨æ³•)

---

## å¿«é€Ÿå¼€å§‹

### æ–¹æ³•1ï¼šä½¿ç”¨å¿«é€Ÿå¯åŠ¨è„šæœ¬ï¼ˆæ¨èï¼‰

```bash
cd /home/ubuntu22/Projects/RemoteCLIP-main
./experiment4/quick_start.sh
```

ç„¶åæ ¹æ®èœå•é€‰æ‹©æ“ä½œï¼š
- `1` - è®­ç»ƒSeenç±»
- `2` - è¯„ä¼°Seenç±»  
- `3` - Zero-shotè¯„ä¼°Unseenç±»
- `4` - è¿è¡ŒDemo
- `5` - æµ‹è¯•æ‰€æœ‰æ¨¡å—

### æ–¹æ³•2ï¼šç›´æ¥è¿è¡ŒPythonè„šæœ¬

```bash
# 1. è®­ç»ƒ
python -m experiment4.train_seen

# 2. è¯„ä¼°Seenç±»
python -m experiment4.inference_seen

# 3. Zero-shotè¯„ä¼°Unseenç±»
python -m experiment4.inference_unseen

# 4. Demo
python experiment4/demo.py assets/airport.jpg
```

---

## è¯¦ç»†ä½¿ç”¨

### 1. è®­ç»ƒSeenç±»

#### åŸºç¡€è®­ç»ƒ

```bash
python -m experiment4.train_seen
```

**é¢„æœŸè¾“å‡º**ï¼š
```
==================================================
å®éªŒ4é…ç½®
==================================================
ä¸»å¹²ç½‘ç»œ: ViT-B/16
åµŒå…¥ç»´åº¦: 512
åŸå­æ¨¡å¼æ•°: 20
ç¨€ç–åº¦: 0.1
Batchå¤§å°: 32
å­¦ä¹ ç‡: 0.0001
è®­ç»ƒè½®æ•°: 50
Seenç±»æ•°: 15
Unseenç±»æ•°: 5
è®¾å¤‡: cuda
==================================================

åˆå§‹åŒ–æ¨¡å‹...
  âœ“ CLIP SurgeryåŠ è½½å®Œæˆ
  âœ“ èƒŒæ™¯è¯è¡¨: 15 ä¸ªè¯
  âœ“ è§„åˆ™å»å™ªå™¨åˆå§‹åŒ–å®Œæˆ
  âœ“ æ–‡æœ¬å¼•å¯¼åˆ†è§£å™¨: XXX å‚æ•°
  âœ“ å›¾åƒåˆ†è§£å™¨: XXX å‚æ•°

åŠ è½½æ•°æ®...
  âœ“ è®­ç»ƒé›†: XX æ ·æœ¬
  âœ“ éªŒè¯é›†: XX æ ·æœ¬
  âœ“ Seenç±»: 15
  âœ“ Unseenç±»: 5

å¼€å§‹è®­ç»ƒ...
Epoch 1/50: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| loss: 0.5234, acc: 0.7850
...
```

#### ä¿®æ”¹è®­ç»ƒå‚æ•°

ç¼–è¾‘`experiment4/config.py`ï¼š

```python
class Config:
    # ä¿®æ”¹è®­ç»ƒè½®æ•°
    epochs = 100
    
    # ä¿®æ”¹batchå¤§å°
    batch_size = 16  # å¦‚æœGPUå†…å­˜ä¸è¶³
    
    # ä¿®æ”¹å­¦ä¹ ç‡
    learning_rate = 5e-5  # æ›´ä¿å®ˆçš„å­¦ä¹ ç‡
    
    # ä¿®æ”¹æŸå¤±æƒé‡
    w_sparse = 0.2  # æ›´å¼ºçš„ç¨€ç–æ€§
```

ç„¶åé‡æ–°è®­ç»ƒï¼š
```bash
python -m experiment4.train_seen
```

#### æ¢å¤è®­ç»ƒ

```python
# åœ¨train_seen.pyä¸­æ·»åŠ 
checkpoint = torch.load('experiment4/checkpoints/latest.pth')
text_decomposer.load_state_dict(checkpoint['text_decomposer'])
img_decomposer.load_state_dict(checkpoint['img_decomposer'])
optimizer.load_state_dict(checkpoint['optimizer'])
start_epoch = checkpoint['epoch'] + 1
```

---

### 2. è¯„ä¼°Seenç±»

```bash
python -m experiment4.inference_seen
```

**è¾“å‡ºç¤ºä¾‹**ï¼š
```
è¯„ä¼°Seenç±»æ€§èƒ½
==================================================
æ¨ç†ä¸­: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:05<00:00]

æ•´ä½“å‡†ç¡®ç‡: 0.8500

å„ç±»åˆ«å‡†ç¡®ç‡:
  airplane: 0.9200
  airport: 0.8800
  baseballfield: 0.8500
  ...

ç»“æœä¿å­˜è‡³: experiment4/outputs/seen_inference_results.json
```

**ç»“æœæ–‡ä»¶**ï¼š
```json
{
    "overall_accuracy": 0.85,
    "class_accuracy": {
        "airplane": 0.92,
        "airport": 0.88,
        ...
    },
    "num_samples": 100,
    "num_classes": 15
}
```

---

### 3. Zero-shotè¯„ä¼°Unseenç±»

```bash
python -m experiment4.inference_unseen
```

**å…³é”®**ï¼š
- ä½¿ç”¨**image-onlyåˆ†è§£å™¨**
- ä¸ä¾èµ–seenç±»çš„æ–‡æœ¬ä¿¡æ¯
- çœŸæ­£çš„zero-shotæ¨ç†

**è¾“å‡ºç¤ºä¾‹**ï¼š
```
è¯„ä¼°Unseenç±»æ€§èƒ½ - Zero-shot
==================================================
æ¨ç†ä¸­: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:02<00:00]

Zero-shotå‡†ç¡®ç‡: 0.6800
å¹³å‡ç½®ä¿¡åº¦: 0.7200

å„ç±»åˆ«å‡†ç¡®ç‡:
  bridge: 0.7500 (10 æ ·æœ¬)
  harbor: 0.6800 (8 æ ·æœ¬)
  parkinglot: 0.6200 (12 æ ·æœ¬)
  ...

ç”Ÿæˆå¯è§†åŒ–...
å¯è§†åŒ–ä¿å­˜è‡³: experiment4/outputs/visualizations/unseen_sample_0.png
...
```

**å¯è§†åŒ–ç»“æœ**ï¼š
- å·¦ï¼šåŸå›¾
- å³ï¼šAttentionçƒ­åŠ›å›¾
- æ ‡é¢˜ï¼šé¢„æµ‹ç±»åˆ« + ç½®ä¿¡åº¦

---

### 4. è¿è¡ŒDemo

#### å•å¼ å›¾åƒæ¨ç†

```bash
# ä½¿ç”¨ç¤ºä¾‹å›¾åƒ
python experiment4/demo.py assets/airport.jpg

# ä½¿ç”¨è‡ªå·±çš„å›¾åƒ
python experiment4/demo.py /path/to/your/image.jpg
```

#### Demoè¾“å‡º

```
==================================================
å®éªŒ4 Demo - Surgery + æ–‡æœ¬å¼•å¯¼ç¨€ç–åˆ†è§£ + è§„åˆ™å»å™ª
==================================================

è®¾å¤‡: cuda

åŠ è½½æ¨¡å‹...
  âœ“ CLIP Surgery
  âœ“ èƒŒæ™¯è¯è¡¨
  âœ“ è§„åˆ™å»å™ªå™¨
  âœ“ å›¾åƒåˆ†è§£å™¨
  âœ“ åŠ è½½æƒé‡: experiment4/checkpoints/best.pth

å‡†å¤‡ç±»åˆ«...
  âœ“ 20 ä¸ªç±»åˆ«

==================================================
æ¨ç†å›¾åƒ: assets/airport.jpg
==================================================

é¢„æµ‹ç»“æœ:
  Top-1: airport (95.23%)

  Top-5:
    1. airport: 95.23%
    2. airplane: 3.12%
    3. freeway: 0.85%
    4. parkinglot: 0.45%
    5. intersection: 0.22%

  ç¨€ç–åº¦: 10.12%
  å‰æ™¯æ¯”ä¾‹: 67.34%
  å™ªå£°é™ä½: 15.67%

å¯è§†åŒ–ä¿å­˜è‡³: assets/airport_result.png
```

#### æ‰¹é‡æ¨ç†

```python
from experiment4.demo import Experiment4Demo

# åˆ›å»ºdemo
demo = Experiment4Demo("experiment4/checkpoints/best.pth")

# æ‰¹é‡æ¨ç†
images = ["img1.jpg", "img2.jpg", "img3.jpg"]
for img_path in images:
    result = demo.predict(img_path)
    print(f"{img_path}: {result['top1_class']} ({result['top1_prob']:.2%})")
```

---

## å¸¸è§é—®é¢˜

### Q1: è®­ç»ƒæ—¶GPUå†…å­˜ä¸è¶³

**é”™è¯¯**ï¼š
```
RuntimeError: CUDA out of memory
```

**è§£å†³**ï¼š
1. å‡å°batchå¤§å°
```python
# config.py
batch_size = 16  # æˆ–æ›´å°
```

2. å‡å°‘workeræ•°é‡
```python
num_workers = 0
```

3. ä½¿ç”¨æ··åˆç²¾åº¦è®­ç»ƒ
```python
# train_seen.pyä¸­æ·»åŠ 
from torch.cuda.amp import autocast, GradScaler
scaler = GradScaler()

with autocast():
    loss = compute_loss(...)
scaler.scale(loss).backward()
scaler.step(optimizer)
scaler.update()
```

---

### Q2: æ•°æ®é›†è·¯å¾„é”™è¯¯

**é”™è¯¯**ï¼š
```
FileNotFoundError: datasets/mini_dataset
```

**è§£å†³**ï¼š
1. æ£€æŸ¥æ•°æ®é›†æ˜¯å¦å­˜åœ¨
```bash
ls datasets/mini_dataset/
```

2. ä¿®æ”¹é…ç½®
```python
# config.py
dataset_root = "/path/to/your/dataset"
```

3. ä½¿ç”¨ç»å¯¹è·¯å¾„
```python
dataset_root = "/home/ubuntu22/Projects/RemoteCLIP-main/datasets/mini_dataset"
```

---

### Q3: CLIPæƒé‡åŠ è½½å¤±è´¥

**é”™è¯¯**ï¼š
```
Error loading CLIP weights
```

**è§£å†³**ï¼š
1. æ£€æŸ¥checkpointsç›®å½•
```bash
ls checkpoints/
```

2. ä¸‹è½½CLIPæƒé‡
```bash
cd checkpoints
wget https://openaipublic.azureedge.net/clip/models/...
```

3. ä½¿ç”¨æ ‡å‡†CLIP
```python
# models/clip_surgery.py
# ä¼šè‡ªåŠ¨ä¸‹è½½æ ‡å‡†CLIPæƒé‡
```

---

### Q4: Zero-shotå‡†ç¡®ç‡å¤ªä½

**å¯èƒ½åŸå› **ï¼š
1. è®­ç»ƒä¸å……åˆ†
2. seen/unseenç±»åˆ«å·®å¼‚å¤ªå¤§
3. å¯¹é½æŸå¤±æƒé‡å¤ªå°

**è§£å†³**ï¼š
1. å¢åŠ è®­ç»ƒè½®æ•°
```python
epochs = 100
```

2. å¢åŠ å¯¹é½æŸå¤±æƒé‡
```python
w_align = 0.5  # é»˜è®¤0.3
```

3. ä½¿ç”¨æ›´å¤šWordNetè¯
```python
wordnet_k = 30  # é»˜è®¤20
```

4. æ£€æŸ¥æ˜¯å¦ä½¿ç”¨äº†image-onlyåˆ†è§£å™¨
```python
# inference_unseen.py
# ç¡®è®¤ä½¿ç”¨çš„æ˜¯img_decomposerè€Œétext_decomposer
```

---

### Q5: è®­ç»ƒé€Ÿåº¦å¤ªæ…¢

**ä¼˜åŒ–æ–¹æ³•**ï¼š

1. å¢åŠ num_workers
```python
num_workers = 4  # CPUæ ¸å¿ƒæ•°
```

2. ä½¿ç”¨pin_memory
```python
pin_memory = True
```

3. é¢„è®¡ç®—WordNetç‰¹å¾ï¼ˆå·²å®ç°ï¼‰
```python
# train_seen.pyå·²ç»ç¼“å­˜äº†WordNetç‰¹å¾
self.wordnet_cache = {...}
```

4. ä½¿ç”¨æ›´å°çš„æ¨¡å‹
```python
backbone = "ViT-B/32"  # æ¯”ViT-B/16æ›´å¿«
```

---

## é«˜çº§ç”¨æ³•

### 1. è‡ªå®šä¹‰ç±»åˆ«

**æ·»åŠ æ–°ç±»åˆ«**ï¼š

1. åœ¨`data/wordnet_utils.py`æ·»åŠ è¯è¡¨ï¼š
```python
WORDNET_DICT['new_class'] = [
    'new_class', 'synonym1', 'synonym2', ...,
    'related_word1', 'related_word2', ...
]
```

2. åœ¨æ•°æ®é›†ä¸­æ·»åŠ æ ·æœ¬

3. é‡æ–°è®­ç»ƒ

### 2. è°ƒæ•´ç¨€ç–åº¦

**æ›´ç¨€ç–**ï¼ˆæ›´å¿«ï¼Œå¯èƒ½é™ä½å‡†ç¡®ç‡ï¼‰ï¼š
```python
sparsity_ratio = 0.05  # åªä¿ç•™5%
```

**æ›´å¯†é›†**ï¼ˆæ›´æ…¢ï¼Œå¯èƒ½æé«˜å‡†ç¡®ç‡ï¼‰ï¼š
```python
sparsity_ratio = 0.2  # ä¿ç•™20%
```

### 3. ä¿®æ”¹åŸå­æ¨¡å¼æ•°é‡

```python
n_components = 10  # æ›´å°‘çš„æ¨¡å¼ï¼ˆæ›´å¿«ï¼‰
n_components = 40  # æ›´å¤šçš„æ¨¡å¼ï¼ˆæ›´ç»†ç²’åº¦ï¼‰
```

**æ³¨æ„**ï¼šä¿®æ”¹åéœ€è¦é‡æ–°è®­ç»ƒã€‚

### 4. ä½¿ç”¨ä¸åŒçš„CLIPæ¨¡å‹

```python
# config.py
backbone = "ViT-L/14"  # æ›´å¤§çš„æ¨¡å‹
embed_dim = 768        # å¯¹åº”çš„ç»´åº¦
```

### 5. é›†æˆtextå’Œimgåˆ†è§£å™¨

åœ¨`inference_unseen.py`ä¸­ï¼š
```python
# åŒæ—¶ä½¿ç”¨ä¸¤ä¸ªåˆ†è§£å™¨
Q_text = text_decomposer(F_clean, text_features)
Q_img = img_decomposer(F_clean)

# åŠ æƒé›†æˆ
alpha = 0.7
Q = alpha * Q_text + (1 - alpha) * Q_img
```

**æ³¨æ„**ï¼šè¿™ä¼šç ´åzero-shotæ€§è´¨ï¼ˆå› ä¸ºç”¨äº†seenç±»çš„textï¼‰ã€‚

---

## æ€§èƒ½åŸºå‡†

### ç¡¬ä»¶è¦æ±‚

| ç»„ä»¶ | æœ€ä½é…ç½® | æ¨èé…ç½® |
|------|----------|----------|
| GPU | GTX 1080 (8GB) | RTX 3090 (24GB) |
| CPU | 4æ ¸ | 8æ ¸+ |
| å†…å­˜ | 16GB | 32GB+ |
| å­˜å‚¨ | 10GB | 50GB+ |

### è®­ç»ƒæ—¶é—´ï¼ˆ50 epochsï¼‰

| ç¡¬ä»¶ | æ—¶é—´ |
|------|------|
| RTX 3090 | ~1å°æ—¶ |
| RTX 2080 Ti | ~1.5å°æ—¶ |
| V100 | ~45åˆ†é’Ÿ |
| GTX 1080 Ti | ~2å°æ—¶ |

### æ¨ç†é€Ÿåº¦

| ç¡¬ä»¶ | é€Ÿåº¦ |
|------|------|
| RTX 3090 | ~80 img/s |
| RTX 2080 Ti | ~50 img/s |
| GTX 1080 Ti | ~30 img/s |
| CPU | ~2 img/s |

---

## æ£€æŸ¥ç‚¹ç®¡ç†

### æ£€æŸ¥ç‚¹ä½ç½®

```
experiment4/checkpoints/
â”œâ”€â”€ best.pth          # æœ€ä½³æ¨¡å‹ï¼ˆéªŒè¯å‡†ç¡®ç‡æœ€é«˜ï¼‰
â”œâ”€â”€ latest.pth        # æœ€æ–°æ¨¡å‹
â”œâ”€â”€ epoch_5.pth       # ç¬¬5ä¸ªepoch
â”œâ”€â”€ epoch_10.pth      # ç¬¬10ä¸ªepoch
â””â”€â”€ ...
```

### åŠ è½½æ£€æŸ¥ç‚¹

```python
checkpoint = torch.load('experiment4/checkpoints/best.pth')

# æŸ¥çœ‹ä¿¡æ¯
print(f"Epoch: {checkpoint['epoch']}")
print(f"History: {checkpoint['history']}")

# åŠ è½½æƒé‡
text_decomposer.load_state_dict(checkpoint['text_decomposer'])
img_decomposer.load_state_dict(checkpoint['img_decomposer'])
```

### å¯¼å‡ºæ¨¡å‹

```python
# åªä¿å­˜æ¨¡å‹æƒé‡ï¼ˆæ›´å°ï¼‰
torch.save({
    'text_decomposer': text_decomposer.state_dict(),
    'img_decomposer': img_decomposer.state_dict()
}, 'experiment4/export/model_weights.pth')
```

---

## æ€»ç»“

å®éªŒ4æä¾›äº†ä¸€å¥—å®Œæ•´çš„é¥æ„Ÿå›¾åƒåˆ†ç±»å’Œzero-shotæ¨ç†æ–¹æ¡ˆï¼š

âœ… **æ˜“ç”¨**ï¼šå¿«é€Ÿå¯åŠ¨è„šæœ¬ï¼Œä¸€é”®è¿è¡Œ  
âœ… **çµæ´»**ï¼šä¸°å¯Œçš„é…ç½®é€‰é¡¹  
âœ… **é«˜æ•ˆ**ï¼šGPUåŠ é€Ÿï¼Œå¿«é€Ÿæ¨ç†  
âœ… **å¯é **ï¼šå®Œå–„çš„é”™è¯¯å¤„ç†  
âœ… **å¯æ‰©å±•**ï¼šæ¨¡å—åŒ–è®¾è®¡ï¼Œæ˜“äºå®šåˆ¶  

**ä¸‹ä¸€æ­¥**ï¼š
- åœ¨è‡ªå·±çš„æ•°æ®é›†ä¸Šè®­ç»ƒ
- è°ƒæ•´è¶…å‚æ•°ä¼˜åŒ–æ€§èƒ½
- æ¢ç´¢æ›´å¤šåº”ç”¨åœºæ™¯

**ç¥å®éªŒé¡ºåˆ©ï¼** ğŸš€

