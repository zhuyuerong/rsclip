# å®éªŒ4æ•°æ®ç»“æ„å…¼å®¹æ€§æ£€æŸ¥æŠ¥å‘Š

## ğŸ“Š æ£€æŸ¥ç»“æœæ€»ç»“

### âœ… ç¬¦åˆçš„éƒ¨åˆ†

1. **æ¨¡å‹åº•å±‚è¾“å‡ºæ ¼å¼æ­£ç¡®**
   - `CLIPSurgery.encode_image()` è¿”å› `[B, 50, 512]`ï¼ˆåŒ…å«CLS token + 49ä¸ªpatchesï¼‰
   - å¯ä»¥åˆ†åˆ«æå–ï¼š
     - CLS token: `features[:, 0, :]` â†’ `[B, 512]`
     - Patch tokens: `features[:, 1:, :]` â†’ `[B, 49, 512]`

2. **æ•°æ®ç»´åº¦åŒ¹é…**
   - ViT-B-32: 7Ã—7 = 49 patches
   - åŠ ä¸ŠCLS token = 50 tokens
   - è¾“å‡ºç»´åº¦: 512ï¼ˆä¸æ–‡æœ¬ç‰¹å¾åŒ¹é…ï¼‰

### âš ï¸ éœ€è¦ä¿®æ”¹çš„éƒ¨åˆ†

1. **CLIPSurgeryWrapperæ¥å£ä¸å®Œæ•´**
   ```python
   # å½“å‰å®ç°
   CLIPSurgeryWrapper.get_patch_features() â†’ [B, 49, 512]  # åªæœ‰patches
   
   # VVæœºåˆ¶æœŸæœ›
   model(images) â†’ [B, 50, 512]  # CLS + patches
   cls_features = features[:, 0, :]  # [B, 512]
   patch_features = features[:, 1:, :]  # [B, 49, 512]
   ```

2. **ç¼ºå°‘è·å–å®Œæ•´ç‰¹å¾çš„æ–¹æ³•**
   - æ²¡æœ‰æ–¹æ³•ç›´æ¥è¿”å›åŒ…å«CLS tokençš„å®Œæ•´ç‰¹å¾
   - æ— æ³•æŒ‰VVæœºåˆ¶çš„æ ¼å¼ä½¿ç”¨

3. **VVæœºåˆ¶æœªå®ç°**
   - å½“å‰ä½¿ç”¨æ ‡å‡†`MultiheadAttention`
   - æœªæ›¿æ¢ä¸º`VVAttention`æœºåˆ¶

## ğŸ”§ ä¿®æ”¹å»ºè®®

### æ–¹æ¡ˆ1ï¼šæ·»åŠ å®Œæ•´ç‰¹å¾è·å–æ–¹æ³•ï¼ˆæ¨èï¼‰

åœ¨`CLIPSurgeryWrapper`ä¸­æ·»åŠ ï¼š

```python
def get_all_features(self, images):
    """
    è·å–å®Œæ•´ç‰¹å¾ï¼ˆåŒ…å«CLS token + patchesï¼‰
    
    Args:
        images: [B, 3, 224, 224]
    
    Returns:
        all_features: [B, N+1, 512]  # CLS + N patches
    """
    return self.model.encode_image(images)  # [B, 50, 512]

def get_cls_features(self, images):
    """
    è·å–CLS tokenç‰¹å¾ï¼ˆå…¨å±€ç‰¹å¾ï¼‰
    
    Args:
        images: [B, 3, 224, 224]
    
    Returns:
        cls_features: [B, 512]
    """
    all_features = self.model.encode_image(images)
    return all_features[:, 0, :]  # [B, 512]

# ä¿æŒç°æœ‰æ–¹æ³•ä»¥å…¼å®¹æ—§ä»£ç 
def get_patch_features(self, images):
    """
    è·å–patchç‰¹å¾ï¼ˆå»æ‰CLS tokenï¼‰
    
    Args:
        images: [B, 3, 224, 224]
    
    Returns:
        patch_features: [B, N, 512]
    """
    all_features = self.model.encode_image(images)
    return all_features[:, 1:, :]  # [B, 49, 512]
```

### æ–¹æ¡ˆ2ï¼šä¿®æ”¹ç°æœ‰æ–¹æ³•è¿”å›å®Œæ•´ç‰¹å¾

å°†`get_patch_features()`æ”¹ä¸ºè¿”å›å®Œæ•´ç‰¹å¾ï¼Œå¹¶æ·»åŠ å‚æ•°æ§åˆ¶ï¼š

```python
def get_patch_features(self, images, include_cls=False):
    """
    è·å–ç‰¹å¾
    
    Args:
        images: [B, 3, 224, 224]
        include_cls: æ˜¯å¦åŒ…å«CLS token
    
    Returns:
        å¦‚æœinclude_cls=True: [B, N+1, 512]  # CLS + patches
        å¦‚æœinclude_cls=False: [B, N, 512]  # åªæœ‰patches
    """
    all_features = self.model.encode_image(images)
    if include_cls:
        return all_features  # [B, 50, 512]
    else:
        return all_features[:, 1:, :]  # [B, 49, 512]
```

âš ï¸ **æ³¨æ„**ï¼šè¿™ä¸ªæ–¹æ¡ˆä¼šç ´åå‘åå…¼å®¹æ€§ï¼Œéœ€è¦ä¿®æ”¹æ‰€æœ‰è°ƒç”¨å¤„ã€‚

## ğŸš€ VVæœºåˆ¶å®ç°æ£€æŸ¥

### å½“å‰çŠ¶æ€

```python
# æ£€æŸ¥ç»“æœ
Transformerå±‚æ•°: 12
æœ€å3å±‚çš„æ³¨æ„åŠ›ç±»å‹:
  ç¬¬12å±‚: MultiheadAttention  # æ ‡å‡†æ³¨æ„åŠ›
  ç¬¬11å±‚: MultiheadAttention
  ç¬¬10å±‚: MultiheadAttention
```

### å¦‚æœè¦ä½¿ç”¨VVæœºåˆ¶

éœ€è¦æ›¿æ¢æœ€å`num_vv_blocks`å±‚çš„æ³¨æ„åŠ›æœºåˆ¶ï¼š

```python
# ä¼ªä»£ç 
for i in range(1, num_vv_blocks + 1):
    block = transformer.resblocks[-i]
    # åˆ›å»ºVVæ³¨æ„åŠ›
    vv_attn = VVAttention(...)
    # å¤åˆ¶æƒé‡
    vv_attn.qkv.weight.data = original_attn.in_proj_weight.clone()
    # æ›¿æ¢
    block.attn = vv_attn
```

**æ³¨æ„**ï¼šå½“å‰å®éªŒ4ä½¿ç”¨CLIP Surgeryçš„V-V attentionï¼ˆé¿å…æ–‡æœ¬æ³„éœ²ï¼‰ï¼Œä½†è¿™ä¸æ˜¯VVæœºåˆ¶ï¼ˆAttention(V,V,V)ï¼‰ã€‚ä¸¤è€…æ˜¯ä¸åŒçš„æ¦‚å¿µã€‚

## ğŸ“ å½“å‰å®éªŒ4çš„æ•°æ®æµ

```python
# train_seen.py ç¬¬175è¡Œ
F_img = self.surgery_model.get_patch_features(images)  # [B, 49, 512]
# â†’ åªæœ‰patchesï¼Œæ²¡æœ‰CLS token
# â†’ ç¬¦åˆå½“å‰è®¾è®¡ï¼ˆç”¨äºå¯†é›†é¢„æµ‹ï¼‰
```

å½“å‰è®¾è®¡ç”¨äº**å¯†é›†é¢„æµ‹ä»»åŠ¡**ï¼ˆæ£€æµ‹ã€åˆ†å‰²ï¼‰ï¼Œåªéœ€è¦patch tokensï¼Œä¸éœ€è¦CLS tokenã€‚

## âœ… ç»“è®º

### å¯¹äºVVæœºåˆ¶çš„å…¼å®¹æ€§

**éƒ¨åˆ†å…¼å®¹**ï¼š
- âœ… æ¨¡å‹åº•å±‚æ”¯æŒæå–å®Œæ•´ç‰¹å¾ï¼ˆCLS + patchesï¼‰
- âš ï¸ æ¥å£å±‚é¢ç¼ºå°‘è·å–å®Œæ•´ç‰¹å¾çš„æ–¹æ³•
- âŒ æœªå®ç°VVæœºåˆ¶ï¼ˆAttention(V,V,V)ï¼‰

### å»ºè®®è¡ŒåŠ¨

1. **çŸ­æœŸ**ï¼ˆä¿æŒå‘åå…¼å®¹ï¼‰ï¼š
   - æ·»åŠ `get_all_features()`æ–¹æ³•è¿”å›å®Œæ•´ç‰¹å¾
   - æ·»åŠ `get_cls_features()`æ–¹æ³•è·å–CLS token

2. **ä¸­æœŸ**ï¼ˆå¦‚æœéœ€è¦VVæœºåˆ¶ï¼‰ï¼š
   - å®ç°`VVAttention`æœºåˆ¶
   - åœ¨æ¨¡å‹åŠ è½½æ—¶æ›¿æ¢æœ€åå‡ å±‚çš„æ³¨æ„åŠ›

3. **é•¿æœŸ**ï¼ˆé‡æ„ï¼‰ï¼š
   - ç»Ÿä¸€æ¥å£è®¾è®¡ï¼Œæ”¯æŒVVæœºåˆ¶å’Œæ ‡å‡†æœºåˆ¶
   - æä¾›é…ç½®é€‰é¡¹é€‰æ‹©ä½¿ç”¨å“ªç§æœºåˆ¶

## ğŸ“Œ å…³é”®å·®å¼‚è¯´æ˜

| é¡¹ç›® | VVæœºåˆ¶æœŸæœ› | å®éªŒ4å½“å‰ | CLIP Surgery |
|------|------------|-----------|--------------|
| è¾“å‡ºæ ¼å¼ | `[B, N+1, D]` | `[B, N, D]` (patch only) | `[B, N+1, D]` (åº•å±‚) |
| æ³¨æ„åŠ›æœºåˆ¶ | `Attention(V,V,V)` | æ ‡å‡†`MultiheadAttention` | æ ‡å‡†`MultiheadAttention` |
| ç”¨é€” | åˆ†ç±» + å¯†é›†é¢„æµ‹ | å¯†é›†é¢„æµ‹ | é¿å…æ–‡æœ¬æ³„éœ² |
| CLS tokenå¯ç”¨æ€§ | âœ… æ˜¯ | âš ï¸ éœ€è¦æ·»åŠ æ–¹æ³• | âœ… åº•å±‚æ”¯æŒ |

