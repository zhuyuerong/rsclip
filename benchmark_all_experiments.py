#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ä¸‰ä¸ªå®éªŒçš„æ€§èƒ½åŸºå‡†æµ‹è¯•

åœ¨ mini_dataset ä¸Šæµ‹è¯•æ¨¡å‹æ¶æ„å’Œæ¨ç†æ€§èƒ½
"""

import torch
import torch.nn as nn
import time
import json
from pathlib import Path
import sys
from collections import defaultdict

print("=" * 70)
print("ä¸‰ä¸ªå®éªŒæ€§èƒ½åŸºå‡†æµ‹è¯•")
print("=" * 70)


def count_parameters(model):
    """ç»Ÿè®¡æ¨¡å‹å‚æ•°"""
    total = sum(p.numel() for p in model.parameters())
    trainable = sum(p.numel() for p in model.parameters() if p.requires_grad)
    return {
        'total': total,
        'trainable': trainable,
        'frozen': total - trainable,
        'total_M': total / 1e6,
        'trainable_M': trainable / 1e6
    }


def test_experiment1():
    """æµ‹è¯• Experiment1"""
    
    print("\n" + "â–¶" * 35)
    print("Experiment1: ä¸¤é˜¶æ®µæ£€æµ‹")
    print("â–¶" * 35)
    
    sys.path.insert(0, 'experiment1')
    from inference.model_loader import ModelLoader
    
    # åŠ è½½æ¨¡å‹
    print("\nåŠ è½½ RemoteCLIP...")
    loader = ModelLoader(model_name='RN50', device='cpu')
    model, preprocess, tokenizer = loader.load_model()
    
    # å‚æ•°ç»Ÿè®¡
    params = count_parameters(model)
    
    print(f"\nğŸ“Š æ¨¡å‹å‚æ•°:")
    print(f"  æ€»å‚æ•°: {params['total']:,} ({params['total_M']:.2f}M)")
    print(f"  RemoteCLIP (å…¨éƒ¨ä¸ºå¯ç”¨): {params['total_M']:.2f}M")
    
    print(f"\nğŸ“‹ æ¨¡å‹æ¶æ„:")
    print(f"  ç±»å‹: ä¸¤é˜¶æ®µæ£€æµ‹")
    print(f"  Stage1: æè®®ç”Ÿæˆ + åˆ†ç±»")
    print(f"  Stage2: ç›®æ ‡æ£€æµ‹ + è¾¹ç•Œæ¡†ç»†åŒ–")
    print(f"  ç‰¹å¾æå–: RemoteCLIP RN50")
    print(f"  æ–¹æ³•: åŸºäºåŒºåŸŸçš„æ£€ç´¢å’Œå¯¹æ¯”")
    
    # æµ‹è¯•æ¨ç†é€Ÿåº¦
    print(f"\nğŸ”¬ æµ‹è¯•æ¨ç†é€Ÿåº¦...")
    
    from PIL import Image
    test_image = Image.new('RGB', (800, 800), color='red')
    test_texts = ['airplane', 'ship', 'harbor']
    
    # é¢„çƒ­
    _ = loader.encode_image(test_image)
    _ = loader.encode_text_batch(test_texts)
    
    # æµ‹è¯•
    num_runs = 10
    start_time = time.time()
    
    for _ in range(num_runs):
        img_feat = loader.encode_image(test_image)
        txt_feat = loader.encode_text_batch(test_texts)
        similarity = (img_feat @ txt_feat.T)
    
    elapsed = time.time() - start_time
    
    print(f"  æµ‹è¯•æ¬¡æ•°: {num_runs}")
    print(f"  æ€»ç”¨æ—¶: {elapsed:.3f}ç§’")
    print(f"  å¹³å‡ç”¨æ—¶: {elapsed/num_runs*1000:.1f}ms/å›¾")
    print(f"  FPS: {num_runs/elapsed:.2f}")
    
    return {
        'experiment': 'Experiment1',
        'name': 'ä¸¤é˜¶æ®µæ£€æµ‹',
        'architecture': {
            'type': 'Two-Stage Detection',
            'backbone': 'RemoteCLIP RN50',
            'method': 'Region-based Retrieval'
        },
        'parameters': params,
        'performance': {
            'avg_time_ms': (elapsed/num_runs) * 1000,
            'fps': num_runs / elapsed
        },
        'features': {
            'stage1': 'Proposal Generation + Classification',
            'stage2': 'Target Detection + BBox Refinement',
            'wordnet': 'Vocabulary Enhancement'
        }
    }


def test_experiment2():
    """æµ‹è¯• Experiment2"""
    
    print("\n" + "â–¶" * 35)
    print("Experiment2: ä¸Šä¸‹æ–‡å¼•å¯¼æ£€æµ‹")
    print("â–¶" * 35)
    
    sys.path.insert(0, 'experiment2')
    
    from config.default_config import DefaultConfig
    from stage1_encoder.clip_text_encoder import CLIPTextEncoder
    
    config = DefaultConfig()
    
    print(f"\nğŸ“‹ æ¨¡å‹é…ç½®:")
    print(f"  æŸ¥è¯¢æ•°é‡: {config.num_queries}")
    print(f"  è§£ç å™¨å±‚æ•°: {config.num_decoder_layers}")
    print(f"  æ¨¡å‹ç»´åº¦: {config.d_model}")
    print(f"  CLIPç»´åº¦: {config.d_clip}")
    print(f"  ä¸Šä¸‹æ–‡é—¨æ§: {config.context_gating_type}")
    
    # åŠ è½½æ–‡æœ¬ç¼–ç å™¨
    print(f"\nåŠ è½½ RemoteCLIP æ–‡æœ¬ç¼–ç å™¨...")
    text_encoder = CLIPTextEncoder(
        model_name=config.clip_model_name,
        pretrained_path=config.clip_checkpoint
    )
    
    params_clip = count_parameters(text_encoder)
    
    print(f"\nğŸ“Š CLIPç¼–ç å™¨å‚æ•°:")
    print(f"  æ€»å‚æ•°: {params_clip['total']:,} ({params_clip['total_M']:.2f}M)")
    
    # ä¼°ç®—å®Œæ•´æ¨¡å‹å‚æ•°ï¼ˆå¦‚æœå®ç°äº†ï¼‰
    estimated_params = {
        'clip_encoder': params_clip['total_M'],
        'context_extractor': 10,  # ä¼°è®¡
        'decoder': 15,  # ä¼°è®¡
        'prediction_heads': 5,  # ä¼°è®¡
        'total_estimated': params_clip['total_M'] + 30
    }
    
    print(f"\nğŸ“Š ä¼°ç®—å®Œæ•´æ¨¡å‹å‚æ•°:")
    print(f"  CLIPç¼–ç å™¨: {estimated_params['clip_encoder']:.2f}M")
    print(f"  ä¸Šä¸‹æ–‡æå–å™¨: ~{estimated_params['context_extractor']:.2f}M")
    print(f"  è§£ç å™¨: ~{estimated_params['decoder']:.2f}M")
    print(f"  é¢„æµ‹å¤´: ~{estimated_params['prediction_heads']:.2f}M")
    print(f"  æ€»è®¡ï¼ˆä¼°ç®—ï¼‰: ~{estimated_params['total_estimated']:.2f}M")
    
    print(f"\nğŸ“‹ æ¨¡å‹æ¶æ„:")
    print(f"  ç±»å‹: Transformer-based Detection")
    print(f"  Stage1: CLIP Text Encoder + Global Context Extractor")
    print(f"  Stage2: Context Gating + Query Initializer + Text Conditioner")
    print(f"  Stage3: Classification Head + Regression Head")
    print(f"  Stage4: Global Contrast Loss + Box Loss + Matcher")
    
    print(f"\nâœ… å·²å®ç°æ¨¡å—: 11ä¸ª")
    print(f"âŒ ç¼ºå¤±æ¨¡å—: 5ä¸ª (æ•°æ®åŠ è½½å™¨ã€å®Œæ•´æ¨¡å‹ã€è®­ç»ƒ/è¯„ä¼°è„šæœ¬)")
    
    return {
        'experiment': 'Experiment2',
        'name': 'ä¸Šä¸‹æ–‡å¼•å¯¼æ£€æµ‹',
        'architecture': {
            'type': 'Context-Guided Transformer',
            'backbone': 'RemoteCLIP RN50',
            'queries': config.num_queries,
            'decoder_layers': config.num_decoder_layers
        },
        'parameters': {
            'clip_only': params_clip,
            'estimated_total_M': estimated_params['total_estimated']
        },
        'config': {
            'd_model': config.d_model,
            'd_clip': config.d_clip,
            'context_gating': config.context_gating_type,
            'temperature': config.temperature
        },
        'status': 'Incomplete - Missing: DataLoader, Train/Eval scripts'
    }


def test_experiment3():
    """æµ‹è¯• Experiment3"""
    
    print("\n" + "â–¶" * 35)
    print("Experiment3: OVA-DETR")
    print("â–¶" * 35)
    
    sys.path.insert(0, 'experiment3')
    
    from config.default_config import DefaultConfig
    from models.ova_detr import OVADETR
    from utils.data_loader import DIOR_CLASSES
    
    config = DefaultConfig()
    
    print(f"\nğŸ“‹ æ¨¡å‹é…ç½®:")
    print(f"  RemoteCLIP: {config.remoteclip_model}")
    print(f"  æŸ¥è¯¢æ•°é‡: {config.num_queries}")
    print(f"  è§£ç å™¨å±‚æ•°: {config.num_decoder_layers}")
    print(f"  æ¨¡å‹ç»´åº¦: {config.d_model}")
    print(f"  æ–‡æœ¬ç»´åº¦: {config.txt_dim}")
    
    # åˆ›å»ºæ¨¡å‹
    print(f"\nåˆ›å»º OVA-DETRæ¨¡å‹...")
    device = torch.device('cpu')  # ä½¿ç”¨CPUé¿å…CUDAé—®é¢˜
    model = OVADETR(config).to(device)
    model.eval()
    
    # å‚æ•°ç»Ÿè®¡
    params = count_parameters(model)
    
    print(f"\nğŸ“Š æ¨¡å‹å‚æ•°:")
    print(f"  æ€»å‚æ•°: {params['total']:,} ({params['total_M']:.2f}M)")
    print(f"  å¯è®­ç»ƒ: {params['trainable']:,} ({params['trainable_M']:.2f}M)")
    print(f"  å†»ç»“: {params['frozen']:,} ({params['frozen']/1e6:.2f}M)")
    
    # æå–æ–‡æœ¬ç‰¹å¾
    print(f"\næå–æ–‡æœ¬ç‰¹å¾...")
    with torch.no_grad():
        text_features = model.backbone.forward_text(DIOR_CLASSES).to(device)
    
    print(f"  æ–‡æœ¬ç‰¹å¾: {text_features.shape}")
    print(f"  ç±»åˆ«æ•°: {len(DIOR_CLASSES)}")
    
    # æµ‹è¯•æ¨ç†
    print(f"\nğŸ”¬ æµ‹è¯•æ¨ç†é€Ÿåº¦...")
    
    batch_size = 2
    test_images = torch.randn(batch_size, 3, 800, 800).to(device)
    
    # é¢„çƒ­
    with torch.no_grad():
        _ = model(test_images, text_features)
    
    # æµ‹è¯•
    num_runs = 5
    start_time = time.time()
    
    with torch.no_grad():
        for _ in range(num_runs):
            outputs = model(test_images, text_features)
    
    elapsed = time.time() - start_time
    
    print(f"  æµ‹è¯•æ¬¡æ•°: {num_runs} (æ¯æ¬¡{batch_size}å¼ å›¾)")
    print(f"  æ€»ç”¨æ—¶: {elapsed:.3f}ç§’")
    print(f"  å¹³å‡ç”¨æ—¶: {elapsed/(num_runs*batch_size)*1000:.1f}ms/å›¾")
    print(f"  FPS: {(num_runs*batch_size)/elapsed:.2f}")
    
    # è¾“å‡ºå½¢çŠ¶
    print(f"\nğŸ“¤ æ¨¡å‹è¾“å‡º:")
    print(f"  pred_logits: {outputs['pred_logits'].shape}")
    print(f"  pred_boxes: {outputs['pred_boxes'].shape}")
    print(f"  è§£é‡Š: ({config.num_decoder_layers}å±‚, {batch_size}æ‰¹æ¬¡, {config.num_queries}æŸ¥è¯¢, 20ç±»åˆ«/4åæ ‡)")
    
    print(f"\nğŸ“‹ å®Œæ•´æ¶æ„:")
    print(f"  1. RemoteCLIP Backbone (å†»ç»“)")
    print(f"  2. FPN ç‰¹å¾é‡‘å­—å¡” (4å±‚)")
    print(f"  3. Hybrid Encoder (CNN + Transformer)")
    print(f"  4. Text-Vision Fusion (VAT + TVG)")
    print(f"  5. Transformer Decoder (6å±‚)")
    print(f"  6. Detection Heads (å¯¹æ¯”åˆ†ç±» + MLPå›å½’)")
    
    return {
        'experiment': 'Experiment3',
        'name': 'OVA-DETR with RemoteCLIP',
        'architecture': {
            'type': 'Open-Vocabulary DETR',
            'backbone': config.remoteclip_model,
            'queries': config.num_queries,
            'decoder_layers': config.num_decoder_layers,
            'd_model': config.d_model,
            'components': [
                'RemoteCLIP Backbone',
                'FPN (4-level)',
                'Hybrid Encoder (6-layer)',
                'Text-Vision Fusion',
                'Transformer Decoder (6-layer)',
                'Contrastive Classification Head',
                'MLP Regression Head'
            ]
        },
        'parameters': params,
        'performance': {
            'avg_time_ms': (elapsed/(num_runs*batch_size)) * 1000,
            'fps': (num_runs*batch_size) / elapsed,
            'batch_size': batch_size
        },
        'loss_functions': {
            'varifocal_loss': f'alpha={config.varifocal_alpha}, gamma={config.varifocal_gamma}',
            'bbox_l1': f'weight={config.loss_bbox_weight}',
            'giou': f'weight={config.loss_giou_weight}'
        },
        'status': 'Complete - All modules implemented'
    }


def generate_comparison_report(results):
    """ç”Ÿæˆå¯¹æ¯”æŠ¥å‘Š"""
    
    print("\n" + "=" * 70)
    print("æ€§èƒ½å¯¹æ¯”æŠ¥å‘Š")
    print("=" * 70)
    
    # è¡¨æ ¼
    print(f"\n{'å®éªŒ':<15} {'æ¨¡å‹ç±»å‹':<25} {'æ€»å‚æ•°':<12} {'å¯è®­ç»ƒ':<12} {'æ¨ç†é€Ÿåº¦':<12}")
    print("-" * 76)
    
    for result in results:
        exp_name = result['experiment']
        model_type = result['architecture']['type']
        
        if 'estimated_total_M' in result['parameters']:
            total_params = f"~{result['parameters']['estimated_total_M']:.1f}M"
            trainable = "æœªçŸ¥"
        else:
            total_params = f"{result['parameters']['total_M']:.1f}M"
            trainable = f"{result['parameters']['trainable_M']:.1f}M"
        
        if 'performance' in result:
            fps = f"{result['performance']['fps']:.2f} FPS"
        else:
            fps = "N/A"
        
        print(f"{exp_name:<15} {model_type:<25} {total_params:<12} {trainable:<12} {fps:<12}")
    
    # è¯¦ç»†å¯¹æ¯”
    print(f"\n" + "=" * 70)
    print("è¯¦ç»†æ¶æ„å¯¹æ¯”")
    print("=" * 70)
    
    for i, result in enumerate(results, 1):
        print(f"\n{i}. {result['name']}")
        print(f"   ç±»å‹: {result['architecture']['type']}")
        print(f"   éª¨å¹²ç½‘ç»œ: {result['architecture'].get('backbone', 'RemoteCLIP')}")
        
        if 'components' in result['architecture']:
            print(f"   ç»„ä»¶:")
            for comp in result['architecture']['components']:
                print(f"     - {comp}")
        
        if 'features' in result:
            print(f"   ç‰¹æ€§:")
            for key, value in result['features'].items():
                print(f"     - {key}: {value}")
        
        status = result.get('status', 'Unknown')
        print(f"   çŠ¶æ€: {status}")
    
    # ä¿å­˜JSONæŠ¥å‘Š
    report_file = Path('experiments_comparison_report.json')
    with open(report_file, 'w', encoding='utf-8') as f:
        json.dump({
            'generated_at': time.strftime('%Y-%m-%d %H:%M:%S'),
            'dataset': 'mini_dataset (100 samples)',
            'experiments': results
        }, f, indent=2, ensure_ascii=False)
    
    print(f"\nâœ… è¯¦ç»†æŠ¥å‘Šä¿å­˜åˆ°: {report_file}")
    
    return report_file


def print_summary(results):
    """æ‰“å°æ€»ç»“"""
    
    print("\n" + "ğŸ¯" * 35)
    print("æ€§èƒ½æ€»ç»“")
    print("ğŸ¯" * 35)
    
    print(f"\nâœ… å®Œæˆåº¦:")
    print(f"  Experiment1: âš ï¸ éƒ¨åˆ†å®ç°ï¼ˆç¼ºå°‘æ ‡å‡†è¯„ä¼°ï¼‰")
    print(f"  Experiment2: âš ï¸ éƒ¨åˆ†å®ç°ï¼ˆç¼ºå°‘æ•°æ®åŠ è½½å’Œè¯„ä¼°ï¼‰")
    print(f"  Experiment3: âœ… å®Œæ•´å®ç°ï¼ˆæ¨èä½¿ç”¨ï¼‰")
    
    print(f"\nğŸ“Š æ¨¡å‹è§„æ¨¡:")
    for result in results:
        exp = result['experiment']
        if 'total_M' in result['parameters']:
            params = result['parameters']['total_M']
            print(f"  {exp}: {params:.1f}M å‚æ•°")
        else:
            params = result['parameters']['estimated_total_M']
            print(f"  {exp}: ~{params:.1f}M å‚æ•°ï¼ˆä¼°ç®—ï¼‰")
    
    print(f"\nâš¡ æ¨ç†é€Ÿåº¦ï¼ˆCPUï¼‰:")
    for result in results:
        if 'performance' in result:
            exp = result['experiment']
            fps = result['performance']['fps']
            print(f"  {exp}: {fps:.2f} FPS")
    
    print(f"\nğŸ¯ æ¨è:")
    print(f"  â­â­â­â­â­ Experiment3 - å®Œæ•´å®ç°ï¼Œä»£ç è´¨é‡æœ€é«˜")
    print(f"  â­â­â­ Experiment2 - æ¶æ„è®¾è®¡å¥½ï¼Œéœ€è¡¥å……å®Œæ•´")
    print(f"  â­â­â­ Experiment1 - ä¸¤é˜¶æ®µæ–¹æ³•ï¼Œéœ€ç»Ÿä¸€è¯„ä¼°")


def main():
    """ä¸»å‡½æ•°"""
    
    results = []
    
    # æµ‹è¯•ä¸‰ä¸ªå®éªŒ
    try:
        result1 = test_experiment1()
        results.append(result1)
    except Exception as e:
        print(f"\nâŒ Experiment1 æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
    
    try:
        result2 = test_experiment2()
        results.append(result2)
    except Exception as e:
        print(f"\nâŒ Experiment2 æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
    
    try:
        result3 = test_experiment3()
        results.append(result3)
    except Exception as e:
        print(f"\nâŒ Experiment3 æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
    
    # ç”ŸæˆæŠ¥å‘Š
    if len(results) > 0:
        generate_comparison_report(results)
        print_summary(results)
    
    print("\n" + "=" * 70)
    print("åŸºå‡†æµ‹è¯•å®Œæˆï¼")
    print("=" * 70)


if __name__ == '__main__':
    main()

