#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
åˆ›å»ºå°æ•°æ®é›†

åŠŸèƒ½ï¼š
1. ä»3ä¸ªæ•°æ®é›†å„é€‰20å¼ å›¾ç‰‡ï¼ˆå…±60å¼ ï¼‰
2. æä¾›ä¸åŒçš„seen/unseenåˆ†å‰²æ¯”ä¾‹
3. ç”Ÿæˆé…ç½®æ–‡ä»¶ä¾›å®éªŒä½¿ç”¨
"""

import os
import shutil
import random
import json
from pathlib import Path
from typing import Dict, List, Tuple


class MiniDatasetCreator:
    """å°æ•°æ®é›†åˆ›å»ºå™¨"""
    
    def __init__(self, output_dir: str = 'datasets/mini_dataset'):
        """
        å‚æ•°:
            output_dir: è¾“å‡ºç›®å½•
        """
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(parents=True, exist_ok=True)
        
        # æ•°æ®é›†é…ç½®
        self.datasets = {
            'hrsc2016': {
                'images_dir': Path('datasets/hrsc2016/images'),
                'annotations_dir': None,  # æš‚æ— æ ‡æ³¨
                'image_ext': '.bmp',
                'num_samples': 20,
                'classes': ['ship']
            },
            'DIOR': {
                'images_dir': Path('datasets/DIOR/images/trainval'),
                'annotations_dir': Path('datasets/DIOR/annotations/horizontal'),
                'image_ext': '.jpg',
                'num_samples': 20,
                'classes': [
                    'airplane', 'ship', 'bridge', 'harbor', 'vehicle',
                    'storage-tank', 'baseball-field', 'tennis-court',
                    'basketball-court', 'stadium'
                ]
            },
            'DOTA': {
                'images_dir': None,  # å›¾ç‰‡æœªä¸‹è½½
                'annotations_dir': Path('datasets/DOTA/DOTA-v2.0/annotations/train'),
                'image_ext': '.png',
                'num_samples': 0,  # æš‚ä¸åŒ…å«
                'classes': [
                    'plane', 'ship', 'storage-tank', 'baseball-diamond',
                    'tennis-court', 'basketball-court', 'harbor', 'bridge'
                ]
            }
        }
        
        # Seen/Unseenç±»åˆ«é…ç½®
        self.class_splits = {
            'seen_classes': [
                'airplane', 'ship', 'vehicle', 'bridge', 'harbor'
            ],
            'unseen_classes': [
                'storage-tank', 'baseball-field', 'tennis-court',
                'basketball-court', 'stadium'
            ]
        }
    
    def sample_images(self, dataset_name: str) -> List[Path]:
        """
        ä»æ•°æ®é›†ä¸­é‡‡æ ·å›¾ç‰‡
        
        å‚æ•°:
            dataset_name: æ•°æ®é›†åç§°
        
        è¿”å›:
            é‡‡æ ·çš„å›¾ç‰‡è·¯å¾„åˆ—è¡¨
        """
        config = self.datasets[dataset_name]
        images_dir = config['images_dir']
        num_samples = config['num_samples']
        
        if images_dir is None or not images_dir.exists() or num_samples == 0:
            return []
        
        # è·å–æ‰€æœ‰å›¾ç‰‡
        image_ext = config['image_ext']
        all_images = list(images_dir.glob(f'*{image_ext}'))
        
        # éšæœºé‡‡æ ·
        if len(all_images) >= num_samples:
            sampled = random.sample(all_images, num_samples)
        else:
            sampled = all_images
        
        return sorted(sampled)
    
    def create_mini_dataset(self):
        """åˆ›å»ºå°æ•°æ®é›†"""
        print("=" * 70)
        print("åˆ›å»ºå°æ•°æ®é›†ï¼ˆ60å¼ å›¾ç‰‡ï¼‰")
        print("=" * 70)
        
        # åˆ›å»ºè¾“å‡ºç›®å½•ç»“æ„
        (self.output_dir / 'images').mkdir(exist_ok=True)
        (self.output_dir / 'annotations').mkdir(exist_ok=True)
        
        all_samples = []
        
        # ä»æ¯ä¸ªæ•°æ®é›†é‡‡æ ·
        for dataset_name, config in self.datasets.items():
            print(f"\nğŸ“¦ å¤„ç† {dataset_name}...")
            
            sampled_images = self.sample_images(dataset_name)
            
            if not sampled_images:
                print(f"  â­ï¸  è·³è¿‡ï¼ˆæ— å¯ç”¨å›¾ç‰‡ï¼‰")
                continue
            
            print(f"  é‡‡æ · {len(sampled_images)} å¼ å›¾ç‰‡")
            
            # å¤åˆ¶å›¾ç‰‡å’Œæ ‡æ³¨
            for img_path in sampled_images:
                # å¤åˆ¶å›¾ç‰‡
                new_name = f"{dataset_name}_{img_path.stem}{img_path.suffix}"
                target_img = self.output_dir / 'images' / new_name
                
                shutil.copy2(img_path, target_img)
                
                # å¤åˆ¶æ ‡æ³¨ï¼ˆå¦‚æœæœ‰ï¼‰
                anno_dir = config['annotations_dir']
                if anno_dir and anno_dir.exists():
                    anno_path = anno_dir / f"{img_path.stem}.xml"
                    if anno_path.exists():
                        target_anno = self.output_dir / 'annotations' / f"{dataset_name}_{anno_path.name}"
                        shutil.copy2(anno_path, target_anno)
                
                # è®°å½•æ ·æœ¬ä¿¡æ¯
                all_samples.append({
                    'dataset': dataset_name,
                    'image_name': new_name,
                    'original_path': str(img_path)
                })
            
            print(f"  âœ… å·²å¤åˆ¶åˆ° mini_dataset/")
        
        # ä¿å­˜æ ·æœ¬åˆ—è¡¨
        samples_file = self.output_dir / 'samples.json'
        with open(samples_file, 'w', encoding='utf-8') as f:
            json.dump(all_samples, f, indent=2, ensure_ascii=False)
        
        print(f"\nğŸ“Š æ€»è®¡: {len(all_samples)} å¼ å›¾ç‰‡")
        print(f"  æ ·æœ¬åˆ—è¡¨å·²ä¿å­˜: {samples_file}")
        
        return all_samples
    
    def create_splits(
        self,
        all_samples: List[Dict],
        seen_ratio: float = 0.7
    ) -> Dict:
        """
        åˆ›å»ºseen/unseenåˆ†å‰²
        
        å‚æ•°:
            all_samples: æ‰€æœ‰æ ·æœ¬åˆ—è¡¨
            seen_ratio: seenç±»åˆ«çš„æ¯”ä¾‹
        
        è¿”å›:
            åˆ†å‰²é…ç½®
        """
        print(f"\nğŸ“Š åˆ›å»ºseen/unseenåˆ†å‰²ï¼ˆseenæ¯”ä¾‹: {seen_ratio:.0%}ï¼‰...")
        
        # æ ¹æ®æ¯”ä¾‹åˆ†é…ç±»åˆ«
        all_classes = self.class_splits['seen_classes'] + self.class_splits['unseen_classes']
        num_seen = int(len(all_classes) * seen_ratio)
        
        # éšæœºåˆ†é…seen/unseen
        random.shuffle(all_classes)
        seen_classes = all_classes[:num_seen]
        unseen_classes = all_classes[num_seen:]
        
        split_config = {
            'seen_ratio': seen_ratio,
            'seen_classes': seen_classes,
            'unseen_classes': unseen_classes,
            'num_seen': len(seen_classes),
            'num_unseen': len(unseen_classes)
        }
        
        print(f"  Seenç±»åˆ« ({len(seen_classes)}): {seen_classes}")
        print(f"  Unseenç±»åˆ« ({len(unseen_classes)}): {unseen_classes}")
        
        return split_config
    
    def save_split_configs(self, all_samples: List[Dict]):
        """
        ä¿å­˜å¤šç§åˆ†å‰²é…ç½®
        
        å‚æ•°:
            all_samples: æ‰€æœ‰æ ·æœ¬åˆ—è¡¨
        """
        print("\nğŸ“ ç”Ÿæˆå¤šç§seen/unseenåˆ†å‰²é…ç½®...")
        
        # ä¸åŒçš„åˆ†å‰²æ¯”ä¾‹
        split_ratios = [0.5, 0.6, 0.7, 0.8]
        
        configs = {}
        
        for ratio in split_ratios:
            split_name = f"seen_{int(ratio*100)}"
            split_config = self.create_splits(all_samples, ratio)
            configs[split_name] = split_config
            
            # ä¿å­˜å•ç‹¬çš„é…ç½®æ–‡ä»¶
            config_file = self.output_dir / f'split_config_{split_name}.json'
            with open(config_file, 'w', encoding='utf-8') as f:
                json.dump(split_config, f, indent=2, ensure_ascii=False)
            
            print(f"  âœ… {split_name}: {config_file}")
        
        # ä¿å­˜æ‰€æœ‰é…ç½®
        all_configs_file = self.output_dir / 'all_split_configs.json'
        with open(all_configs_file, 'w', encoding='utf-8') as f:
            json.dump(configs, f, indent=2, ensure_ascii=False)
        
        print(f"\n  âœ… æ‰€æœ‰é…ç½®å·²ä¿å­˜: {all_configs_file}")
        
        return configs
    
    def create_readme(self, all_samples: List[Dict], configs: Dict):
        """åˆ›å»ºREADMEæ–‡æ¡£"""
        readme_path = self.output_dir / 'README.md'
        
        with open(readme_path, 'w', encoding='utf-8') as f:
            f.write("# Mini Datasetï¼ˆå°æ•°æ®é›†ï¼‰\n\n")
            f.write("ç”¨äºå¿«é€Ÿå®éªŒçš„å°è§„æ¨¡æ•°æ®é›†ã€‚\n\n")
            
            f.write("## ğŸ“Š æ•°æ®é›†ç»Ÿè®¡\n\n")
            f.write(f"- **æ€»å›¾ç‰‡æ•°**: {len(all_samples)}\n")
            
            # æŒ‰æ¥æºç»Ÿè®¡
            dataset_counts = {}
            for sample in all_samples:
                ds = sample['dataset']
                dataset_counts[ds] = dataset_counts.get(ds, 0) + 1
            
            f.write("- **æ¥æºåˆ†å¸ƒ**:\n")
            for ds, count in dataset_counts.items():
                f.write(f"  - {ds}: {count}å¼ \n")
            
            f.write("\n## ğŸ¯ Seen/Unseenåˆ†å‰²é…ç½®\n\n")
            f.write("æä¾›4ç§åˆ†å‰²æ¯”ä¾‹ï¼Œå¯é€šè¿‡å‚æ•°é€‰æ‹©ï¼š\n\n")
            
            for split_name, config in configs.items():
                ratio = config['seen_ratio']
                f.write(f"### {split_name} (seen: {ratio:.0%})\n")
                f.write(f"- Seenç±»åˆ« ({config['num_seen']}ä¸ª): {', '.join(config['seen_classes'])}\n")
                f.write(f"- Unseenç±»åˆ« ({config['num_unseen']}ä¸ª): {', '.join(config['unseen_classes'])}\n\n")
            
            f.write("## ğŸš€ ä½¿ç”¨æ–¹å¼\n\n")
            f.write("### åœ¨Experiment1ä¸­ä½¿ç”¨\n\n")
            f.write("```bash\n")
            f.write("# ä½¿ç”¨seenç±»åˆ«è®­ç»ƒ\n")
            f.write("python experiment1/stage2/target_detection.py \\\n")
            f.write("  --image datasets/mini_dataset/images/DIOR_00001.jpg \\\n")
            f.write("  --target airplane\n")
            f.write("```\n\n")
            
            f.write("### åœ¨Experiment2ä¸­ä½¿ç”¨\n\n")
            f.write("```bash\n")
            f.write("# åŠ è½½åˆ†å‰²é…ç½®\n")
            f.write("python experiment2/scripts/train.py \\\n")
            f.write("  --mini-dataset datasets/mini_dataset \\\n")
            f.write("  --split-config split_config_seen_70.json\n")
            f.write("```\n\n")
            
            f.write("### Python API\n\n")
            f.write("```python\n")
            f.write("import json\n")
            f.write("from pathlib import Path\n\n")
            f.write("# åŠ è½½åˆ†å‰²é…ç½®\n")
            f.write("config_path = 'datasets/mini_dataset/split_config_seen_70.json'\n")
            f.write("with open(config_path, 'r') as f:\n")
            f.write("    split_config = json.load(f)\n\n")
            f.write("seen_classes = split_config['seen_classes']\n")
            f.write("unseen_classes = split_config['unseen_classes']\n\n")
            f.write("print(f'Seen: {seen_classes}')\n")
            f.write("print(f'Unseen: {unseen_classes}')\n")
            f.write("```\n\n")
            
            f.write("## ğŸ“ ç›®å½•ç»“æ„\n\n")
            f.write("```\n")
            f.write("mini_dataset/\n")
            f.write("â”œâ”€â”€ images/                  # 60å¼ å›¾ç‰‡\n")
            f.write("â”œâ”€â”€ annotations/             # å¯¹åº”çš„æ ‡æ³¨æ–‡ä»¶\n")
            f.write("â”œâ”€â”€ samples.json             # æ ·æœ¬åˆ—è¡¨\n")
            f.write("â”œâ”€â”€ split_config_seen_50.json  # 50%é…ç½®\n")
            f.write("â”œâ”€â”€ split_config_seen_60.json  # 60%é…ç½®\n")
            f.write("â”œâ”€â”€ split_config_seen_70.json  # 70%é…ç½®\n")
            f.write("â”œâ”€â”€ split_config_seen_80.json  # 80%é…ç½®\n")
            f.write("â”œâ”€â”€ all_split_configs.json   # æ‰€æœ‰é…ç½®\n")
            f.write("â””â”€â”€ README.md                # æœ¬æ–‡æ¡£\n")
            f.write("```\n\n")
            
            f.write("## ğŸ¯ å®éªŒå»ºè®®\n\n")
            f.write("1. **seen_50**: å¯¹åŠåˆ†ï¼Œæµ‹è¯•é›¶æ ·æœ¬æ³›åŒ–èƒ½åŠ›\n")
            f.write("2. **seen_60**: è½»å¾®å€¾å‘seenï¼Œå¹³è¡¡æµ‹è¯•\n")
            f.write("3. **seen_70**: æ¨èé…ç½®ï¼Œè¶³å¤Ÿçš„seenç±»åˆ«è®­ç»ƒ\n")
            f.write("4. **seen_80**: å¤§éƒ¨åˆ†seenï¼Œå°‘é‡unseenæµ‹è¯•\n")
        
        print(f"âœ… READMEå·²åˆ›å»º: {readme_path}")


def main():
    """ä¸»å‡½æ•°"""
    print("=" * 70)
    print("åˆ›å»ºå°æ•°æ®é›†ï¼ˆç”¨äºå¿«é€Ÿå®éªŒï¼‰")
    print("=" * 70)
    
    # è®¾ç½®éšæœºç§å­
    random.seed(42)
    
    # åˆ›å»ºå°æ•°æ®é›†
    creator = MiniDatasetCreator()
    
    # é‡‡æ ·å›¾ç‰‡
    all_samples = creator.create_mini_dataset()
    
    # åˆ›å»ºå¤šç§åˆ†å‰²é…ç½®
    configs = creator.save_split_configs(all_samples)
    
    # åˆ›å»ºREADME
    creator.create_readme(all_samples, configs)
    
    print("\n" + "=" * 70)
    print("âœ… å°æ•°æ®é›†åˆ›å»ºå®Œæˆï¼")
    print("=" * 70)
    print(f"\nğŸ“ ä½ç½®: {creator.output_dir}")
    print(f"ğŸ“Š å›¾ç‰‡: {len(all_samples)} å¼ ")
    print(f"ğŸ“‹ é…ç½®: {len(configs)} ç§seen/unseenåˆ†å‰²")


if __name__ == "__main__":
    main()

