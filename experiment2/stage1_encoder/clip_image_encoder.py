#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
RemoteCLIP å›¾åƒç¼–ç å™¨

åŠŸèƒ½ï¼š
æå–å›¾åƒçš„å…¨å±€ç‰¹å¾ç”¨äºä¸Šä¸‹æ–‡å¼•å¯¼
"""

import torch
import torch.nn as nn
import open_clip


class CLIPImageEncoder(nn.Module):
    """RemoteCLIP å›¾åƒç¼–ç å™¨"""
    
    def __init__(
        self,
        model_name: str = 'RN50',
        pretrained_path: str = 'checkpoints/RemoteCLIP-RN50.pt',
        freeze: bool = True
    ):
        super().__init__()
        
        self.model_name = model_name
        
        # åŠ è½½RemoteCLIPæ¨¡å‹
        print(f"ğŸ”„ åŠ è½½RemoteCLIPå›¾åƒç¼–ç å™¨: {model_name}")
        self.model, _, _ = open_clip.create_model_and_transforms(model_name)
        
        if pretrained_path:
            print(f"ğŸ“¦ åŠ è½½RemoteCLIPæƒé‡: {pretrained_path}")
            ckpt = torch.load(pretrained_path, map_location='cpu')
            self.model.load_state_dict(ckpt)
            print(f"âœ… RemoteCLIPæƒé‡åŠ è½½æˆåŠŸ")
        
        # å†»ç»“å‚æ•°
        if freeze:
            for param in self.model.parameters():
                param.requires_grad = False
            self.model.eval()
    
    def forward(self, images: torch.Tensor):
        """
        å‰å‘ä¼ æ’­
        
        å‚æ•°:
            images: (B, 3, H, W) å›¾åƒå¼ é‡
        
        è¿”å›:
            multi_scale_features: å¤šå°ºåº¦ç‰¹å¾ï¼ˆç®€åŒ–ç‰ˆï¼Œè¿”å›å…¨å±€ç‰¹å¾ï¼‰
            global_embedding: (B, d_clip) å…¨å±€å›¾åƒç‰¹å¾
        """
        # å¦‚æœæ˜¯è®­ç»ƒæ¨¡å¼ï¼Œå…è®¸æ¢¯åº¦
        if self.training:
            global_embedding = self.model.encode_image(images)
            global_embedding = global_embedding / global_embedding.norm(dim=-1, keepdim=True)
        else:
            with torch.no_grad():
                global_embedding = self.model.encode_image(images)
                global_embedding = global_embedding / global_embedding.norm(dim=-1, keepdim=True)
        
        # è¿”å›å…¨å±€ç‰¹å¾ï¼ˆå¤šå°ºåº¦ç‰¹å¾ç®€åŒ–ä¸ºå…¨å±€ç‰¹å¾ï¼‰
        multi_scale_features = global_embedding  # ç®€åŒ–å®ç°
        
        return multi_scale_features, global_embedding


if __name__ == "__main__":
    encoder = CLIPImageEncoder(
        model_name='RN50',
        pretrained_path='../../checkpoints/RemoteCLIP-RN50.pt'
    )
    
    images = torch.randn(2, 3, 800, 800)
    features = encoder(images)
    
    print(f"å›¾åƒç‰¹å¾å½¢çŠ¶: {features.shape}")
    print("âœ… RemoteCLIPå›¾åƒç¼–ç å™¨æµ‹è¯•å®Œæˆï¼")

