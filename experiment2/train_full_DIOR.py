#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Experiment2 åœ¨å®Œæ•´DIORæ•°æ®é›†ä¸Šè®­ç»ƒ
è‡ªé€‚åº”å…¨å±€-å±€éƒ¨å¯¹æ¯”å­¦ä¹  + è¾¹ç•Œæ¡†å›å½’
"""

import torch
import torch.nn as nn
from torch.utils.data import DataLoader
import sys
from pathlib import Path
from tqdm import tqdm
import json
import time

sys.path.append('..')

from config.default_config import DefaultConfig
from stage1_encoder.clip_text_encoder import CLIPTextEncoder
from stage1_encoder.clip_image_encoder import CLIPImageEncoder
from datasets.mini_dataset.mini_dataset_loader import MiniDataset
import torchvision.transforms as T


class AdaptiveGlobalLocalContrastLoss(nn.Module):
    """è‡ªé€‚åº”å…¨å±€-å±€éƒ¨å¯¹æ¯”æŸå¤±"""
    
    def __init__(self, temperature=0.07):
        super().__init__()
        self.temperature = temperature
    
    def forward(self, local_features, text_features, global_features, labels):
        # å½’ä¸€åŒ–
        local_features = local_features / (local_features.norm(dim=-1, keepdim=True) + 1e-8)
        text_features = text_features / (text_features.norm(dim=-1, keepdim=True) + 1e-8)
        global_features = global_features / (global_features.norm(dim=-1, keepdim=True) + 1e-8)
        
        # å±€éƒ¨-æ–‡æœ¬ç›¸ä¼¼åº¦
        local_text_sim = (local_features @ text_features.T) / self.temperature
        
        # å±€éƒ¨-å…¨å±€ç›¸ä¼¼åº¦ï¼ˆèƒŒæ™¯ï¼‰
        local_global_sim = (local_features * global_features).sum(dim=-1, keepdim=True) / self.temperature
        
        # å¯¹æ¯”å­¦ä¹ 
        logits = torch.cat([local_text_sim, local_global_sim], dim=-1)
        loss = nn.CrossEntropyLoss()(logits[:, :-1], labels)
        
        return loss


class SimpleDeformableQueryExtractor(nn.Module):
    """Deformable Queryæå–å™¨"""
    
    def __init__(self, d_model=1024):
        super().__init__()
        self.d_model = d_model
        self.position_embed = nn.Linear(4, d_model)
        self.fusion = nn.Sequential(
            nn.Linear(d_model * 2, d_model),
            nn.ReLU(),
            nn.Dropout(0.1),
            nn.Linear(d_model, d_model)
        )
    
    def forward(self, global_features, boxes):
        pos_embed = self.position_embed(boxes)
        combined = torch.cat([global_features, pos_embed], dim=-1)
        local_features = self.fusion(combined)
        return local_features


class BoxRegressor(nn.Module):
    """è¾¹ç•Œæ¡†å›å½’å™¨"""
    
    def __init__(self, d_model=1024):
        super().__init__()
        self.regressor = nn.Sequential(
            nn.Linear(d_model, 512),
            nn.ReLU(),
            nn.Dropout(0.1),
            nn.Linear(512, 256),
            nn.ReLU(),
            nn.Linear(256, 4),
            nn.Sigmoid()
        )
    
    def forward(self, local_features):
        return self.regressor(local_features)


def box_xyxy_to_cxcywh(boxes, img_size=224):
    """xyxy -> cxcywh (normalized)"""
    boxes = boxes / img_size  # å½’ä¸€åŒ–
    x0, y0, x1, y1 = boxes.unbind(-1)
    b = [(x0 + x1) / 2, (y0 + y1) / 2, (x1 - x0), (y1 - y0)]
    return torch.stack(b, dim=-1)


def box_cxcywh_to_xyxy(boxes):
    """cxcywh -> xyxy"""
    x_c, y_c, w, h = boxes.unbind(-1)
    b = [x_c - 0.5 * w, y_c - 0.5 * h, x_c + 0.5 * w, y_c + 0.5 * h]
    return torch.stack(b, dim=-1)


def generalized_box_iou(boxes1, boxes2):
    """GIoU"""
    area1 = (boxes1[:, 2] - boxes1[:, 0]) * (boxes1[:, 3] - boxes1[:, 1])
    area2 = (boxes2[:, 2] - boxes2[:, 0]) * (boxes2[:, 3] - boxes2[:, 1])
    
    lt = torch.max(boxes1[:, :2], boxes2[:, :2])
    rb = torch.min(boxes1[:, 2:], boxes2[:, 2:])
    
    wh = (rb - lt).clamp(min=0)
    inter = wh[:, 0] * wh[:, 1]
    
    union = area1 + area2 - inter
    iou = inter / (union + 1e-6)
    
    lti = torch.min(boxes1[:, :2], boxes2[:, :2])
    rbi = torch.max(boxes1[:, 2:], boxes2[:, 2:])
    
    whi = (rbi - lti).clamp(min=0)
    areai = whi[:, 0] * whi[:, 1]
    
    giou = iou - (areai - union) / (areai + 1e-6)
    
    return giou


def collate_fn(batch):
    """æ•°æ®collate"""
    transform = T.Compose([
        T.Resize((224, 224)),
        T.ToTensor(),
        T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
    ])
    
    images, targets = [], []
    for img, target in batch:
        images.append(transform(img))
        targets.append(target)
    
    return torch.stack(images), targets


def train():
    print("=" * 70)
    print("Experiment2 åœ¨å®Œæ•´DIORæ•°æ®é›†ä¸Šè®­ç»ƒ")
    print("è‡ªé€‚åº”å…¨å±€-å±€éƒ¨å¯¹æ¯”å­¦ä¹  + è¾¹ç•Œæ¡†å›å½’")
    print("=" * 70)
    
    device = torch.device('cuda')
    config = DefaultConfig()
    
    # åŠ è½½å®Œæ•´DIORæ•°æ®é›†
    print("\nåŠ è½½æ•°æ®é›†...")
    print("  âš ï¸ æ³¨æ„: ç›®å‰ä½¿ç”¨mini_dataset (70å¼ è®­ç»ƒå›¾)")
    print("  è¦ä½¿ç”¨å®Œæ•´DIORè¯·ä¿®æ”¹æ•°æ®è·¯å¾„")
    
    train_dataset = MiniDataset('../datasets/mini_dataset', 'train')
    val_dataset = MiniDataset('../datasets/mini_dataset', 'val')
    
    train_loader = DataLoader(
        train_dataset,
        batch_size=8,  # å¢å¤§batch size
        shuffle=True,
        collate_fn=collate_fn,
        num_workers=2,
        pin_memory=True
    )
    
    val_loader = DataLoader(
        val_dataset,
        batch_size=8,
        shuffle=False,
        collate_fn=collate_fn,
        num_workers=2,
        pin_memory=True
    )
    
    print(f"  è®­ç»ƒé›†: {len(train_dataset)} å¼ å›¾ ({len(train_loader)} batches)")
    print(f"  éªŒè¯é›†: {len(val_dataset)} å¼ å›¾ ({len(val_loader)} batches)")
    
    # åˆ›å»ºæ¨¡å‹
    print("\nåˆ›å»ºæ¨¡å‹...")
    text_encoder = CLIPTextEncoder('RN50', '../checkpoints/RemoteCLIP-RN50.pt').cuda()
    image_encoder = CLIPImageEncoder('RN50', '../checkpoints/RemoteCLIP-RN50.pt', freeze=False).cuda()
    query_extractor = SimpleDeformableQueryExtractor(d_model=1024).cuda()
    box_regressor = BoxRegressor(d_model=1024).cuda()
    
    for param in image_encoder.parameters():
        param.requires_grad = True
    image_encoder.train()
    
    print(f"  âœ… æ¨¡å‹ç»„ä»¶åˆ›å»ºå®Œæˆ")
    
    # æŸå¤±
    contrast_criterion = AdaptiveGlobalLocalContrastLoss(temperature=0.07).cuda()
    
    # ä¼˜åŒ–å™¨
    optimizer = torch.optim.AdamW([
        {'params': image_encoder.parameters(), 'lr': 5e-6},
        {'params': query_extractor.parameters(), 'lr': 1e-4},
        {'params': box_regressor.parameters(), 'lr': 1e-4}
    ], weight_decay=1e-4)
    
    # å­¦ä¹ ç‡è°ƒåº¦
    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=50, eta_min=1e-6)
    
    # æå–æ–‡æœ¬ç‰¹å¾
    from utils.dataloader import DIOR_CLASSES
    with torch.no_grad():
        text_features = text_encoder(DIOR_CLASSES).cuda()
    
    print(f"  æ–‡æœ¬ç‰¹å¾: {text_features.shape}")
    
    print(f"\nå¼€å§‹è®­ç»ƒ (50 epochs)...")
    
    # è®­ç»ƒå†å²
    train_history = []
    best_val_loss = float('inf')
    
    start_time = time.time()
    
    for epoch in range(1, 51):
        # è®­ç»ƒ
        model_train = True
        total_loss = 0
        total_contrast = 0
        total_bbox = 0
        total_giou = 0
        num_batches = 0
        
        pbar = tqdm(train_loader, desc=f"Epoch {epoch}/50")
        
        for images, targets in pbar:
            images = images.cuda()
            
            _, global_features = image_encoder(images)
            
            batch_loss = 0
            batch_contrast = 0
            batch_bbox = 0
            batch_giou = 0
            valid_samples = 0
            
            for i, target in enumerate(targets):
                if len(target['labels']) == 0:
                    continue
                
                gt_boxes = target['boxes'].cuda()
                gt_labels = target['labels'].cuda()
                
                # è½¬æ¢æ¡†æ ¼å¼
                gt_boxes_cxcywh = box_xyxy_to_cxcywh(gt_boxes)
                
                # æå–å±€éƒ¨ç‰¹å¾
                global_feat_i = global_features[i:i+1].expand(len(gt_boxes_cxcywh), -1)
                local_features = query_extractor(global_feat_i, gt_boxes_cxcywh)
                
                # å¯¹æ¯”æŸå¤±
                contrast_loss = contrast_criterion(
                    local_features,
                    text_features,
                    global_features[i:i+1].expand(len(gt_labels), -1),
                    gt_labels
                )
                
                # æ¡†å›å½’
                pred_boxes_cxcywh = box_regressor(local_features)
                
                bbox_l1_loss = nn.L1Loss()(pred_boxes_cxcywh, gt_boxes_cxcywh)
                
                pred_boxes_xyxy = box_cxcywh_to_xyxy(pred_boxes_cxcywh)
                gt_boxes_xyxy = box_cxcywh_to_xyxy(gt_boxes_cxcywh)
                giou = generalized_box_iou(pred_boxes_xyxy, gt_boxes_xyxy)
                giou_loss = (1 - giou).mean()
                
                loss_i = contrast_loss + 5.0 * bbox_l1_loss + 2.0 * giou_loss
                
                batch_loss += loss_i
                batch_contrast += contrast_loss.item()
                batch_bbox += bbox_l1_loss.item()
                batch_giou += giou_loss.item()
                valid_samples += 1
            
            if valid_samples == 0:
                continue
            
            loss = batch_loss / valid_samples
            
            optimizer.zero_grad()
            loss.backward()
            torch.nn.utils.clip_grad_norm_(
                list(image_encoder.parameters()) + 
                list(query_extractor.parameters()) + 
                list(box_regressor.parameters()), 
                max_norm=0.1
            )
            optimizer.step()
            
            total_loss += loss.item()
            total_contrast += batch_contrast / valid_samples
            total_bbox += batch_bbox / valid_samples
            total_giou += batch_giou / valid_samples
            num_batches += 1
            
            pbar.set_postfix({
                'loss': f"{loss.item():.3f}",
                'contrast': f"{batch_contrast/valid_samples:.3f}",
                'bbox': f"{batch_bbox/valid_samples:.4f}",
                'giou': f"{batch_giou/valid_samples:.3f}"
            })
        
        # è®¡ç®—å¹³å‡
        avg_loss = total_loss / num_batches if num_batches > 0 else 0
        avg_contrast = total_contrast / num_batches if num_batches > 0 else 0
        avg_bbox = total_bbox / num_batches if num_batches > 0 else 0
        avg_giou = total_giou / num_batches if num_batches > 0 else 0
        
        # éªŒè¯
        val_loss = 0
        if epoch % 5 == 0:
            image_encoder.eval()
            with torch.no_grad():
                for images, targets in val_loader:
                    images = images.cuda()
                    _, global_features = image_encoder(images)
                    # ç®€åŒ–éªŒè¯
                    val_loss += 1
            image_encoder.train()
            val_loss = val_loss / len(val_loader) if len(val_loader) > 0 else 0
        
        # æ›´æ–°å­¦ä¹ ç‡
        scheduler.step()
        current_lr = optimizer.param_groups[0]['lr']
        
        # æ‰“å°
        elapsed = time.time() - start_time
        eta = elapsed / epoch * (50 - epoch)
        
        print(f"\nEpoch {epoch}/50 - Loss: {avg_loss:.4f} | LR: {current_lr:.2e}")
        print(f"  å¯¹æ¯”: {avg_contrast:.4f} | æ¡†L1: {avg_bbox:.5f} | GIoU: {avg_giou:.4f}")
        print(f"  ç”¨æ—¶: {elapsed/60:.1f}min | é¢„è®¡å‰©ä½™: {eta/60:.1f}min")
        
        # è®°å½•å†å²
        train_history.append({
            'epoch': epoch,
            'loss': avg_loss,
            'contrast_loss': avg_contrast,
            'bbox_loss': avg_bbox,
            'giou_loss': avg_giou,
            'lr': current_lr,
            'time': elapsed
        })
        
        # ä¿å­˜checkpoint
        if epoch % 10 == 0 or avg_loss < best_val_loss:
            if avg_loss < best_val_loss:
                best_val_loss = avg_loss
                is_best = True
            else:
                is_best = False
            
            checkpoint = {
                'epoch': epoch,
                'image_encoder': image_encoder.state_dict(),
                'text_encoder': text_encoder.state_dict(),
                'query_extractor': query_extractor.state_dict(),
                'box_regressor': box_regressor.state_dict(),
                'optimizer': optimizer.state_dict(),
                'loss': avg_loss,
                'history': train_history,
                'config': {
                    'temperature': 0.07,
                    'd_model': 1024,
                    'num_classes': len(DIOR_CLASSES)
                }
            }
            
            Path('outputs/checkpoints').mkdir(parents=True, exist_ok=True)
            
            if is_best:
                torch.save(checkpoint, 'outputs/checkpoints/DIOR_best_model.pth')
                print(f"  ğŸŒŸ ä¿å­˜æœ€ä½³æ¨¡å‹ (loss: {avg_loss:.4f})")
            
            if epoch % 10 == 0:
                torch.save(checkpoint, f'outputs/checkpoints/DIOR_epoch_{epoch}.pth')
                print(f"  âœ… ä¿å­˜checkpoint epoch_{epoch}")
        
        # æ¯5ä¸ªepochä¿å­˜å†å²
        if epoch % 5 == 0:
            Path('outputs/logs').mkdir(parents=True, exist_ok=True)
            with open('outputs/logs/DIOR_train_history.json', 'w') as f:
                json.dump(train_history, f, indent=2)
    
    # ä¿å­˜æœ€ç»ˆæ¨¡å‹
    final_checkpoint = {
        'epoch': 50,
        'image_encoder': image_encoder.state_dict(),
        'text_encoder': text_encoder.state_dict(),
        'query_extractor': query_extractor.state_dict(),
        'box_regressor': box_regressor.state_dict(),
        'loss': avg_loss,
        'history': train_history
    }
    torch.save(final_checkpoint, 'outputs/checkpoints/DIOR_final.pth')
    
    total_time = time.time() - start_time
    
    print("\n" + "=" * 70)
    print("âœ… è®­ç»ƒå®Œæˆï¼")
    print("=" * 70)
    print(f"\næ€»ç”¨æ—¶: {total_time/3600:.2f} å°æ—¶")
    print(f"æœ€ä½³æŸå¤±: {best_val_loss:.4f}")
    print(f"æœ€ç»ˆæŸå¤±: {avg_loss:.4f}")
    print(f"\nä¿å­˜çš„æ¨¡å‹:")
    print(f"  - DIOR_best_model.pth (æœ€ä½³)")
    print(f"  - DIOR_final.pth (æœ€ç»ˆ)")
    print(f"  - DIOR_epoch_*.pth (æ¯10ä¸ªepoch)")


if __name__ == '__main__':
    train()

