#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æ¨ç†å¼•æ“

æ”¯æŒï¼š
1. å•å¼ å›¾åƒ + å•ä¸ªç±»åˆ«
2. å•å¼ å›¾åƒ + å¤šä¸ªç±»åˆ«
3. æ‰¹é‡å›¾åƒ + å¼€æ”¾è¯æ±‡
"""

import torch
import argparse
import cv2
import numpy as np
from PIL import Image
import sys
import os

# æ·»åŠ è·¯å¾„
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from models import ContextGuidedDetector
from .post_processor import PostProcessor


class InferenceEngine:
    """æ¨ç†å¼•æ“ - åŸºäºRemoteCLIP"""
    
    def __init__(
        self,
        model_name: str = 'RN50',
        pretrained_path: str = 'checkpoints/RemoteCLIP-RN50.pt',
        score_threshold: float = 0.5,
        nms_threshold: float = 0.7,
        device: str = 'cuda'
    ):
        """
        å‚æ•°:
            model_name: RemoteCLIPæ¨¡å‹åç§°
            pretrained_path: RemoteCLIPé¢„è®­ç»ƒæƒé‡è·¯å¾„
            score_threshold: åˆ†æ•°é˜ˆå€¼
            nms_threshold: NMSé˜ˆå€¼
            device: è®¡ç®—è®¾å¤‡
        """
        self.device = device if torch.cuda.is_available() else 'cpu'
        
        print(f"ğŸ”„ åˆå§‹åŒ–æ¨ç†å¼•æ“ï¼ˆRemoteCLIP-{model_name}ï¼‰...")
        
        # åŠ è½½æ¨¡å‹
        self.model = ContextGuidedDetector(
            model_name=model_name,
            pretrained_path=pretrained_path
        )
        self.model = self.model.to(self.device).eval()
        
        print(f"âœ… æ¨ç†å¼•æ“åˆå§‹åŒ–å®Œæˆ")
        
        # åå¤„ç†å™¨
        self.post_processor = PostProcessor(
            score_threshold=score_threshold,
            nms_threshold=nms_threshold
        )
        
        # é¢„å¤„ç†
        self.preprocess = self.model.image_encoder.preprocess
    
    @torch.no_grad()
    def infer_single(
        self,
        image_path: str,
        text_query: str
    ) -> dict:
        """
        å•å¼ å›¾åƒ + å•ä¸ªç±»åˆ«æ¨ç†
        
        å‚æ•°:
            image_path: å›¾åƒè·¯å¾„
            text_query: æ–‡æœ¬æŸ¥è¯¢ï¼Œå¦‚ "airplane"
        
        è¿”å›:
            result: æ£€æµ‹ç»“æœå­—å…¸
        """
        # åŠ è½½å›¾åƒ
        image = Image.open(image_path).convert('RGB')
        image_tensor = self.preprocess(image).unsqueeze(0).to(self.device)
        
        # æ¨ç†
        outputs = self.model(image_tensor, [text_query])
        
        # åå¤„ç†
        results = self.post_processor(
            outputs['pred_boxes'],
            outputs['scores']
        )
        
        return results[0]
    
    @torch.no_grad()
    def infer_multi_class(
        self,
        image_path: str,
        text_queries: List[str]
    ) -> Dict[str, dict]:
        """
        å•å¼ å›¾åƒ + å¤šä¸ªç±»åˆ«æ¨ç†
        
        å‚æ•°:
            image_path: å›¾åƒè·¯å¾„
            text_queries: æ–‡æœ¬æŸ¥è¯¢åˆ—è¡¨
        
        è¿”å›:
            results_dict: æ¯ä¸ªç±»åˆ«çš„æ£€æµ‹ç»“æœå­—å…¸
        """
        results_dict = {}
        
        for text_query in text_queries:
            result = self.infer_single(image_path, text_query)
            results_dict[text_query] = result
        
        return results_dict
    
    def visualize_results(
        self,
        image_path: str,
        results: dict,
        text_query: str,
        output_path: str = None
    ):
        """
        å¯è§†åŒ–ç»“æœ
        
        å‚æ•°:
            image_path: å›¾åƒè·¯å¾„
            results: æ£€æµ‹ç»“æœ
            text_query: æ–‡æœ¬æŸ¥è¯¢
            output_path: è¾“å‡ºè·¯å¾„
        """
        # åŠ è½½å›¾åƒ
        image = cv2.imread(image_path)
        h, w = image.shape[:2]
        
        boxes = results['boxes'].cpu().numpy()
        scores = results['scores'].cpu().numpy()
        
        # ç»˜åˆ¶è¾¹ç•Œæ¡†
        for i, (box, score) in enumerate(zip(boxes, scores)):
            cx, cy, bw, bh = box
            
            # è½¬æ¢åˆ°åƒç´ åæ ‡
            x1 = int((cx - bw/2) * w)
            y1 = int((cy - bh/2) * h)
            x2 = int((cx + bw/2) * w)
            y2 = int((cy + bh/2) * h)
            
            # ç»˜åˆ¶çŸ©å½¢
            cv2.rectangle(image, (x1, y1), (x2, y2), (0, 255, 0), 2)
            
            # ç»˜åˆ¶æ ‡ç­¾
            label = f"{text_query}: {score:.2f}"
            cv2.putText(image, label, (x1, y1-10), 
                       cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)
        
        # ä¿å­˜æˆ–æ˜¾ç¤º
        if output_path:
            cv2.imwrite(output_path, image)
            print(f"ç»“æœå·²ä¿å­˜: {output_path}")
        
        return image


def main():
    """æµ‹è¯•æ¨ç†å¼•æ“"""
    parser = argparse.ArgumentParser(description='Experiment2 æ¨ç†å¼•æ“')
    parser.add_argument('--image', type=str, default='assets/airport.jpg',
                        help='è¾“å…¥å›¾åƒè·¯å¾„')
    parser.add_argument('--text', type=str, nargs='+', default=['airplane'],
                        help='æ–‡æœ¬æŸ¥è¯¢')
    parser.add_argument('--model', type=str, default='RN50',
                        help='æ¨¡å‹åç§°')
    parser.add_argument('--threshold', type=float, default=0.5,
                        help='åˆ†æ•°é˜ˆå€¼')
    parser.add_argument('--output', type=str, default=None,
                        help='è¾“å‡ºè·¯å¾„')
    
    args = parser.parse_args()
    
    print("=" * 70)
    print("Experiment2 æ¨ç†å¼•æ“")
    print("=" * 70)
    
    # åˆ›å»ºæ¨ç†å¼•æ“
    engine = InferenceEngine(
        model_name=args.model,
        score_threshold=args.threshold
    )
    
    # æ¨ç†
    if len(args.text) == 1:
        # å•ç±»åˆ«æ¨ç†
        result = engine.infer_single(args.image, args.text[0])
        
        print(f"\næ£€æµ‹ç»“æœ ({args.text[0]}):")
        print(f"  æ£€æµ‹æ•°: {result['num_detections']}")
        
        # å¯è§†åŒ–
        if args.output:
            engine.visualize_results(args.image, result, args.text[0], args.output)
    
    else:
        # å¤šç±»åˆ«æ¨ç†
        results_dict = engine.infer_multi_class(args.image, args.text)
        
        print("\næ£€æµ‹ç»“æœ:")
        for text_query, result in results_dict.items():
            print(f"  {text_query}: {result['num_detections']} ä¸ªæ£€æµ‹")
    
    print("\nâœ… æ¨ç†å®Œæˆï¼")


if __name__ == "__main__":
    main()

