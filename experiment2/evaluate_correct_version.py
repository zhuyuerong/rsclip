#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
è¯„ä¼°æ­£ç¡®ç‰ˆæœ¬çš„Experiment2
ä½¿ç”¨æ–‡æœ¬é©±åŠ¨çš„ä½ç½®é¢„æµ‹
"""

import torch
import torch.nn as nn
import sys
from pathlib import Path
import json
import numpy as np
from tqdm import tqdm
import torchvision.transforms as T

sys.path.append('..')

from datasets.mini_dataset.mini_dataset_loader import MiniDataset, DIOR_CLASSES


# ä»è®­ç»ƒè„šæœ¬å¯¼å…¥æ¨¡å‹ç±»
class TextToPositionProjector(nn.Module):
    def __init__(self, text_dim=1024, hidden_dim=512):
        super().__init__()
        self.projector = nn.Sequential(
            nn.Linear(text_dim, hidden_dim),
            nn.ReLU(),
            nn.Dropout(0.1),
            nn.Linear(hidden_dim, 256),
            nn.ReLU(),
            nn.Linear(256, 4),
            nn.Sigmoid()
        )
    
    def forward(self, text_features):
        return self.projector(text_features)


class SimpleDeformableQueryExtractor(nn.Module):
    def __init__(self, d_model=1024):
        super().__init__()
        self.position_embed = nn.Linear(4, d_model)
        self.fusion = nn.Sequential(
            nn.Linear(d_model * 2, d_model),
            nn.ReLU(),
            nn.Dropout(0.1),
            nn.Linear(d_model, d_model)
        )
    
    def forward(self, global_features, boxes):
        pos_embed = self.position_embed(boxes)
        combined = torch.cat([global_features, pos_embed], dim=-1)
        return self.fusion(combined)


class BoxRegressor(nn.Module):
    def __init__(self, d_model=1024):
        super().__init__()
        self.regressor = nn.Sequential(
            nn.Linear(d_model, 512),
            nn.ReLU(),
            nn.Dropout(0.1),
            nn.Linear(512, 256),
            nn.ReLU(),
            nn.Linear(256, 4),
            nn.Sigmoid()
        )
    
    def forward(self, local_features):
        return self.regressor(local_features)


def box_cxcywh_to_xyxy(boxes):
    x_c, y_c, w, h = boxes.unbind(-1)
    b = [x_c - 0.5 * w, y_c - 0.5 * h, x_c + 0.5 * w, y_c + 0.5 * h]
    return torch.stack(b, dim=-1)


def calculate_iou(boxes1, boxes2):
    """è®¡ç®—IoUçŸ©é˜µ (boxes1: [N, 4], boxes2: [M, 4])"""
    area1 = (boxes1[:, 2] - boxes1[:, 0]) * (boxes1[:, 3] - boxes1[:, 1])
    area2 = (boxes2[:, 2] - boxes2[:, 0]) * (boxes2[:, 3] - boxes2[:, 1])
    
    lt = torch.max(boxes1[:, None, :2], boxes2[:, :2])
    rb = torch.min(boxes1[:, None, 2:], boxes2[:, 2:])
    
    wh = (rb - lt).clamp(min=0)
    inter = wh[:, :, 0] * wh[:, :, 1]
    
    union = area1[:, None] + area2 - inter
    iou = inter / (union + 1e-6)
    
    return iou


def calculate_map(all_predictions, all_targets, iou_threshold=0.5):
    """è®¡ç®—mAP"""
    num_classes = 20
    aps = []
    
    for cls in range(num_classes):
        # æ”¶é›†è¯¥ç±»çš„æ‰€æœ‰é¢„æµ‹å’ŒGT
        cls_preds = []
        cls_targets = []
        
        for preds, targets in zip(all_predictions, all_targets):
            # é¢„æµ‹
            if len(preds['boxes']) > 0:
                cls_mask = preds['labels'] == cls
                if cls_mask.any():
                    cls_preds.append({
                        'boxes': preds['boxes'][cls_mask],
                        'scores': preds['scores'][cls_mask]
                    })
                else:
                    cls_preds.append({'boxes': torch.empty(0, 4), 'scores': torch.empty(0)})
            else:
                cls_preds.append({'boxes': torch.empty(0, 4), 'scores': torch.empty(0)})
            
            # GT
            if len(targets['boxes']) > 0:
                cls_mask_gt = targets['labels'] == cls
                if cls_mask_gt.any():
                    cls_targets.append(targets['boxes'][cls_mask_gt])
                else:
                    cls_targets.append(torch.empty(0, 4))
            else:
                cls_targets.append(torch.empty(0, 4))
        
        # è®¡ç®—è¯¥ç±»çš„AP
        all_scores = []
        all_tp = []
        num_gt = sum([len(gt) for gt in cls_targets])
        
        if num_gt == 0:
            continue
        
        for pred, gt in zip(cls_preds, cls_targets):
            if len(pred['boxes']) == 0:
                continue
            
            scores = pred['scores'].cpu()
            boxes = pred['boxes'].cpu()
            
            if len(gt) == 0:
                all_scores.extend(scores.tolist())
                all_tp.extend([0] * len(scores))
            else:
                gt_boxes = gt.cpu()
                iou = calculate_iou(boxes, gt_boxes)
                
                for i in range(len(boxes)):
                    all_scores.append(scores[i].item())
                    if iou[i].max() >= iou_threshold:
                        all_tp.append(1)
                    else:
                        all_tp.append(0)
        
        if len(all_scores) == 0:
            continue
        
        # æŒ‰åˆ†æ•°æ’åº
        indices = np.argsort(all_scores)[::-1]
        tp = np.array(all_tp)[indices]
        
        # è®¡ç®—precisionå’Œrecall
        ç´¯è®¡tp = np.cumsum(tp)
        ç´¯è®¡fp = np.cumsum(1 - tp)
        
        recall = ç´¯è®¡tp / num_gt
        precision = ç´¯è®¡tp / (ç´¯è®¡tp + ç´¯è®¡fp)
        
        # è®¡ç®—AP (11-point interpolation)
        ap = 0
        for t in np.arange(0, 1.1, 0.1):
            if np.sum(recall >= t) == 0:
                p = 0
            else:
                p = np.max(precision[recall >= t])
            ap += p / 11
        
        aps.append(ap)
        print(f"  {DIOR_CLASSES[cls]:15s} AP: {ap*100:.2f}% (GT: {num_gt})")
    
    if len(aps) == 0:
        return 0.0
    
    return np.mean(aps)


def evaluate():
    print("=" * 70)
    print("Experiment2 æ­£ç¡®ç‰ˆæœ¬è¯„ä¼°")
    print("æ–‡æœ¬é©±åŠ¨ä½ç½®é¢„æµ‹ + å…¨å±€-å±€éƒ¨å¯¹æ¯”å­¦ä¹ ")
    print("=" * 70)
    
    device = torch.device('cuda')
    
    # åŠ è½½æœ€ä½³æ¨¡å‹
    checkpoint_path = 'outputs/checkpoints/correct_best_model.pth'
    print(f"\nåŠ è½½checkpoint: {checkpoint_path}")
    
    if not Path(checkpoint_path).exists():
        print(f"âŒ Checkpointä¸å­˜åœ¨ï¼Œè¯·å…ˆå®Œæˆè®­ç»ƒ")
        return
    
    checkpoint = torch.load(checkpoint_path)
    
    # å¯¼å…¥ç¼–ç å™¨
    from stage1_encoder.clip_text_encoder import CLIPTextEncoder
    from stage1_encoder.clip_image_encoder import CLIPImageEncoder
    
    # åˆ›å»ºæ¨¡å‹
    print("\nåˆ›å»ºæ¨¡å‹...")
    text_encoder = CLIPTextEncoder('RN50', '../checkpoints/RemoteCLIP-RN50.pt').cuda()
    image_encoder = CLIPImageEncoder('RN50', '../checkpoints/RemoteCLIP-RN50.pt', freeze=False).cuda()
    text_to_position = TextToPositionProjector(text_dim=1024).cuda()
    query_extractor = SimpleDeformableQueryExtractor(d_model=1024).cuda()
    box_regressor = BoxRegressor(d_model=1024).cuda()
    
    # åŠ è½½æƒé‡
    text_encoder.load_state_dict(checkpoint['text_encoder'])
    image_encoder.load_state_dict(checkpoint['image_encoder'])
    text_to_position.load_state_dict(checkpoint['text_to_position'])
    query_extractor.load_state_dict(checkpoint['query_extractor'])
    box_regressor.load_state_dict(checkpoint['box_regressor'])
    
    text_encoder.eval()
    image_encoder.eval()
    text_to_position.eval()
    query_extractor.eval()
    box_regressor.eval()
    
    print(f"âœ… æ¨¡å‹åŠ è½½æˆåŠŸ (Epoch {checkpoint['epoch']}, Loss: {checkpoint['loss']:.4f})")
    
    # æå–æ–‡æœ¬ç‰¹å¾
    with torch.no_grad():
        text_features = text_encoder(DIOR_CLASSES).cuda()
        text_features_norm = text_features / text_features.norm(dim=-1, keepdim=True)
    
    # åŠ è½½æµ‹è¯•æ•°æ®
    print("\nåŠ è½½æµ‹è¯•æ•°æ®...")
    test_dataset = MiniDataset('../datasets/mini_dataset', 'test')
    print(f"  æµ‹è¯•é›†: {len(test_dataset)} å¼ å›¾")
    
    transform = T.Compose([
        T.Resize((224, 224)),
        T.ToTensor(),
        T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
    ])
    
    print("\nå¼€å§‹è¯„ä¼°...")
    print("  ä½¿ç”¨æ–‡æœ¬é¢„æµ‹åˆå§‹ä½ç½® â†’ Query â†’ ç²¾ä¿®æ¡†")
    
    all_predictions = []
    all_targets = []
    
    with torch.no_grad():
        for idx in tqdm(range(len(test_dataset)), desc="è¯„ä¼°"):
            image_pil, target = test_dataset[idx]
            image = transform(image_pil).unsqueeze(0).cuda()
            
            # æå–å…¨å±€ç‰¹å¾
            _, global_features = image_encoder(image)
            
            # å…³é”®ï¼šç”¨æ–‡æœ¬é¢„æµ‹åˆå§‹æ¡†ä½ç½®
            predicted_init_boxes = text_to_position(text_features)  # [20, 4]
            
            # æ‰©å±•å…¨å±€ç‰¹å¾
            global_feat_expanded = global_features.expand(20, -1)
            
            # æå–å±€éƒ¨ç‰¹å¾
            local_features = query_extractor(global_feat_expanded, predicted_init_boxes)
            
            # ç²¾ä¿®æ¡†
            refined_boxes = box_regressor(local_features)  # [20, 4]
            
            # è®¡ç®—åˆ†æ•°
            local_features_norm = local_features / (local_features.norm(dim=-1, keepdim=True) + 1e-8)
            scores = (local_features_norm * text_features_norm).sum(dim=-1)
            
            # è¿‡æ»¤ä½åˆ†
            score_threshold = 0.3
            keep = scores > score_threshold
            
            pred_boxes = refined_boxes[keep]
            pred_labels = torch.arange(20, device=device)[keep]
            pred_scores = scores[keep]
            
            # è½¬æ¢ä¸ºxyxyåƒç´ åæ ‡
            pred_boxes_xyxy = box_cxcywh_to_xyxy(pred_boxes) * 224
            
            all_predictions.append({
                'boxes': pred_boxes_xyxy,
                'labels': pred_labels,
                'scores': pred_scores
            })
            
            # GTä¹Ÿè½¬æ¢ä¸ºxyxyåƒç´ åæ ‡
            gt_boxes = target['boxes'].cuda()
            gt_labels = target['labels'].cuda()
            gt_boxes_xyxy = box_cxcywh_to_xyxy(gt_boxes) * 224
            
            all_targets.append({
                'boxes': gt_boxes_xyxy,
                'labels': gt_labels
            })
    
    # è®¡ç®—mAP
    print("\n" + "=" * 70)
    print("è®¡ç®—mAP@50...")
    print("=" * 70)
    
    map_50 = calculate_map(all_predictions, all_targets, iou_threshold=0.5)
    
    print("\n" + "=" * 70)
    print(f"ğŸ“Š æœ€ç»ˆç»“æœ")
    print("=" * 70)
    print(f"  mAP@50:    {map_50*100:.2f}%")
    print(f"  æµ‹è¯•å›¾ç‰‡:   {len(test_dataset)}")
    print(f"  æ€»é¢„æµ‹æ¡†:   {sum([len(p['boxes']) for p in all_predictions])}")
    print(f"  æ€»GTæ¡†:     {sum([len(t['boxes']) for t in all_targets])}")
    
    # ä¿å­˜ç»“æœ
    results = {
        'mAP@50': float(map_50),
        'num_test_images': len(test_dataset),
        'total_predictions': sum([len(p['boxes']) for p in all_predictions]),
        'total_gt': sum([len(t['boxes']) for t in all_targets]),
        'epoch': checkpoint['epoch'],
        'training_loss': checkpoint['loss']
    }
    
    Path('outputs').mkdir(exist_ok=True)
    with open('outputs/correct_version_results.json', 'w', encoding='utf-8') as f:
        json.dump(results, f, indent=2, ensure_ascii=False)
    
    print(f"\nâœ… ç»“æœä¿å­˜åˆ°: outputs/correct_version_results.json")


if __name__ == '__main__':
    evaluate()

