#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Experiment2 è®­ç»ƒè„šæœ¬æ¡†æ¶

æ³¨æ„ï¼šéœ€è¦å…ˆç»„è£…å®Œæ•´æ¨¡å‹
"""

import torch
import argparse
from pathlib import Path

from config.default_config import DefaultConfig
from utils.dataloader import create_dataloader, DIOR_CLASSES


def train():
    """è®­ç»ƒå‡½æ•°æ¡†æ¶"""
    
    print("=" * 70)
    print("Experiment2 è®­ç»ƒè„šæœ¬")
    print("=" * 70)
    
    print("\nâš ï¸ è®­ç»ƒå‰éœ€è¦å®Œæˆ:")
    print("  1. ç»„è£…å®Œæ•´æ¨¡å‹ (models/context_guided_detector.py)")
    print("  2. ç¡®ä¿æ‰€æœ‰æ¨¡å—æ­£ç¡®è¿æ¥")
    print("  3. æµ‹è¯•å‰å‘ä¼ æ’­")
    
    print("\nâœ… å·²å‡†å¤‡çš„ç»„ä»¶:")
    print("  - æ•°æ®åŠ è½½å™¨ âœ…")
    print("  - æŸå¤±å‡½æ•° âœ… (box_loss, global_contrast_loss)")
    print("  - åŒ¹é…å™¨ âœ… (Hungarian matcher)")
    print("  - åå¤„ç†å™¨ âœ… (NMS)")
    
    print("\nğŸ“‹ è®­ç»ƒæµç¨‹æ¡†æ¶:")
    print("""
    1. åŠ è½½æ•°æ®
       train_loader = create_dataloader('datasets/mini_dataset', 'train')
       val_loader = create_dataloader('datasets/mini_dataset', 'val')
    
    2. åˆ›å»ºæ¨¡å‹
       model = ContextGuidedDetector(config)
       # éœ€è¦å®ç°å®Œæ•´çš„å‰å‘ä¼ æ’­
    
    3. åˆ›å»ºä¼˜åŒ–å™¨
       optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4)
    
    4. è®­ç»ƒå¾ªç¯
       for epoch in range(num_epochs):
           for images, targets in train_loader:
               # å‰å‘ä¼ æ’­
               outputs = model(images, text_features)
               
               # è®¡ç®—æŸå¤±
               loss_dict = criterion(outputs, targets)
               loss = loss_dict['total_loss']
               
               # åå‘ä¼ æ’­
               optimizer.zero_grad()
               loss.backward()
               optimizer.step()
    
    5. éªŒè¯å’Œä¿å­˜
       validate(model, val_loader)
       save_checkpoint(model, optimizer, epoch)
    """)
    
    # æµ‹è¯•æ•°æ®åŠ è½½
    print("\nğŸ”¬ æµ‹è¯•æ•°æ®åŠ è½½...")
    
    try:
        config = DefaultConfig()
        
        train_loader = create_dataloader(
            root_dir='datasets/mini_dataset',
            split='train',
            batch_size=config.batch_size,
            image_size=config.image_size,
            augment=True,
            num_workers=0
        )
        
        print(f"âœ… è®­ç»ƒé›†: {len(train_loader.dataset)}å¼ å›¾ç‰‡, {len(train_loader)}ä¸ªæ‰¹æ¬¡")
        
        # æµ‹è¯•ä¸€ä¸ªæ‰¹æ¬¡
        images, targets = next(iter(train_loader))
        print(f"\næ‰¹æ¬¡æµ‹è¯•:")
        print(f"  å›¾åƒ: {images.shape}")
        print(f"  ç›®æ ‡æ•°: {[len(t['labels']) for t in targets]}")
        
    except Exception as e:
        print(f"âŒ æ•°æ®åŠ è½½å¤±è´¥: {e}")
    
    print("\n" + "=" * 70)
    print("è®­ç»ƒæ¡†æ¶å‡†å¤‡å®Œæˆï¼")
    print("å®Œæˆæ¨¡å‹ç»„è£…åå³å¯å¼€å§‹è®­ç»ƒ")
    print("=" * 70)


if __name__ == '__main__':
    train()


