# Experiment2 æ¨¡å‹ç»„è£…æŒ‡å—

## ğŸ“‹ ç°çŠ¶

### âœ… å·²å®ç°çš„æ¨¡å— (11ä¸ª)

**Stage1: ç¼–ç å™¨** (2ä¸ª)
- âœ… `stage1_encoder/clip_text_encoder.py` - CLIPæ–‡æœ¬ç¼–ç å™¨
- âœ… `stage1_encoder/global_context_extractor.py` - å…¨å±€ä¸Šä¸‹æ–‡æå–å™¨

**Stage2: è§£ç å™¨** (3ä¸ª)
- âœ… `stage2_decoder/query_initializer.py` - æŸ¥è¯¢åˆå§‹åŒ–å™¨
- âœ… `stage2_decoder/context_gating.py` - ä¸Šä¸‹æ–‡é—¨æ§
- âœ… `stage2_decoder/text_conditioner.py` - æ–‡æœ¬è°ƒèŠ‚å™¨

**Stage3: é¢„æµ‹** (2ä¸ª)
- âœ… `stage3_prediction/classification_head.py` - åˆ†ç±»å¤´
- âœ… `stage3_prediction/regression_head.py` - å›å½’å¤´

**Stage4: ç›‘ç£** (3ä¸ª)
- âœ… `stage4_supervision/box_loss.py` - è¾¹ç•Œæ¡†æŸå¤±
- âœ… `stage4_supervision/global_contrast_loss.py` - å…¨å±€å¯¹æ¯”æŸå¤±
- âœ… `stage4_supervision/matcher.py` - åŒˆç‰™åˆ©åŒ¹é…å™¨

**æ¨ç†** (1ä¸ª)
- âœ… `inference/post_processor.py` - åå¤„ç†å™¨ (NMS)

**æ–°å¢** (3ä¸ª)
- âœ… `utils/dataloader.py` - æ•°æ®åŠ è½½å™¨
- âœ… `utils/evaluation.py` - mAPè¯„ä¼°
- âœ… `utils/box_utils.py` - è¾¹ç•Œæ¡†å·¥å…·

---

## ğŸ—ï¸ éœ€è¦ç»„è£…çš„å®Œæ•´æ¨¡å‹

### æ–‡ä»¶: `models/context_guided_detector.py`

```python
#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ä¸Šä¸‹æ–‡å¼•å¯¼æ£€æµ‹å™¨ - å®Œæ•´æ¨¡å‹

æ•´åˆæ‰€æœ‰æ¨¡å—
"""

import torch
import torch.nn as nn

from stage1_encoder.clip_text_encoder import CLIPTextEncoder
from stage1_encoder.global_context_extractor import GlobalContextExtractor
from stage2_decoder.query_initializer import QueryInitializer
from stage2_decoder.context_gating import ContextGating
from stage2_decoder.text_conditioner import TextConditioner
from stage3_prediction.classification_head import ClassificationHead
from stage3_prediction.regression_head import RegressionHead


class ContextGuidedDetector(nn.Module):
    """
    ä¸Šä¸‹æ–‡å¼•å¯¼æ£€æµ‹å™¨
    
    å®Œæ•´æ¶æ„ï¼š
    1. CLIPæ–‡æœ¬ç¼–ç  + å…¨å±€ä¸Šä¸‹æ–‡æå–
    2. æŸ¥è¯¢åˆå§‹åŒ– + ä¸Šä¸‹æ–‡é—¨æ§ + æ–‡æœ¬è°ƒèŠ‚
    3. åˆ†ç±»é¢„æµ‹ + è¾¹ç•Œæ¡†å›å½’
    """
    
    def __init__(self, config):
        super().__init__()
        
        self.config = config
        
        # Stage1: ç¼–ç å™¨
        self.text_encoder = CLIPTextEncoder(
            model_name=config.clip_model_name,
            pretrained_path=config.clip_checkpoint
        )
        
        self.context_extractor = GlobalContextExtractor(
            input_dim=config.d_clip,
            output_dim=config.d_model
        )
        
        # Stage2: è§£ç å™¨
        self.query_initializer = QueryInitializer(
            num_queries=config.num_queries,
            d_model=config.d_model
        )
        
        self.context_gating = ContextGating(
            d_model=config.d_model,
            gating_type=config.context_gating_type
        )
        
        self.text_conditioner = TextConditioner(
            d_model=config.d_model,
            num_heads=config.num_heads
        )
        
        # Stage3: é¢„æµ‹å¤´
        self.classification_head = ClassificationHead(
            d_model=config.d_model,
            num_classes=20  # DIOR
        )
        
        self.regression_head = RegressionHead(
            d_model=config.d_model
        )
    
    def forward(self, images, text_queries):
        """
        å‰å‘ä¼ æ’­
        
        å‚æ•°:
            images: (B, 3, H, W)
            text_queries: List of text queries
        
        è¿”å›:
            outputs: {
                'pred_logits': (B, M, num_classes),
                'pred_boxes': (B, M, 4),
                'text_features': (num_classes, d_clip)
            }
        """
        batch_size = images.size(0)
        
        # 1. æ–‡æœ¬ç¼–ç 
        text_features = self.text_encoder(text_queries)  # (N, d_clip)
        
        # 2. å…¨å±€ä¸Šä¸‹æ–‡æå– (éœ€è¦å›¾åƒç¼–ç å™¨ - å¾…å®ç°)
        # global_context = self.context_extractor(image_features)
        
        # 3. æŸ¥è¯¢åˆå§‹åŒ–
        queries = self.query_initializer(batch_size)  # (B, M, d_model)
        
        # 4. ä¸Šä¸‹æ–‡é—¨æ§ (éœ€è¦è§†è§‰ç‰¹å¾ - å¾…å®ç°)
        # queries_gated = self.context_gating(queries, global_context)
        
        # 5. æ–‡æœ¬è°ƒèŠ‚
        # queries_conditioned = self.text_conditioner(queries_gated, text_features)
        
        # 6. é¢„æµ‹
        # pred_logits = self.classification_head(queries_conditioned, text_features)
        # pred_boxes = self.regression_head(queries_conditioned)
        
        # ä¸´æ—¶è¿”å›ï¼ˆå¾…å®Œæ•´å®ç°ï¼‰
        outputs = {
            'pred_logits': torch.zeros(batch_size, self.config.num_queries, 20),
            'pred_boxes': torch.zeros(batch_size, self.config.num_queries, 4),
            'text_features': text_features
        }
        
        return outputs


# æ³¨æ„ï¼šè¿™æ˜¯ä¸€ä¸ªæ¡†æ¶ï¼Œéœ€è¦è¡¥å……ï¼š
# 1. å›¾åƒç¼–ç å™¨ï¼ˆæå–è§†è§‰ç‰¹å¾ï¼‰
# 2. ç‰¹å¾èåˆçš„å…·ä½“å®ç°
# 3. å¤šå±‚Transformerè§£ç å™¨ï¼ˆå¦‚æœéœ€è¦ï¼‰
# 4. å®Œæ•´çš„å‰å‘ä¼ æ’­é€»è¾‘
```

---

## ğŸ”§ ç»„è£…æ­¥éª¤

### æ­¥éª¤1: è¡¥å……å›¾åƒç¼–ç å™¨

éœ€è¦åˆ›å»º `stage1_encoder/clip_image_encoder.py`:

```python
class CLIPImageEncoder(nn.Module):
    """CLIPå›¾åƒç¼–ç å™¨"""
    
    def __init__(self, model_name='RN50', pretrained_path=None):
        super().__init__()
        # åŠ è½½RemoteCLIPå›¾åƒç¼–ç å™¨
        self.model, _, _ = open_clip.create_model_and_transforms(model_name)
        if pretrained_path:
            ckpt = torch.load(pretrained_path)
            self.model.load_state_dict(ckpt)
    
    def forward(self, images):
        return self.model.encode_image(images)
```

### æ­¥éª¤2: å®Œå–„å‰å‘ä¼ æ’­

è¿æ¥æ‰€æœ‰æ¨¡å—ï¼š

```python
def forward(self, images, text_queries):
    # 1. å›¾åƒç¼–ç 
    image_features = self.image_encoder(images)  # (B, d_clip)
    
    # 2. æ–‡æœ¬ç¼–ç 
    text_features = self.text_encoder(text_queries)  # (N, d_clip)
    
    # 3. å…¨å±€ä¸Šä¸‹æ–‡
    global_context = self.context_extractor(image_features)  # (B, d_model)
    
    # 4. æŸ¥è¯¢åˆå§‹åŒ–
    queries = self.query_initializer(batch_size)  # (B, M, d_model)
    
    # 5. ä¸Šä¸‹æ–‡é—¨æ§
    queries_gated = self.context_gating(queries, global_context)
    
    # 6. æ–‡æœ¬è°ƒèŠ‚
    queries_conditioned = self.text_conditioner(queries_gated, text_features)
    
    # 7. é¢„æµ‹
    pred_logits = self.classification_head(queries_conditioned, text_features)
    pred_boxes = self.regression_head(queries_conditioned)
    
    return {
        'pred_logits': pred_logits,
        'pred_boxes': pred_boxes
    }
```

### æ­¥éª¤3: åˆ›å»ºè®­ç»ƒè„šæœ¬

å‚è€ƒ `experiment3/train.py` çš„ç»“æ„

### æ­¥éª¤4: æµ‹è¯•

```bash
# æµ‹è¯•æ¨¡å‹åˆ›å»º
python -c "from models.context_guided_detector import ContextGuidedDetector"

# æµ‹è¯•å‰å‘ä¼ æ’­
python models/context_guided_detector.py
```

---

## ğŸ“š å‚è€ƒèµ„æº

### Experiment3 å‚è€ƒ

å¯ä»¥å‚è€ƒ Experiment3 çš„ä»¥ä¸‹æ–‡ä»¶ï¼š

1. **å®Œæ•´æ¨¡å‹**: `experiment3/models/ova_detr.py`
   - æ¨¡å—ç»„è£…æ–¹å¼
   - å‰å‘ä¼ æ’­é€»è¾‘
   - ç‰¹å¾ç»´åº¦åŒ¹é…

2. **è®­ç»ƒè„šæœ¬**: `experiment3/train.py`
   - è®­ç»ƒå¾ªç¯
   - ä¼˜åŒ–å™¨é…ç½®
   - æ£€æŸ¥ç‚¹ä¿å­˜

3. **è¯„ä¼°è„šæœ¬**: `experiment3/evaluate.py`
   - è¯„ä¼°æµç¨‹
   - mAPè®¡ç®—
   - ç»“æœä¿å­˜

---

## âœ… ç»„è£…åçš„å®Œæ•´ç³»ç»Ÿ

å®Œæˆåä½ å°†æ‹¥æœ‰ï¼š

```
experiment2/
â”œâ”€â”€ stage1_encoder/
â”‚   â”œâ”€â”€ clip_image_encoder.py    âœ… (éœ€åˆ›å»º)
â”‚   â”œâ”€â”€ clip_text_encoder.py     âœ…
â”‚   â””â”€â”€ global_context_extractor.py  âœ…
â”œâ”€â”€ stage2_decoder/
â”‚   â”œâ”€â”€ query_initializer.py     âœ…
â”‚   â”œâ”€â”€ context_gating.py        âœ…
â”‚   â””â”€â”€ text_conditioner.py      âœ…
â”œâ”€â”€ stage3_prediction/
â”‚   â”œâ”€â”€ classification_head.py   âœ…
â”‚   â””â”€â”€ regression_head.py       âœ…
â”œâ”€â”€ stage4_supervision/
â”‚   â”œâ”€â”€ box_loss.py              âœ…
â”‚   â”œâ”€â”€ global_contrast_loss.py  âœ…
â”‚   â””â”€â”€ matcher.py               âœ…
â”œâ”€â”€ models/
â”‚   â””â”€â”€ context_guided_detector.py  âœ… (éœ€å®Œå–„)
â”œâ”€â”€ utils/
â”‚   â”œâ”€â”€ dataloader.py            âœ…
â”‚   â”œâ”€â”€ evaluation.py            âœ…
â”‚   â””â”€â”€ box_utils.py             âœ…
â”œâ”€â”€ train.py                     âœ… (æ¡†æ¶å·²æœ‰)
â””â”€â”€ evaluate.py                  âœ… (æ¡†æ¶å·²æœ‰)
```

---

## ğŸ¯ ä¸‹ä¸€æ­¥

1. åˆ›å»º `stage1_encoder/clip_image_encoder.py`
2. å®Œå–„ `models/context_guided_detector.py` çš„å‰å‘ä¼ æ’­
3. æµ‹è¯•æ¨¡å‹åˆ›å»ºå’Œå‰å‘ä¼ æ’­
4. è¿è¡Œè®­ç»ƒè„šæœ¬
5. è¯„ä¼°æ€§èƒ½

---

**åˆ›å»ºæ—¶é—´**: 2025-10-24  
**çŠ¶æ€**: æ¡†æ¶å®Œæˆï¼Œå¾…æ¨¡å‹ç»„è£…  
**é¢„è®¡å·¥ä½œé‡**: 2-3å°æ—¶

