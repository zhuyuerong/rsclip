#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Experiment2 è¯„ä¼°è„šæœ¬
åŠ è½½è®­ç»ƒå¥½çš„æ¨¡å‹ï¼Œè®¡ç®—mAPå¹¶å¯è§†åŒ–
"""

import torch
import torch.nn as nn
from torch.utils.data import DataLoader
import sys
from pathlib import Path
import json
import numpy as np
from PIL import Image, ImageDraw, ImageFont
import matplotlib.pyplot as plt
from tqdm import tqdm

sys.path.append('..')

from config.default_config import DefaultConfig
from stage1_encoder.clip_text_encoder import CLIPTextEncoder
from stage1_encoder.clip_image_encoder import CLIPImageEncoder
from datasets.mini_dataset.mini_dataset_loader import MiniDataset
import torchvision.transforms as T


def calculate_iou(box1, box2):
    """è®¡ç®—IoU (xyxyæ ¼å¼)"""
    x1 = max(box1[0], box2[0])
    y1 = max(box1[1], box2[1])
    x2 = min(box1[2], box2[2])
    y2 = min(box1[3], box2[3])
    
    intersection = max(0, x2 - x1) * max(0, y2 - y1)
    area1 = (box1[2] - box1[0]) * (box1[3] - box1[1])
    area2 = (box2[2] - box2[0]) * (box2[3] - box2[1])
    union = area1 + area2 - intersection
    
    return intersection / union if union > 0 else 0


def calculate_ap(precisions, recalls):
    """è®¡ç®—APï¼ˆ11ç‚¹æ’å€¼ï¼‰"""
    ap = 0.0
    for t in np.arange(0, 1.1, 0.1):
        if np.sum(recalls >= t) == 0:
            p = 0
        else:
            p = np.max(precisions[recalls >= t])
        ap += p / 11.0
    return ap


def evaluate_classification(image_features, text_features, labels):
    """
    è¯„ä¼°åˆ†ç±»æ€§èƒ½
    
    Args:
        image_features: [N, D] å›¾åƒç‰¹å¾
        text_features: [C, D] æ–‡æœ¬ç‰¹å¾  
        labels: [N] çœŸå®æ ‡ç­¾
    
    Returns:
        accuracy, top5_accuracy
    """
    # å½’ä¸€åŒ–
    image_features = image_features / image_features.norm(dim=-1, keepdim=True)
    text_features = text_features / text_features.norm(dim=-1, keepdim=True)
    
    # è®¡ç®—ç›¸ä¼¼åº¦
    similarity = image_features @ text_features.T  # [N, C]
    
    # Top-1å‡†ç¡®ç‡
    pred_labels = similarity.argmax(dim=-1)
    accuracy = (pred_labels == labels).float().mean().item()
    
    # Top-5å‡†ç¡®ç‡
    top5_pred = similarity.topk(5, dim=-1)[1]
    top5_accuracy = sum([labels[i] in top5_pred[i] for i in range(len(labels))]) / len(labels)
    
    return accuracy, top5_accuracy


def visualize_predictions(image, predictions, ground_truth_label, class_names, save_path):
    """
    å¯è§†åŒ–é¢„æµ‹ç»“æœï¼ˆåˆ†ç±»æ¨¡å‹ï¼‰
    
    Args:
        image: PIL Image
        predictions: list of dict with 'label', 'score'
        ground_truth_label: int, GTæ ‡ç­¾
        class_names: list of class names
        save_path: ä¿å­˜è·¯å¾„
    """
    # åˆ›å»ºç”»å¸ƒ
    draw = ImageDraw.Draw(image)
    
    # å°è¯•åŠ è½½å­—ä½“
    try:
        font = ImageFont.truetype("/usr/share/fonts/truetype/dejavu/DejaVuSans-Bold.ttf", 24)
        small_font = ImageFont.truetype("/usr/share/fonts/truetype/dejavu/DejaVuSans.ttf", 18)
    except:
        font = ImageFont.load_default()
        small_font = font
    
    w, h = image.size
    
    # ç»˜åˆ¶GTæ ‡ç­¾ï¼ˆé¡¶éƒ¨ï¼Œç»¿è‰²ï¼‰
    gt_text = f"Ground Truth: {class_names[ground_truth_label]}"
    draw.rectangle([0, 0, w, 40], fill='green')
    draw.text((10, 10), gt_text, fill='white', font=font)
    
    # ç»˜åˆ¶Top-3é¢„æµ‹ï¼ˆåº•éƒ¨ï¼‰
    y_offset = h - 150
    draw.rectangle([0, y_offset, w, h], fill='black')
    draw.text((10, y_offset + 5), "Top-3 Predictions:", fill='white', font=font)
    
    for i, pred in enumerate(predictions[:3]):
        label = pred['label']
        score = pred['score']
        color = 'lime' if label == ground_truth_label else 'red'
        text = f"{i+1}. {class_names[label]}: {score:.2f}"
        draw.text((10, y_offset + 35 + i*30), text, fill=color, font=small_font)
    
    # ä¿å­˜
    image.save(save_path)
    return save_path


def evaluate():
    print("=" * 70)
    print("Experiment2 è¯„ä¼°")
    print("=" * 70)
    
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    print(f"\nè®¾å¤‡: {device}")
    
    # åŠ è½½ç±»åˆ«åç§°
    from utils.dataloader import DIOR_CLASSES
    
    # åŠ è½½checkpoint
    checkpoint_path = 'outputs/checkpoints/simple_epoch_10.pth'
    print(f"\nåŠ è½½checkpoint: {checkpoint_path}")
    
    checkpoint = torch.load(checkpoint_path, map_location=device)
    
    # åˆ›å»ºæ¨¡å‹
    print("\nåˆ›å»ºæ¨¡å‹...")
    text_encoder = CLIPTextEncoder('RN50', '../checkpoints/RemoteCLIP-RN50.pt').to(device)
    image_encoder = CLIPImageEncoder('RN50', '../checkpoints/RemoteCLIP-RN50.pt', freeze=False).to(device)
    
    # åŠ è½½æƒé‡
    text_encoder.load_state_dict(checkpoint['text_encoder'])
    image_encoder.load_state_dict(checkpoint['image_encoder'])
    
    text_encoder.eval()
    image_encoder.eval()
    
    print(f"âœ… æ¨¡å‹åŠ è½½æˆåŠŸ (Epoch {checkpoint['epoch']}, Loss: {checkpoint['loss']:.4f})")
    
    # æå–æ–‡æœ¬ç‰¹å¾
    print("\næå–æ–‡æœ¬ç‰¹å¾...")
    with torch.no_grad():
        text_features = text_encoder(DIOR_CLASSES).to(device)
    
    print(f"  æ–‡æœ¬ç‰¹å¾: {text_features.shape}")
    
    # åŠ è½½æµ‹è¯•æ•°æ®
    print("\nåŠ è½½æµ‹è¯•æ•°æ®...")
    test_dataset = MiniDataset('../datasets/mini_dataset', 'test', transforms=None)
    print(f"  æµ‹è¯•é›†: {len(test_dataset)} å¼ å›¾")
    
    # è¯„ä¼°åˆ†ç±»æ€§èƒ½
    print("\nè¯„ä¼°åˆ†ç±»æ€§èƒ½...")
    
    all_image_features = []
    all_labels = []
    all_images = []
    all_targets = []
    
    transform = T.Compose([
        T.Resize((224, 224)),
        T.ToTensor(),
        T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
    ])
    
    with torch.no_grad():
        for idx in tqdm(range(len(test_dataset)), desc="æå–ç‰¹å¾"):
            image, target = test_dataset[idx]
            
            # ä¿å­˜åŸå›¾ç”¨äºå¯è§†åŒ–
            all_images.append(image.copy())
            all_targets.append(target)
            
            # è½¬æ¢å›¾åƒ
            image_tensor = transform(image).unsqueeze(0).to(device)
            
            # æå–ç‰¹å¾
            _, image_feat = image_encoder(image_tensor)
            
            all_image_features.append(image_feat.squeeze(0))
            
            # è·å–æ ‡ç­¾ï¼ˆä½¿ç”¨ç¬¬ä¸€ä¸ªæ¡†çš„æ ‡ç­¾ï¼‰
            if len(target['labels']) > 0:
                all_labels.append(target['labels'][0])
    
    # è½¬æ¢ä¸ºtensor
    all_image_features = torch.stack(all_image_features)
    all_labels = torch.tensor(all_labels).to(device)
    
    print(f"\næ”¶é›†ç‰¹å¾:")
    print(f"  å›¾åƒç‰¹å¾: {all_image_features.shape}")
    print(f"  æ ‡ç­¾æ•°é‡: {len(all_labels)}")
    
    # è®¡ç®—åˆ†ç±»å‡†ç¡®ç‡
    accuracy, top5_acc = evaluate_classification(all_image_features, text_features, all_labels)
    
    print(f"\nğŸ“Š åˆ†ç±»æ€§èƒ½:")
    print(f"  Top-1 å‡†ç¡®ç‡: {accuracy*100:.2f}%")
    print(f"  Top-5 å‡†ç¡®ç‡: {top5_acc*100:.2f}%")
    
    # å¯è§†åŒ–å‰10å¼ å›¾ç‰‡çš„é¢„æµ‹
    print(f"\nå¯è§†åŒ–é¢„æµ‹ç»“æœ...")
    vis_dir = Path('outputs/visualizations')
    vis_dir.mkdir(parents=True, exist_ok=True)
    
    # å½’ä¸€åŒ–ç‰¹å¾ç”¨äºé¢„æµ‹
    image_features_norm = all_image_features / all_image_features.norm(dim=-1, keepdim=True)
    text_features_norm = text_features / text_features.norm(dim=-1, keepdim=True)
    similarity = image_features_norm @ text_features_norm.T
    
    for idx in range(min(10, len(all_images))):
        image = all_images[idx]
        gt_label = int(all_labels[idx].item())
        
        # è·å–é¢„æµ‹
        scores = similarity[idx]
        top_scores, top_labels = scores.topk(3)
        
        # åˆ›å»ºé¢„æµ‹åˆ—è¡¨
        predictions = []
        for label, score in zip(top_labels, top_scores):
            predictions.append({
                'label': int(label),
                'score': float(score)
            })
        
        # å¯è§†åŒ–
        save_path = visualize_predictions(
            image.copy(),
            predictions,
            gt_label,
            DIOR_CLASSES,
            vis_dir / f'test_{idx:03d}.jpg'
        )
    
    print(f"âœ… ä¿å­˜äº† {min(10, len(all_images))} å¼ å¯è§†åŒ–å›¾ç‰‡")
    
    # è®¡ç®—æ¯ä¸ªç±»åˆ«çš„å‡†ç¡®ç‡
    print(f"\nğŸ“‹ å„ç±»åˆ«å‡†ç¡®ç‡:")
    pred_labels = similarity.argmax(dim=-1).cpu().numpy()
    true_labels = all_labels.cpu().numpy()
    
    class_accuracy = {}
    for class_id in range(len(DIOR_CLASSES)):
        mask = true_labels == class_id
        if mask.sum() > 0:
            acc = (pred_labels[mask] == true_labels[mask]).mean()
            class_accuracy[DIOR_CLASSES[class_id]] = acc
            print(f"  {DIOR_CLASSES[class_id]:20s}: {acc*100:.1f}% ({mask.sum()} samples)")
    
    # ä¿å­˜ç»“æœ
    results = {
        'checkpoint': checkpoint_path,
        'epoch': checkpoint['epoch'],
        'train_loss': checkpoint['loss'],
        'test_metrics': {
            'top1_accuracy': accuracy,
            'top5_accuracy': top5_acc,
            'num_test_samples': len(all_labels)
        },
        'class_accuracy': {k: float(v) for k, v in class_accuracy.items()}
    }
    
    results_file = 'outputs/evaluation_results.json'
    with open(results_file, 'w', encoding='utf-8') as f:
        json.dump(results, f, indent=2, ensure_ascii=False)
    
    print(f"\nâœ… è¯„ä¼°ç»“æœä¿å­˜åˆ°: {results_file}")
    print(f"âœ… å¯è§†åŒ–ç»“æœä¿å­˜åˆ°: {vis_dir}/")
    
    # ç»˜åˆ¶æ··æ·†çŸ©é˜µ
    print(f"\nç”Ÿæˆæ··æ·†çŸ©é˜µ...")
    from sklearn.metrics import confusion_matrix
    import seaborn as sns
    
    cm = confusion_matrix(true_labels, pred_labels)
    
    plt.figure(figsize=(12, 10))
    sns.heatmap(cm, annot=False, fmt='d', cmap='Blues',
                xticklabels=DIOR_CLASSES, yticklabels=DIOR_CLASSES)
    plt.xlabel('Predicted')
    plt.ylabel('True')
    plt.title(f'Confusion Matrix (Accuracy: {accuracy*100:.1f}%)')
    plt.tight_layout()
    plt.savefig(vis_dir / 'confusion_matrix.png', dpi=150)
    print(f"âœ… æ··æ·†çŸ©é˜µä¿å­˜åˆ°: {vis_dir}/confusion_matrix.png")
    
    print("\n" + "=" * 70)
    print("è¯„ä¼°å®Œæˆï¼")
    print("=" * 70)
    
    return results


if __name__ == '__main__':
    evaluate()

